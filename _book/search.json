[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for Research in Action",
    "section": "",
    "text": "Preface\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Mauris sed nisi porta, lobortis mi eget, laoreet sapien. Etiam sit amet tellus luctus, rhoncus dolor pharetra, faucibus sem. Nulla viverra risus et urna porttitor mattis. In et tortor a justo convallis ultricies. Praesent consectetur bibendum tellus eget malesuada. In fermentum sodales luctus. Curabitur in ornare eros. In pulvinar tempor odio et cursus. Aenean tristique vel leo vel congue. Pellentesque id metus mollis, tincidunt nibh a, dapibus elit. Etiam at augue et nisl volutpat ornare pulvinar vitae urna. Duis auctor urna quam, eget rhoncus diam fermentum at.\nPraesent scelerisque efficitur euismod. Aliquam neque augue, fermentum a magna vitae, interdum varius turpis. Fusce commodo maximus magna sit amet rutrum. Nulla sit amet sem a eros tempus finibus non ac leo. Donec sit amet gravida eros. Donec ut lacus eget velit aliquet luctus. Ut pellentesque tellus varius, gravida velit eu, finibus ante. Maecenas vehicula, elit non volutpat mollis, est augue placerat orci, nec condimentum neque nulla vulputate elit. Nunc hendrerit hendrerit cursus. Aenean elementum mi dolor, sed maximus ex rutrum convallis. Nulla lobortis faucibus justo, in elementum ligula semper non. Quisque sollicitudin pretium tortor vel sollicitudin. Curabitur quis luctus ante. Ut sit amet dui consequat, tristique diam a, suscipit lacus."
  },
  {
    "objectID": "objects.html#classes",
    "href": "objects.html#classes",
    "title": "1  R Objects",
    "section": "1.1 Classes",
    "text": "1.1 Classes\nR has five basic or “atomic” classes of objects:\n\ncharacter\nnumeric (real numbers)\ninteger\ncomplex\nlogical (True/False)\n\nEntering 1 in R gives you a numeric object; entering 1L explicitly gives you an integer object."
  },
  {
    "objectID": "objects.html#attributes",
    "href": "objects.html#attributes",
    "title": "1  R Objects",
    "section": "1.2 Attributes",
    "text": "1.2 Attributes\nAttributes of an object (if any) can be accessed using the attributes() function:\n\nnames, dimnames\ndimensions (e.g. matrices, arrays)\nclass (e.g. integer, numeric)\nlength\n\nThe mode() of an object tells us how it’s stored. It could happen that two different objects are stored in the same mode with different classes.\n\nFor vectors the class and mode will always be numeric, logical, or character.\nFor matrices and arrays a class is always a matrix or array, but its mode can be numeric, character, or logical.\n\nThe primary purpose of the class() function is to know how different functions, including generic functions, work (e.g. print, or plot). There is a collection of R commands used to assess whether a particular object belongs to a certain class, these start with is.; for example, is.numeric(), is.logical(), is.character(), is.list(), is.factor(), and is.data.frame()"
  },
  {
    "objectID": "objects.html#mixing-objects",
    "href": "objects.html#mixing-objects",
    "title": "1  R Objects",
    "section": "1.3 Mixing Objects",
    "text": "1.3 Mixing Objects\nThis is not allowed! When different objects are mixed in a vector, coercion occurs so that every element in the vector is of the same class.\n\ny &lt;- c(1.7, \"a\")\nclass(y) \n\n[1] \"character\"\n\n\n\ny &lt;- c(TRUE, 2)\nclass(y) \n\n[1] \"numeric\"\n\n\n\ny &lt;- c(\"a\", TRUE)\nclass(y) \n\n[1] \"character\""
  },
  {
    "objectID": "objects.html#explicit-coercion",
    "href": "objects.html#explicit-coercion",
    "title": "1  R Objects",
    "section": "1.4 Explicit Coercion",
    "text": "1.4 Explicit Coercion\nObjects can be explicitly coerced from one class to another using the as. functions, if available.\n\nx &lt;- 0:6 \nclass(x) \n\n[1] \"integer\"\n\n\n\nas.numeric(x)\n\n[1] 0 1 2 3 4 5 6"
  },
  {
    "objectID": "objects.html#names",
    "href": "objects.html#names",
    "title": "1  R Objects",
    "section": "1.5 Names",
    "text": "1.5 Names\nObject names must start with a letter and can only contain letters, numbers, _, and ..\nYou want your object names to be descriptive, so you’ll need to adopt a convention for multiple words.\nExample:\ni_use_snake_case\notherPeopleUseCamelCase\nsome.people.use.periods"
  },
  {
    "objectID": "objects.html#r-workspace",
    "href": "objects.html#r-workspace",
    "title": "1  R Objects",
    "section": "1.6 R Workspace",
    "text": "1.6 R Workspace\nThe objects that you create using R remain in existence until you explicitly delete them or you conclude the session.\n\nTo list all currently defined objects, use ls() or objects().\nTo remove object x, use rm(x). To remove all currently defined objects, use rm(list = ls()).\nTo save all of your existing objects to a file called fname in the current working directory, use save.image(file = \"fname\").\nTo save specific objects (say x and y) use save(x, y, file = \"fname\").\nTo load a set of saved objects use load(file = \"fname\").\nTo save this history to the file fname use savehistory(file = \"fname\") and to load the history file fname use loadhistory(file = \"fname\")."
  },
  {
    "objectID": "objects.html#expressions-and-assignments",
    "href": "objects.html#expressions-and-assignments",
    "title": "1  R Objects",
    "section": "1.7 Expressions and Assignments",
    "text": "1.7 Expressions and Assignments\nIn R an expression is used to denote a phrase of code that can be executed.\nExample:\n## An expression\nseq(10, 20, 3)\nThe combination of expressions that are saved for evaluation is called an assignment:\n## An assignment\nobject_name &lt;- value\nWhen reading that code, say “object name gets value” in your head. You will make lots of assignments, and &lt;- is a pain to type.\nYou can save time with RStudio’s keyboard shortcut: Alt + - (the minus sign)."
  },
  {
    "objectID": "types.html#vectors",
    "href": "types.html#vectors",
    "title": "2  Data Types",
    "section": "2.1 Vectors",
    "text": "2.1 Vectors\nA vector is the most convenient way to store more than one data value.\nA vector is a contiguous cell that contains data, where each cell can be accessed by an index. In other words, a vector is an indexed set of objects.\nAll the elements of an atomic vector have to be of the same type —numeric, character, or logical— which is called the mode of the vector.\n\n2.1.1 How to create a vector?\nThere are many ways to create a vector, but these are four basic functions for constructing vectors:\n\nThe c() (combine) function can be used to create vectors of objects by concatenating things together.\nExamples:\n\nx &lt;- c(0.5, 0.6) \nclass(x) \n\n[1] \"numeric\"\n\n\n\nx &lt;- c(TRUE, FALSE) \nclass(x) \n\n[1] \"logical\"\n\n\n\nx &lt;- c(T, F)  \nclass(x) \n\n[1] \"logical\"\n\n\n\nx &lt;- c(\"a\", \"b\", \"c\")   \nclass(x) \n\n[1] \"character\"\n\n\n\nx &lt;- 9:29   \nclass(x) \n\n[1] \"integer\"\n\n\n\nx &lt;- c(1+0i, 2+4i)  \nclass(x) \n\n[1] \"complex\"\n\n\nseq(from, to, by):\n\n(x &lt;- seq(1, 20, 2))\n\n [1]  1  3  5  7  9 11 13 15 17 19\n\n\nrep(x, times):\n\n(y &lt;- rep(3, 4))\n\n[1] 3 3 3 3\n\n\nYou can also use the vector() function to initialize vectors:\nExample:\n\nx &lt;- vector(\"numeric\", length = 10) \nclass(x) \n\n[1] \"numeric\"\n\n\n\n\n\n2.1.2 Add a value to a variable\nExample:\nx[4] &lt;- 9\n\n\n2.1.3 Add names to a vector\n\n# Create a vector with the name of each element\nnamed.num.vec &lt;- c(x1=1, x2=3, x3=5) \nnamed.num.vec \n\nx1 x2 x3 \n 1  3  5 \n\n\nThis is another option to add names:\nnames(x) &lt;- c(\"a\", \"b\", \"c)\n\n\n2.1.4 length()\nThe function length(x) gives the number of elements of ‘x’.\n\nx &lt;- 100:100\nlength(x)\n\n[1] 1\n\n\nIt is possible to have a vector with no elements\n\nx &lt;- c()\nlength(x)\n\n[1] 0"
  },
  {
    "objectID": "types.html#factors",
    "href": "types.html#factors",
    "title": "2  Data Types",
    "section": "2.2 Factors",
    "text": "2.2 Factors\nStatisticians typically recognise three basic types of variable: numeric, ordinal, and categorical. In R the data type for ordinal and categorical vectors is factor. The possible values of a factor are referred to as its levels.\nIn practice, a factor is not much different from a character vector, except that the elements of a factor can take only a limited number of values (of which R keeps a record), and in statistical routines R is able to treat a factor differently than a character vector.\nTo create a factor we apply the function factor() to some vector x. By default the distinct values of x become the levels, or we can specify them using the optional levels argument.\nExample:\n\nx &lt;- factor(c(\"yes\", \"yes\", \"no\", \"yes\", \"no\")) \ntable(x)\n\nx\n no yes \n  2   3 \n\nlevels(x)\n\n[1] \"no\"  \"yes\"\n\n\nNote the use of the function table() to calculate the number of times each level of the factor appears. table() can be applied to other modes of vectors as well as factors. The output of the table() function is a one-dimensional array (as opposed to a vector). If more than one vector is passed to table(), then it produces a multidimensional array.\nThe order of the levels of a factor can be set using the levels argument to factor(). By default R arranges the levels of a factor alphabetically. If you specify the levels yourself, then R uses the ordering that you provide.\nExample:\n\nx &lt;- factor(c(\"yes\", \"yes\", \"no\", \"yes\", \"no\"), levels = c(\"yes\", \"no\"), ordered=TRUE) \nx\n\n[1] yes yes no  yes no \nLevels: yes &lt; no\n\n\nUsing factors with labels is better than using integers because factors are self-describing.\nExample:\nphys.act &lt;- factor(phys.act, levels = c(\"L\", \"M\", \"H\"),\n  labels = c(\"Low\", \"Medium\", \"High\"),\n  ordered = TRUE)\nWe check whether or not an object x is a factor using is.factor(x).\nExample:\n\nis.factor(x)\n\n[1] TRUE\n\n\nUsually it is convenient to transform a numeric variable into a data.frame:\nairquality &lt;- transform(airquality, Month = factor(Month))\ncut() is a generic command to create factor variables from numeric variables:\nExample:\n\nnumvar &lt;- rnorm(100) \nnum2factor &lt;- cut(numvar, breaks=5) ## the levels are produced using the actual range of values\nnum2factor\n\n  [1] (-0.363,0.495] (-1.22,-0.363] (-1.22,-0.363] (-0.363,0.495] (0.495,1.35]  \n  [6] (0.495,1.35]   (-0.363,0.495] (-0.363,0.495] (-1.22,-0.363] (0.495,1.35]  \n [11] (-1.22,-0.363] (-1.22,-0.363] (-0.363,0.495] (0.495,1.35]   (-0.363,0.495]\n [16] (-1.22,-0.363] (1.35,2.21]    (-0.363,0.495] (-0.363,0.495] (-2.08,-1.22] \n [21] (-1.22,-0.363] (0.495,1.35]   (0.495,1.35]   (1.35,2.21]    (-0.363,0.495]\n [26] (0.495,1.35]   (-0.363,0.495] (-0.363,0.495] (0.495,1.35]   (-1.22,-0.363]\n [31] (0.495,1.35]   (0.495,1.35]   (-0.363,0.495] (1.35,2.21]    (-0.363,0.495]\n [36] (-2.08,-1.22]  (-1.22,-0.363] (-0.363,0.495] (-0.363,0.495] (-0.363,0.495]\n [41] (0.495,1.35]   (-2.08,-1.22]  (0.495,1.35]   (1.35,2.21]    (-1.22,-0.363]\n [46] (0.495,1.35]   (-1.22,-0.363] (-0.363,0.495] (-0.363,0.495] (-1.22,-0.363]\n [51] (-0.363,0.495] (-0.363,0.495] (-1.22,-0.363] (0.495,1.35]   (-0.363,0.495]\n [56] (-2.08,-1.22]  (-0.363,0.495] (0.495,1.35]   (-0.363,0.495] (-1.22,-0.363]\n [61] (0.495,1.35]   (-0.363,0.495] (-1.22,-0.363] (1.35,2.21]    (1.35,2.21]   \n [66] (-0.363,0.495] (-1.22,-0.363] (-0.363,0.495] (-1.22,-0.363] (0.495,1.35]  \n [71] (1.35,2.21]    (0.495,1.35]   (1.35,2.21]    (-1.22,-0.363] (-0.363,0.495]\n [76] (-1.22,-0.363] (-1.22,-0.363] (-0.363,0.495] (-0.363,0.495] (1.35,2.21]   \n [81] (1.35,2.21]    (-1.22,-0.363] (0.495,1.35]   (-1.22,-0.363] (0.495,1.35]  \n [86] (0.495,1.35]   (0.495,1.35]   (-1.22,-0.363] (-2.08,-1.22]  (0.495,1.35]  \n [91] (-1.22,-0.363] (-1.22,-0.363] (-1.22,-0.363] (-0.363,0.495] (-1.22,-0.363]\n [96] (-0.363,0.495] (-0.363,0.495] (1.35,2.21]    (-0.363,0.495] (-0.363,0.495]\n5 Levels: (-2.08,-1.22] (-1.22,-0.363] (-0.363,0.495] ... (1.35,2.21]\n\n\n\nnum2factor &lt;- cut(numvar, breaks=5, labels= c(\"lowest group\", \"lower middle group\", \"middle group\", \"upper middle\", \"highest group\"))\ndata.frame(table(num2factor)) ## displaying the data in tabular form\n\n          num2factor Freq\n1       lowest group    5\n2 lower middle group   27\n3       middle group   34\n4       upper middle   23\n5      highest group   11"
  },
  {
    "objectID": "types.html#matrices",
    "href": "types.html#matrices",
    "title": "2  Data Types",
    "section": "2.3 Matrices",
    "text": "2.3 Matrices\nMatrices are stored as vectors with an added dimension attribute. The dimension attribute is itself an integer vector of length 2, which gives the number of rows and columns.\nThe matrix elements are stored column-wise in the vector. This means that it is possible to access the matrix elements using a single index:\n\n(A &lt;- matrix(c(3,5,2,3), 2, 2))\n\n     [,1] [,2]\n[1,]    3    2\n[2,]    5    3\n\nA[2]\n\n[1] 5\n\nA[,2]\n\n[1] 2 3\n\n\n\n2.3.1 How to create a matrix?\nMatrices are constructed column-wise, so entries can be thought of starting in the “upper left” corner and running down the columns.\nExample:\n\n(m &lt;- matrix(1:6, nrow = 2, ncol = 3, byrow = FALSE))\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\ndim(m) \n\n[1] 2 3\n\n\nMatrices can be created by column-binding or row-binding with the cbind() and rbind() functions:\nExamples:\n\nx &lt;- 1:3\ny &lt;- 10:12\ncbind(x, y)\n\n     x  y\n[1,] 1 10\n[2,] 2 11\n[3,] 3 12\n\nrbind(x, y)\n\n  [,1] [,2] [,3]\nx    1    2    3\ny   10   11   12\n\n\nMatrices can also be created directly from vectors by adding a dimension attribute:\nExample:\n\nm &lt;- 1:10\ndim(m) &lt;- c(2, 5)\nm\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    5    7    9\n[2,]    2    4    6    8   10\n\n\nTo create a matrixA with one column from a vector x, we use:\nA &lt;- as.matrix(x)\nTo create a vector from a matrix A, we use:\nx &lt;- as.vector(A)\n\n\n2.3.2 Create a diagonal matrix\nTo create a diagonal matrix use diag(x):\n\nB &lt;- diag(c(1,2,3))\n\n\n\n2.3.3 Add names to matrices\nMatrices can have names:\nExample:\ndimnames(m) &lt;- list(c(\"a\", \"b\"), c(\"c\", \"d\", \"e\")) \nm\nThese are other options for column and row names:\ncolnames(m) &lt;- c(\"c\", \"d\", \"e\")\nrownames(m) &lt;- c(\"a\", \"b\")\n\n\n2.3.4 Operations with matrices\nTo perform matrix multiplication we use the operator %*%. Remember that * acts element wise on matrices.\nOther functions for using with matrices are:\n\nnrow(x)\nncol(x)\ndet(x) (the determinant)\nt(x) (the transpose)\nsolve(A, B) (returns x such that A %*% x == B).\nIf A is invertible then solve(A) returns the matrix inverse of A."
  },
  {
    "objectID": "types.html#data-frames",
    "href": "types.html#data-frames",
    "title": "2  Data Types",
    "section": "2.4 Data Frames",
    "text": "2.4 Data Frames\nIt is a list of vectors restricted to be of equal length. Each vector —or column— corresponds to a variable in an experiment, and each row corresponds to a single observation or experimental unit. Each vector can be of any of the basic modes of object.\nThe dataframe is like a matrix but extended to allow for different object modes in different columns. Unlike matrices, data frames can store different classes of objects in each column (matrices must have every element be the same class, e.g. all integers or all numeric). Obviously to work with datasets from real experiments we need a way to group data of differing modes.\nData frames are used to store tabular data in R. Data frames are represented as a special type of list where every element of the list has to have the same length. Each element of the list can be thought of as a column and the length of each element of the list is the number of rows.\nData frames are usually created by:\n\nreading in a dataset using the read.table() or read.csv()\ncreating a dataframe with data.frame():\nExample:\n\n(x &lt;- data.frame(foo = 1:4, bar = c(T, T, F, F)))\n\n  foo   bar\n1   1  TRUE\n2   2  TRUE\n3   3 FALSE\n4   4 FALSE\n\n# To summarise the structure of a list (or dataframe), use str()\nstr(x)\n\n'data.frame':   4 obs. of  2 variables:\n $ foo: int  1 2 3 4\n $ bar: logi  TRUE TRUE FALSE FALSE\n\n\ncoerced from other types of objects like lists:\nx &lt;- as.data.frame(x)\n\nDataframes can be converted to a matrix by calling data.matrix().\nThe dplyr package has an optimized set of functions designed to work efficiently with dataframes.\n\n2.4.1 Columns and Rows\nYou can construct a dataframe from a collection of vectors and/or existing dataframes using the function data.frame, which has the form: data.frame(col1 = x1, col2 = x2, ..., df1, df2, ...). Here col1, col2, etc., are the column names (given as character strings without quotes) and x1, x2, etc., are vectors of equal length. df1, df2, etc., are dataframes, whose columns must be the same length as the vectors x1, x2, etc. Column names may be omitted, in which case R will choose a name for you.\nColumn names indicate the names of the variables or predictors names(). We can also create a new variable within a dataframe, by naming it and assigning it a value:\nExample:\nufc$volume.m3 &lt;- pi * (ufc$dbh.cm / 200)^2 * ufc$height.m / 2\nEquivalently one could assign to ufc[6] or ufc[\"volume.m3\"] or ufc[[6]] or ufc[[\"volume.m3\"]].\nThe command names(df) will return the names of the dataframe df as a vector of character strings.\nExample:\nufc.names &lt;- names(ufc)\n# To change the names of df you pass a vector of character strings to `names(df)`\nnames(ufc) &lt;- c(\"P\", \"T\", \"S\", \"D\", \"H\", \"V\")\nWhen you create dataframes and any one of the column’s classes is a character, it automatically gets converted to factor, which is a default R operation. However, there is one argument, stringsAsFactors=FALSE, that allows us to prevent the automatic conversion of character to factor during data frame creation.\nDataframes have a special attribute called row.names() which indicate information about each row of the data frame. You can change the row names of df by making an assignment to row.names(df).\n\n\n2.4.2 subset()\nThe function subset() is a convenient tool for selecting the rows of a dataframe, especially when combined with the operator %in%.\nExample:\n# Suppose you are only interested in the height of trees of species DF (Douglas Fir) or GF (Grand Fir)\nfir.height &lt;- subset(ufc, subset = species %in% c(\"DF\", \"GF\"),\n                    select = c(plot, tree, height.m))\nhead(fir.height)\n\n\n2.4.3 attach()\nR allows you to attach a dataframe to the workspace. When attached, the variables in the dataframe can be referred to without being prefixed by the name of the dataframe.\nExample:\nattach(ufc)\nmax(height.m[species == \"GF\"])\nWhen you attach a dataframe R actually makes a copy of each variable, which is deleted when the dataframe is detached. Thus, if you change an attached variable you do not change the dataframe. After we use the attach() command, we need to use detach() to remove individual variables from the working environment.\nNonetheless, note that the with() and transform()functions provide a safer alternative."
  },
  {
    "objectID": "types.html#lists",
    "href": "types.html#lists",
    "title": "2  Data Types",
    "section": "2.5 Lists",
    "text": "2.5 Lists\nLists are a special type of vector that can contain elements of different type (we can store single constants, vectors of numeric values, factors, data frames, matrices, and even arrays), namely, a list is a general data storage object that can house pretty much any other kind of R object.\nLike a vector, a list is an indexed set of objects (and so has a length), but unlike a vector the elements of a list can be of different types, including other lists! The mode of a list is list.\nThe power and utility of lists comes from this generality. A list might contain an individual measurement, a vector of observations on a single response variable, a dataframe, or even a list of dataframes containing the results of several experiments.\nIn R lists are often used for collecting and storing complicated function output. Dataframes are special kinds of lists.\n\n2.5.1 How to Create a List?\nLists can be explicitly created using the list() function, which takes an arbitrary number of arguments:\nExample 1:\n\n(x &lt;- list(1, \"a\", TRUE, 1 + 4i))\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] \"a\"\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] 1+4i\n\n\nExample 2:\n\n(my.list &lt;- list(\"one\", TRUE, 3, c(\"f\",\"o\",\"u\",\"r\")))\n\n[[1]]\n[1] \"one\"\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] 3\n\n[[4]]\n[1] \"f\" \"o\" \"u\" \"r\"\n\nmy.list[[2]]\n\n[1] TRUE\n\nmode(my.list[[2]])\n\n[1] \"logical\"\n\nmy.list[[4]][1]\n\n[1] \"f\"\n\nmy.list[4][1]\n\n[[1]]\n[1] \"f\" \"o\" \"u\" \"r\"\n\n\nWe can also create an empty list of a pre-specified length with the vector() function:\nExample:\n\n(x &lt;- vector(mode = \"list\", length = 5)) # the elements are NULL\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n[[5]]\nNULL\n\n\nTo flatten a list x, that is convert it to a vector, we use unlist(x).\nExample:\n\nx &lt;- list(1, c(2, 3), c(4, 5, 6))\nunlist(x)\n\n[1] 1 2 3 4 5 6\n\n\nMany functions produce list objects as their output. For example, when we fit a least squares regression, the regression object itself is a list, and can be manipulated using list operations.\nExample:\n\nlm.xy &lt;- lm(y ~ x, data = data.frame(x = 1:5, y = 1:5))\nmode(lm.xy)\n\n[1] \"list\"\n\nnames(lm.xy)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n\n\n\n2.5.2 Names\nThe elements of a list can be named when the list is created, using arguments of the form name1 = x1, name2 = x2, etc., or they can be named later by assigning a value to the names attribute.\nExample:\n\nmy.list &lt;- list(first = \"one\", second = TRUE, third = 3,\n                fourth = c(\"f\",\"o\",\"u\",\"r\"))\nnames(my.list)\n\n[1] \"first\"  \"second\" \"third\"  \"fourth\"\n\n\nUnlike a dataframe, the elements of a list do not have to be named. Names can be used (within quotes) when indexing with single or double square brackets, or they can be used (with or without quotes) after a dollar sign to extract a list element."
  },
  {
    "objectID": "types.html#arrays",
    "href": "types.html#arrays",
    "title": "2  Data Types",
    "section": "2.6 Arrays",
    "text": "2.6 Arrays\nSometimes, you need to store multiple matrices or data frames into a single object; in this case, we can use arrays to store this data.\nData frames and matrices are of two dimensions only, but an array can be of any number of dimensions.\nHere is a simple example to store three matrices of order 2 x 2 in a single array object:\nExamples:\n\n(mat.array &lt;- array(dim=c(2,2,3)))\n\n, , 1\n\n     [,1] [,2]\n[1,]   NA   NA\n[2,]   NA   NA\n\n, , 2\n\n     [,1] [,2]\n[1,]   NA   NA\n[2,]   NA   NA\n\n, , 3\n\n     [,1] [,2]\n[1,]   NA   NA\n[2,]   NA   NA\n\n(mat.array[,,1] &lt;- rnorm(4))\n\n[1]  0.2901673 -0.8170549  1.7245399  0.7556317\n\n(mat.array[,,1] &lt;- rnorm(4))\n\n[1] -1.56335160 -0.14797761  0.46545173  0.08769665\n\n(mat.array[,,2] &lt;- rnorm(4))\n\n[1] -0.09392193 -0.19469974 -0.57751827  0.20679941\n\n(mat.array[,,3] &lt;- rnorm(4))\n\n[1] -1.8386513 -1.4426650 -0.1492357 -0.6911807"
  },
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "3  Packages",
    "section": "",
    "text": "A package is an archive of files that conforms to a certain format and structure and that provides extra functionality, usually extending R in a particular direction. The R community has produced many high-quality R packages for performing specific tasks,\nAny package is in one of three states:\n\nInstalled and loaded. A package that is loaded is directly available to your R session. Find out which packages are loaded using sessionInfo().\nInstalled but not loaded. A package that is installed is available for loading but its contents are not available until it is loaded. The function help.start() gives details of the packages that are installed on your computer.\nNot installed. These packages cannot be loaded. If a package is not installed then the library() function produces an error. If the install status is uncertain at the time of calling library (for example if you are writing a function that requires the package), then use the require() function, which returns FALSE if the package is not installed, rather than an error.\n\nInstalling all available packages would be a waste of space and time, as you would never use most of them. Similarly, loading all installed packages every time you start R would take some time, so by default R only loads the base packages when it starts and requires the user to load any others as and when they are needed.\nPackages are divided into three groups:\n\nBase. Base packages are installed along with R, and their objects are always available.\nRecommended. Recommended packages are installed along with R but must be loaded before they can be used.\nOther. Other packages are not installed by default, and must be installed separately.\n\nThe command to find out what packages are available for loading is installed.packages. The output of the function is quite verbose, but we only need the first column:\nExample:\n\ninstalled.packages()[1:5, 1] # Returns only the first five packages\n\n    askpass   backports   base64enc         bit       bit64 \n  \"askpass\" \"backports\" \"base64enc\"       \"bit\"     \"bit64\" \n\n\nAll the packages that are available at a repository, and whose requirements are matched by the currently running version of R, can be listed using the command available.packages(). A package that is available in the repository but has not yet been installed may be installed using the install.packages() function. If we include the argument dependencies = TRUE, then the function will also install packages that are necessary to run the package or packages of interest; such packages are called dependencies.\nThe status of the packages that are installed can be compared with the repository using the old.packages() function, and easily updated using the update.packages() function.\nR can be easily updated with installr package. To update R on MacOS, you need to use updateR” package instead.\nExample:\ninstall.packages(\"installr\")\nlibrary(installr)\nupdateR()"
  },
  {
    "objectID": "style.html#names",
    "href": "style.html#names",
    "title": "4  Code Style",
    "section": "4.1 Names",
    "text": "4.1 Names\nRemember that variable names should use only lowercase letters, numbers, and _. Use _to separate words within a name.\nAs a general rule of thumb, it’s better to prefer long, descriptive names that are easy to understand rather than concise names that are fast to type.\nIn general, if you have a bunch of variables that are a variation on a theme, you’re better off giving them a common prefix rather than a common suffix because autocomplete works best on the start of a variable."
  },
  {
    "objectID": "style.html#spaces",
    "href": "style.html#spaces",
    "title": "4  Code Style",
    "section": "4.2 Spaces",
    "text": "4.2 Spaces\nPut spaces on either side of mathematical operators apart from ^ (i.e. +, -, ==, &lt;, …), and around the assignment operator (&lt;-).\nExample:\n# Strive for\nz &lt;- (a + b)^2 / d\n\n# Avoid\nz&lt;-( a + b ) ^ 2/d\nDon’t put spaces inside or outside parentheses for regular function calls. Always put a space after a comma, just like in standard English.\n# Strive for\nmean(x, na.rm = TRUE)\n\n# Avoid\nmean (x ,na.rm=TRUE)"
  },
  {
    "objectID": "style.html#pipes",
    "href": "style.html#pipes",
    "title": "4  Code Style",
    "section": "4.3 Pipes",
    "text": "4.3 Pipes\n|&gt; should always have a space before it and should typically be the last thing on a line. This makes it easier to add new steps, rearrange existing steps, modify elements within a step, and get a sky view by skimming the verbs on the left-hand side.\n# Strive for \nflights |&gt;  \n  filter(!is.na(arr_delay), !is.na(tailnum)) |&gt; \n  count(dest)\n\n# Avoid\nflights|&gt;filter(!is.na(arr_delay), !is.na(tailnum))|&gt;count(dest)\nIf the function you’re piping into has named arguments (like mutate() or summarize()), put each argument on a new line. If the function doesn’t have named arguments (like select() or filter()), keep everything on one line unless it doesn’t fit, in which case you should put each argument on its own line.\n# Strive for\nflights |&gt;  \n  group_by(tailnum) |&gt; \n  summarize(\n    delay = mean(arr_delay, na.rm = TRUE),\n    n = n()\n  )\n\n# Avoid\nflights |&gt;\n  group_by(\n    tailnum\n  ) |&gt; \n  summarize(delay = mean(arr_delay, na.rm = TRUE), n = n())\nAfter the first step of the pipeline, indent each line by two spaces. RStudio will automatically put the spaces in for you after a line break following a |&gt; . If you’re putting each argument on its own line, indent by an extra two spaces. Make sure )is on its own line, and un-indented to match the horizontal position of the function name.\n# Strive for \nflights |&gt;  \n  group_by(tailnum) |&gt; \n  summarize(\n    delay = mean(arr_delay, na.rm = TRUE),\n    n = n()\n  )\nBe wary of writing very long pipes, say longer than 10-15 lines. Try to break them up into smaller sub-tasks, giving each task an informative name.\nThe same basic rules that apply to the pipe also apply to ggplot2; just treat + the same way as |&gt;."
  },
  {
    "objectID": "reading.html#tabular-data",
    "href": "reading.html#tabular-data",
    "title": "5  Reading Data",
    "section": "5.1 Tabular Data",
    "text": "5.1 Tabular Data\nIt is common for data to be arranged in tables, with columns corresponding to variables and rows corresponding to separate observations. These dataframes are usually read into R using the function read.table(), which has the form:\nread.table(file, header = FALSE, sep = \"\") \n# read.table() returns a dataframe\nread_table() reads a common variation of fixed-width files where columns are separated by white space.\nThere are two commonly used variants of read.table():\n\n5.1.1 read.csv()\nread.csv() is for comma-separated data and is equivalent to read.table(file, header = TRUE, sep = \",\").\nSometimes, it could happen that the file extension is .csv, but the data is not comma separated. In that case, we can still use the read.csv() function, but in this case we have to specify the separator:\nExample:\nread.csv(\"iris_semicolon.csv\", stringsAsFactors = FALSE, sep=\";\")\nanscombe_tab_2 &lt;- read.table(\"anscombe.txt\", header=TRUE)\nUsually, read_csv() uses the first line of the data for the column names, which is a very common convention. But it’s not uncommon for a few lines of metadata to be included at the top of the file. You can use skip = n to skip the first n lines or use comment = \"#\" to drop all lines that start with (e.g.) #.\nExample:\nread_csv(\"data/students.csv,\n  skip = 2,\n  comment = \"#\"\n)\nIn other cases, the data might not have column names. You can use col_names = FALSE to tell read_csv() not to treat the first row as headings and instead label them sequentially from \\(X_1\\) to \\(X_n\\). Alternatively, you can pass col_names a character vector which will be used as the column names (e.g. col_names = c(\"x\", \"y\", \"z\")):\nNote that by default read_csv() only recognizes empty strings (““) in as NAs, however you surely want it to also recognize the character string “N/A”.\nExample:\nstudents &lt;- read_csv(\"data/students.csv\", na = c(\"N/A\", \"\"))\nIf a .csv file contains both numeric and character variables, and we use read.csv(), the character variables get automatically converted to the factor type. We can prevent character variables from this automatic conversion to factor, by specifying stringsAsFactors=FALSE within the read.csv() function.\nExample:\nread.csv(\"iris.csv\", stringsAsFactors=F)\nreadr provides a total of nine column types for you to use: col_logical(), col_logical(), col_factor()… Find more information here.\n\n\n5.1.2 read.delim()\nread_delim() reads in files with any delimiter, attempting to automatically guess the delimiter if you don’t specify it. For example, read.delim()is for tab-delimited data and is equivalent to read.table(file, header = TRUE, sep = \"\\t\").\nExample:\n## skips the first 2 lines\nanscombe &lt;- read.csv(\"CSVanscombe.csv\", skip=2) \n\n\n5.1.3 Other file types\nOnce you’ve mastered read_csv(), using readr’s other functions is straightforward; it’s just a matter of knowing which function to reach for:\n\nread_csv2(): reads semicolon-separated files. These use ; instead of , to separate fields and are common in countries that use , as the decimal marker.\nread_tsv(): reads tab-delimited files.\nread_fwf(): reads fixed-width files. You can specify fields by their widths with fwf_widths() or by their positions with fwf_positions().\nread_log(): reads Apache-style log files."
  },
  {
    "objectID": "reading.html#reading-multiple-files",
    "href": "reading.html#reading-multiple-files",
    "title": "5  Reading Data",
    "section": "5.2 Reading Multiple Files",
    "text": "5.2 Reading Multiple Files\nSometimes your data is split across multiple files instead of being contained in a single file. For example, you might have sales data for multiple months, with each month’s data in a separate file: 01-sales.csv for January, 02-sales.csv for February, and 03-sales.csv for March.\nWith read_csv()you can read these data in at once and stack them on top of each other in a single data frame.\nExample:\nsales_files &lt;- c(\"data/01-sales.csv\", \"data/02-sales.csv\", \"data/03-sales.csv\")\nread_csv(sales_files, id = \"file\")\nThe id argument adds a new column called file to the resulting data frame that identifies the file the data come from. This is especially helpful in circumstances where the files you’re reading in do not have an identifying column that can help you trace the observations back to their original sources.\nAn alternative to read multiple data made available online is this:\nExample:\nsales_files &lt;- c(\n  \"https://pos.it/r4ds-01-sales\",\n  \"https://pos.it/r4ds-02-sales\",\n  \"https://pos.it/r4ds-03-sales\"\n)\nread_csv(sales_files, id = \"file\")"
  },
  {
    "objectID": "reading.html#reading-line-by-line",
    "href": "reading.html#reading-line-by-line",
    "title": "5  Reading Data",
    "section": "5.3 Reading Line by Line",
    "text": "5.3 Reading Line by Line\nText files can be read line by line using the readLines() function:\nExample:\ncon &lt;- gzfile(\"words.gz\") \nx &lt;- readLines(con, 10)\nThis approach is useful because it allows you to read from a file without having to uncompress the file first, which would be a waste of space and time.\nTo read in lines of webpages it can be useful the readLines() function:\nExample:\ncon &lt;- url(\"http://www.jhsph.edu\", \"r\") ## Read the web page \nx &lt;- readLines(con) ## Print out the first few lines head(x)"
  },
  {
    "objectID": "reading.html#xls-format",
    "href": "reading.html#xls-format",
    "title": "5  Reading Data",
    "section": "5.4 xls Format",
    "text": "5.4 xls Format\nIf the dataset is stored in the .xls or .xlsx format you can use the readxl package. This package is non-core tidyverse, so you need to load it explicitly, but it is installed automatically when you install the tidyverse package.\nMost of readxl’s functions allow you to load Excel spreadsheets into R:\n\nread_xls() reads Excel files with xls format.\nread_xlsx() read Excel files with xlsx format.\nread_excel() can read files with both xls and xlsx format. It guesses the file type based on the input.\n\nExample:\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(writexl)\nstudents &lt;- read_excel(\"data/students.xlsx\")\nstr(students)"
  },
  {
    "objectID": "reading.html#rdata-format",
    "href": "reading.html#rdata-format",
    "title": "5  Reading Data",
    "section": "5.6 RData Format",
    "text": "5.6 RData Format\nIf you need to store more than one dataset in a single file we can use the *.RData format:\nExample:\n## example to load multiple datasets, and a vector of R objects from a single *.RData\nload(\"robjects.RData\")\n## to check if objects have been loaded correctly\nobjects()"
  },
  {
    "objectID": "reading.html#import-stataspss-file",
    "href": "reading.html#import-stataspss-file",
    "title": "5  Reading Data",
    "section": "5.7 Import Stata/SPSS File",
    "text": "5.7 Import Stata/SPSS File\nTo import a Stata/SPSS file into R:\n\nFirst you need to call the foreign library:\ninstall.packages(\"foreign\")\nThen use read.data()/read.spss():\ndata &lt;- read.spss(file=\"data.spss\", to.data.frame=TRUE)\nThe output will always be a data frame:\nwrite.foreign(data, \"mydata.txt\", \"mydata.sps\", package=\"SPSS\")"
  },
  {
    "objectID": "reading.html#json-file",
    "href": "reading.html#json-file",
    "title": "5  Reading Data",
    "section": "5.8 JSON File",
    "text": "5.8 JSON File\nTo read a JSON file:\ninstall.packages(\"rjson\") \ndata &lt;- fromJSON(file=\"data.json\")\ndata2 &lt;- as.data.frame(data)"
  },
  {
    "objectID": "reading.html#view",
    "href": "reading.html#view",
    "title": "5  Reading Data",
    "section": "5.9 view()",
    "text": "5.9 view()\nTo view the data variable, you can use the view()."
  },
  {
    "objectID": "reading.html#manual-data-entry",
    "href": "reading.html#manual-data-entry",
    "title": "5  Reading Data",
    "section": "5.10 Manual Data Entry",
    "text": "5.10 Manual Data Entry\nSometimes you’ll need to assemble a tibble “by hand” doing a little data entry in your R script.\nThere are two useful functions to help you do this which differ in whether you layout the tibble by columns or by rows.\n\ntibble() works by column:\nExample:\n\ntidyr::tibble(\n  x = c(1, 2, 5), \n  y = c(\"h\", \"m\", \"g\"),\n  z = c(0.08, 0.83, 0.60)\n)   \n\n# A tibble: 3 × 3\n      x y         z\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 h      0.08\n2     2 m      0.83\n3     5 g      0.6 \n\n\ntribble() (transposed tibble) lets you lay out your data row by row: column headings start with ~ and entries are separated by commas. This makes it possible to lay out small amounts of data in an easy to read form:\nExample:\n\ntidyr::tribble(\n  ~x, ~y, ~z,\n  1, \"h\", 0.08,\n  2, \"m\", 0.83,\n  5, \"g\", 0.60\n)\n\n# A tibble: 3 × 3\n      x y         z\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 h      0.08\n2     2 m      0.83\n3     5 g      0.6"
  },
  {
    "objectID": "writing.html#writing-a-dataframe",
    "href": "writing.html#writing-a-dataframe",
    "title": "6  Writing Data",
    "section": "6.3 Writing a Dataframe",
    "text": "6.3 Writing a Dataframe\nTo write a dataframe to a file we use this:\nwrite.table(x, file = \"\", append = FALSE, sep = \" \", row.names = TRUE, col.names = TRUE)\nHere x is the vector to be written. If x is a matrix or array then it is converted to a vector (column by column) before being written. The other parameters are optional.\nWe can identify the complete rows from a two-dimensional object such as a dataframe (that is, rows that have no missing values) via the complete.cases command. We can easily remove rows with missing values using the na.omit function."
  },
  {
    "objectID": "writing.html#writing-matrices",
    "href": "writing.html#writing-matrices",
    "title": "6  Writing Data",
    "section": "6.4 Writing Matrices",
    "text": "6.4 Writing Matrices\nBecause write() converts matrices to vectors before writing them, using it to write a matrix to a file can cause unexpected results. Since R stores its matrices by column, you should pass the transpose of the matrix to write if you want the output to reflect the matrix structure.\nx &lt;- matrix(1:24, nrow = 4, ncol = 6)\nwrite(t(x), file = \"../results/out.txt\", ncolumns = 6)"
  },
  {
    "objectID": "writing.html#cat",
    "href": "writing.html#cat",
    "title": "6  Writing Data",
    "section": "6.5 cat()",
    "text": "6.5 cat()\nA more flexible command for writing to a file is cat(), which has the form:\ncat(..., file = \"\", sep = \" \", append = FALSE)\nNote that cat does not automatically write a newline after the expressions …. If you want a newline you must explicitly include the string \\n."
  },
  {
    "objectID": "writing.html#tabular-data",
    "href": "writing.html#tabular-data",
    "title": "6  Writing Data",
    "section": "6.6 Tabular Data",
    "text": "6.6 Tabular Data\nFor writing tabular data to text files (i.e. CSV) or connections you may use write.table(), which has this format:\nExample:\nwrite.table()"
  },
  {
    "objectID": "writing.html#dump",
    "href": "writing.html#dump",
    "title": "6  Writing Data",
    "section": "6.7 dump()",
    "text": "6.7 dump()\nThere is also the very useful dump() function, which creates a text representation of almost any R object that can subsequently be read by source.\nExample:\nx &lt;- matrix(rep(1:5, 1:5), nrow = 3, ncol = 5)\ndump(\"x\", file = \"../results/x.txt\")\nrm(x)\nsource(\"../results/x.txt\")\nx"
  },
  {
    "objectID": "transforming.html#headandtail",
    "href": "transforming.html#headandtail",
    "title": "7  Transforming Data",
    "section": "7.1 ’head()andtail()`",
    "text": "7.1 ’head()andtail()`\nAfter reading the file, you can use the head() and tail() functions to examine the object:\nExample:\nhead(dataset)\ntail()"
  },
  {
    "objectID": "transforming.html#changing-variable-types",
    "href": "transforming.html#changing-variable-types",
    "title": "7  Transforming Data",
    "section": "7.2 Changing Variable Types",
    "text": "7.2 Changing Variable Types\nA common task after reading in data is to consider variable types. For example, if a variable in a dataset is a categorical variable with a known set of possible values but you see it is represented as a character, then you will want to make it a factor. Or a numeric variable is a character variable because one of the observations is typed out as a character instead of a numeric (e.g. “five” instead of 5).\nExample:\nstudents |&gt;\n  mutate(meal_plan = factor(meal_plan),\n  age = parse_number(if_else(age == \"five\", \"5\", age))\nAfter this, you will that the type of variable denoted underneath the variable name has changed from character () to factor ()."
  },
  {
    "objectID": "transforming.html#changing-columns-names",
    "href": "transforming.html#changing-columns-names",
    "title": "7  Transforming Data",
    "section": "7.3 Changing Columns Names",
    "text": "7.3 Changing Columns Names\nNote that you can assign your own column names after reading a dataframe using the names() function, or when you read it in, using the col.names argument, which should be assigned a character vector the same length as the number of columns.\nIf there is no header and no col.names argument, then R uses the names V1, V2, etc."
  },
  {
    "objectID": "transforming.html#columns-no-syntatic-names",
    "href": "transforming.html#columns-no-syntatic-names",
    "title": "7  Transforming Data",
    "section": "7.4 Columns No-Syntatic Names",
    "text": "7.4 Columns No-Syntatic Names\nSometimes, you might also notice that the columns names are surrounded by backticks. That’s because they contain spaces, breaking R’s usual rules for variable names; they’re non-syntactic names.\n*Example:*\n\n``` R\nstudents |&gt; \n  rename(\n    student_id = `Student ID`,\n    full_name = `Full Name`\n  )\n```\nAn alternative approach is to use janitor::clean_names() to use some heuristics to turn them all into snake case at once1.\nstudents |&gt; janitor::clean_names()"
  },
  {
    "objectID": "transforming.html#removing-duplicates",
    "href": "transforming.html#removing-duplicates",
    "title": "7  Transforming Data",
    "section": "7.5 Removing Duplicates",
    "text": "7.5 Removing Duplicates\nYou can remove duplicates based on the x variable using:\nx &lt;- c(1, 2, NA, 4, NA, 5) \ndata[!duplicated(data$x), ]"
  },
  {
    "objectID": "transforming.html#reshaping-datasets",
    "href": "transforming.html#reshaping-datasets",
    "title": "7  Transforming Data",
    "section": "7.6 Reshaping Datasets",
    "text": "7.6 Reshaping Datasets\nStatistical analysis sometimes requires wide data and sometimes long data. In such cases, we need to be able to fluently and fluidly reshape the data to meet the requirements of statistical analysis. The function reshape() reshapes a dataframe between ‘wide’ format (with repeated measurements in separate columns of the same row) and ‘long’ format (with the repeated measurements in separate rows).\nData reshaping is just a rearrangement of the form of the data—it does not change the content of the dataset.\nExample:\nstudents &lt;- data.frame(sid=c(1,1,2,2), exmterm=c(1,2,1,2), math=c(50,65,75,69), \nliterature=c(40,45,55,59), language=c(70,80,75,78))\n\n# Reshaping dataset using reshape function to wide format\nwide_students &lt;- reshape(students, direction=\"wide\", idvar=\"sid\", timevar=\"exmterm\")\n\n# Now again reshape to long format\nlong_students &lt;- reshape (wide_students, direction=\"long\", idvar=\"id\")\n\n7.6.1 The reshape package\n\nMelting data (molten data):\n\nThough melting can be applied to different R objects, the most common use is to melt a data frame.\nTo perform melting operations using the melt function, we need to know what the identification variables and measured variables in the original input dataset are.\nOne important thing to note is that, whenever we use the melt function, all the measured variables should be of the same type, that is, the measured variables should be either numeric, factor, character, or date.\nTo deal with the implicit missing value, it is good to use na.rm = TRUE with the melt function to remove the structural missing value (i.e., it will fill empty cells in the data table with NA).\n\nExample:\n## the format of the resulting table is id/variable/value\nmelt(students, id=c(\"sid\",\"exmterm\"), measured=c(\"math\", \"literature\", \"language\"))\nCasting molten data:\n\nOnce we have molten data, we can rearrange it in any layout using the cast function from the reshape package.\nThere are two main arguments required to cast molten data: data and formula.\nThe basic casting formula is col_var_1+col_var_2 ~ row_var_1+ row_var_2, which describes the variables to appear in columns and rows.\n\nExample:\n# to return to the original data structure  \ncast(molten_students, sid+exmterm ~ variable)\nFor faster and large data rearrangement, use the reshape2 package and the functions dcast (data frames) and acast (arrays):\nExample:\nacast(molten_students, sid+exmterm~variable)"
  },
  {
    "objectID": "transforming.html#missing-values",
    "href": "transforming.html#missing-values",
    "title": "7  Transforming Data",
    "section": "7.7 Missing Values",
    "text": "7.7 Missing Values\nR represents missing observations through the data value NA. It is easiest to think of NA values as place holders for data that should have been there, but, for some reason, are not.\n\n7.7.1 Detect NA\nWe can detect whether variables are missing value using:\n\nis.na() is used to test objects if they are NA.\nis.nan() is used to test for NaN.\n\nExample:\n\nx &lt;- c(1, 2, NaN, NA, 4) \nis.na(x) \n\n[1] FALSE FALSE  TRUE  TRUE FALSE\n\nis.nan(x)\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\n\nTo check if there is any NA in a dataframe:\nExample:\n\nany(is.na(x))\n\n[1] TRUE\n\n\n\n\n7.7.2 Remove NAs\nExample:\n\na &lt;- c(11, NA, 13)\nmean(a, na.rm = TRUE)\n\n[1] 12\n\n\n\n\n7.7.3 Extract NA from a vector\nExample:\n\nx &lt;- c(1, 2, NA, 3) \nz &lt;- is.na(x) \nx[!z]\n\n[1] 1 2 3\n\n\n\n\n7.7.4 NA vs NULL\nNote that NA and NULL are not equivalent:\n\nNA is a place holder for something that exists but is missing.\nNULL stands for something that never existed at all.\n\n\n\n7.7.5 Removing NA values\nYou can remove rows with NA values in any variables:\nna.omit(data)\n\n## Another example\nx &lt;- c(1, 2, NA, 4, NA, 5) \nbad &lt;- is.na(x) \nx[!bad]\n\n[1] 1 2 4 5\n\n\nHow can you take the subset with no missing values in any of those objects?:\nExample:\ngood &lt;- complete.cases(airquality) \nhead(airquality[good,])"
  },
  {
    "objectID": "tidydata.html#installation",
    "href": "tidydata.html#installation",
    "title": "8  Tidy Data",
    "section": "8.1 Installation",
    "text": "8.1 Installation\n\nInstall all the packages in the tidyverse by running:\ninstall.packages(\"tidyverse\")\nRun library(tidyverse) to load the core tidyverse and make it available in your current R session.\nNote the conflicts message that’s printed when you load the tidyverse. It tells you that dplyr overwrites some functions in base R. If you want to use the base version of these functions after loading dplyr, you’ll need to use their full names: stats::filter() and stats::lag().\nLearn more about the tidyverse package https://tidyverse.tidyverse.org"
  },
  {
    "objectID": "tidydata.html#core-packages",
    "href": "tidydata.html#core-packages",
    "title": "8  Tidy Data",
    "section": "8.2 Core Packages",
    "text": "8.2 Core Packages\nlibrary(tidyverse) will load the core tidyverse packages:\n\nggplot2, for data visualisation, more info.\ndplyr, for data manipulation, more info.\ntidyr, for data tidying, more info\nreadr, for data import, more info\npurrr, for functional programming, more info.\ntibble, for tibbles, a modern re-imagining of data frames, more info.\nstringr, for strings, more info\nforcats, for factors, [more info] (https://forcats.tidyverse.org/)\nlubridate, for date/times."
  },
  {
    "objectID": "tidydata.html#functionalities",
    "href": "tidydata.html#functionalities",
    "title": "8  Tidy Data",
    "section": "8.3 Functionalities",
    "text": "8.3 Functionalities\n\n8.3.1 Import\nAs well as readr, for reading flat files, the tidyverse package installs a number of other packages for reading data:\n\nDBI for relational databases. You’ll need to pair DBI with a database specific backends like RSQLite, RMariaDB, RPostgres, or odbc. More info here.\nhaven for SPSS, Stata, and SAS data.\nhttr for web APIs.\nreadxl for .xls and .xlsx sheets.\ngooglesheets4 for Google Sheets via the Sheets API v4.\ngoogledrive for Google Drive files.\nrvest for web scraping.\njsonlite for JSON. (Maintained by Jeroen Ooms.)\nxml2 for XML.\n\n\n\n8.3.2 Wrangle\nIn addition to tidyr, and dplyr, there are five packages (including stringr and forcats) which are designed to work with specific types of data:\n\nlubridate for dates and date-times.\nhms for time-of-day values.\nblob for storing blob (binary) data.\n\nThere are also two packages that allow you to interface with different backends using the same dplyr syntax:\n\ndbplyr allows you to use remote database tables by converting dplyr code into SQL.\ndtplyr provides a data.table backend by automatically translating to the equivalent, but usually much faster, data.table code. Program\n\n\n\n8.3.3 Programming\nIn addition to purrr, which provides very consistent and natural methods for iterating on R objects, there are two additional tidyverse packages that help with general programming challenges:\n\nmagrittr provides the pipe, %&gt;% used throughout the tidyverse. It also provide a number of more specialised piping operators (like %$% and %&lt;&gt;%) that can be useful in other places.\nglue provides an alternative to paste() that makes it easier to combine data and strings.\n\n\n\n8.3.4 Modeling\nModeling with the tidyverse uses the collection of tidymodels packages, which largely replace the modelr package used in R4DS.\nVisit the Getting Started Guide for more detailed examples, or go straight to the Learn page."
  },
  {
    "objectID": "tidydata.html#tidying-data",
    "href": "tidydata.html#tidying-data",
    "title": "8  Tidy Data",
    "section": "8.4 Tidying Data",
    "text": "8.4 Tidying Data\nWe’ll focus on tidyr, a package that provides a bunch of tools to help tidy up messy datasets. tidyr is a member of the core tidyverse.\nlibrary(tidyverse)\nThere are two main advantages of tidy data:\n\nThere’s a general advantage to picking one consistent way of storing data. If you have a consistent data structure, it’s easier to learn the tools that work with it because they have an underlying uniformity.\nThere’s a specific advantage to placing variables in columns because it allows R’s vectorized nature to shine.\n\n\n8.4.1 Pivot Data\nYou’ll begin by figuring out what the underlying variables and observations are. Sometimes this is easy; other times you’ll need to consult with the people who originally generated the data. Next, you’ll pivot your data into a tidy form, with variables in the columns and observations in the rows.\ntidyr provides two functions for pivoting data: pivot_longer() and pivot_wider(). We’ll first start with pivot_longer() because it’s the most common case. Let’s dive into some examples.\n\n\n8.4.2 Lengthening Data\nLengthening data means increasing the number of rows and decreasing the number of columns. The inverse transformation is pivot_wider().\nIn this dataset, each observation is a song. The first three columns (artist, track and date.entered) are variables that describe the song. Then we have 76 columns (wk1-wk76) that describe the rank of the song in each week1. Here, the column names are one variable (the week) and the cell values are another (the rank).\n\ntidyr::billboard\n\n# A tibble: 317 × 79\n   artist     track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n   &lt;chr&gt;      &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac      Baby… 2000-02-26      87    82    72    77    87    94    99    NA\n 2 2Ge+her    The … 2000-09-02      91    87    92    NA    NA    NA    NA    NA\n 3 3 Doors D… Kryp… 2000-04-08      81    70    68    67    66    57    54    53\n 4 3 Doors D… Loser 2000-10-21      76    76    72    69    67    65    55    59\n 5 504 Boyz   Wobb… 2000-04-15      57    34    25    17    17    31    36    49\n 6 98^0       Give… 2000-08-19      51    39    34    26    26    19     2     2\n 7 A*Teens    Danc… 2000-07-08      97    97    96    95   100    NA    NA    NA\n 8 Aaliyah    I Do… 2000-01-29      84    62    51    41    38    35    35    38\n 9 Aaliyah    Try … 2000-03-18      59    53    38    28    21    18    16    14\n10 Adams, Yo… Open… 2000-08-26      76    76    74    69    68    67    61    58\n# ℹ 307 more rows\n# ℹ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;, …\n\n\nTo tidy this data, we’ll use pivot_longer():\n\ntidyr::billboard |&gt; \n  tidyr::pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE # to get rid of NAs\n  )\n\n# A tibble: 5,307 × 5\n   artist  track                   date.entered week   rank\n   &lt;chr&gt;   &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk1      87\n 2 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk2      82\n 3 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk3      72\n 4 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk4      77\n 5 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk5      87\n 6 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk6      94\n 7 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk7      99\n 8 2Ge+her The Hardest Part Of ... 2000-09-02   wk1      91\n 9 2Ge+her The Hardest Part Of ... 2000-09-02   wk2      87\n10 2Ge+her The Hardest Part Of ... 2000-09-02   wk3      92\n# ℹ 5,297 more rows\n\n\nThis data is now tidy, but we could make future computation a bit easier by converting values of week from character strings to numbers using mutate() and readr::parse_number(). parse_number() is a handy function that will extract the first number from a string, ignoring all other text.\n\nbillboard_longer &lt;- tidyr::billboard |&gt; \n  tidyr::pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE\n  ) |&gt; \n  dplyr::mutate(\n    week = readr::parse_number(week)\n  )\nbillboard_longer\n\n# A tibble: 5,307 × 5\n   artist  track                   date.entered  week  rank\n   &lt;chr&gt;   &lt;chr&gt;                   &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac   Baby Don't Cry (Keep... 2000-02-26       1    87\n 2 2 Pac   Baby Don't Cry (Keep... 2000-02-26       2    82\n 3 2 Pac   Baby Don't Cry (Keep... 2000-02-26       3    72\n 4 2 Pac   Baby Don't Cry (Keep... 2000-02-26       4    77\n 5 2 Pac   Baby Don't Cry (Keep... 2000-02-26       5    87\n 6 2 Pac   Baby Don't Cry (Keep... 2000-02-26       6    94\n 7 2 Pac   Baby Don't Cry (Keep... 2000-02-26       7    99\n 8 2Ge+her The Hardest Part Of ... 2000-09-02       1    91\n 9 2Ge+her The Hardest Part Of ... 2000-09-02       2    87\n10 2Ge+her The Hardest Part Of ... 2000-09-02       3    92\n# ℹ 5,297 more rows\n\n\nOther cases on how to deal with lenghtening data can be found here.\n\n\n8.4.3 Widening data\npivot_wider() makes datasets wider by increasing columns and reducing rows and helps when one observation is spread across multiple rows.\nExample:\n\ntidyr::cms_patient_experience\n\n# A tibble: 500 × 5\n   org_pac_id org_nm                           measure_cd measure_title prf_rate\n   &lt;chr&gt;      &lt;chr&gt;                            &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n 1 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       63\n 2 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       87\n 3 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       86\n 4 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       57\n 5 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       85\n 6 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       24\n 7 0446162697 ASSOCIATION OF UNIVERSITY PHYSI… CAHPS_GRP… CAHPS for MI…       59\n 8 0446162697 ASSOCIATION OF UNIVERSITY PHYSI… CAHPS_GRP… CAHPS for MI…       85\n 9 0446162697 ASSOCIATION OF UNIVERSITY PHYSI… CAHPS_GRP… CAHPS for MI…       83\n10 0446162697 ASSOCIATION OF UNIVERSITY PHYSI… CAHPS_GRP… CAHPS for MI…       63\n# ℹ 490 more rows\n\n\nThe core unit being studied is an organization, but each organization is spread across six rows, with one row for each measurement taken in the survey organization. We can see the complete set of values for measure_cd and measure_title by using distinct():\n\ntidyr::cms_patient_experience |&gt; \n  dplyr::distinct(measure_cd, measure_title)\n\n# A tibble: 6 × 2\n  measure_cd   measure_title                                                    \n  &lt;chr&gt;        &lt;chr&gt;                                                            \n1 CAHPS_GRP_1  CAHPS for MIPS SSM: Getting Timely Care, Appointments, and Infor…\n2 CAHPS_GRP_2  CAHPS for MIPS SSM: How Well Providers Communicate               \n3 CAHPS_GRP_3  CAHPS for MIPS SSM: Patient's Rating of Provider                 \n4 CAHPS_GRP_5  CAHPS for MIPS SSM: Health Promotion and Education               \n5 CAHPS_GRP_8  CAHPS for MIPS SSM: Courteous and Helpful Office Staff           \n6 CAHPS_GRP_12 CAHPS for MIPS SSM: Stewardship of Patient Resources             \n\n\nWe’ll use measure_cd as the source for our new column names for now. Instead of choosing new column names, we need to provide the existing columns that define the values (values_from) and the column name (names_from):\n\ntidyr::cms_patient_experience |&gt; \n  tidyr::pivot_wider(\n    names_from = measure_cd,\n    values_from = prf_rate\n  )\n\n# A tibble: 500 × 9\n   org_pac_id org_nm           measure_title CAHPS_GRP_1 CAHPS_GRP_2 CAHPS_GRP_3\n   &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 0446157747 USC CARE MEDICA… CAHPS for MI…          63          NA          NA\n 2 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          87          NA\n 3 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          86\n 4 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          NA\n 5 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          NA\n 6 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          NA\n 7 0446162697 ASSOCIATION OF … CAHPS for MI…          59          NA          NA\n 8 0446162697 ASSOCIATION OF … CAHPS for MI…          NA          85          NA\n 9 0446162697 ASSOCIATION OF … CAHPS for MI…          NA          NA          83\n10 0446162697 ASSOCIATION OF … CAHPS for MI…          NA          NA          NA\n# ℹ 490 more rows\n# ℹ 3 more variables: CAHPS_GRP_5 &lt;dbl&gt;, CAHPS_GRP_8 &lt;dbl&gt;, CAHPS_GRP_12 &lt;dbl&gt;\n\n\nThe output doesn’t look quite right; we still seem to have multiple rows for each organization. That’s because, we also need to tell pivot_wider() which column or columns have values that uniquely identify each row; in this case those are the variables starting with “org”:\n\ntidyr::cms_patient_experience |&gt; \n  tidyr::pivot_wider(\n    id_cols = starts_with(\"org\"),\n    names_from = measure_cd,\n    values_from = prf_rate\n  )\n\n# A tibble: 95 × 8\n   org_pac_id org_nm CAHPS_GRP_1 CAHPS_GRP_2 CAHPS_GRP_3 CAHPS_GRP_5 CAHPS_GRP_8\n   &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 0446157747 USC C…          63          87          86          57          85\n 2 0446162697 ASSOC…          59          85          83          63          88\n 3 0547164295 BEAVE…          49          NA          75          44          73\n 4 0749333730 CAPE …          67          84          85          65          82\n 5 0840104360 ALLIA…          66          87          87          64          87\n 6 0840109864 REX H…          73          87          84          67          91\n 7 0840513552 SCL H…          58          83          76          58          78\n 8 0941545784 GRITM…          46          86          81          54          NA\n 9 1052612785 COMMU…          65          84          80          58          87\n10 1254237779 OUR L…          61          NA          NA          65          NA\n# ℹ 85 more rows\n# ℹ 1 more variable: CAHPS_GRP_12 &lt;dbl&gt;"
  },
  {
    "objectID": "subsetting.html#operator",
    "href": "subsetting.html#operator",
    "title": "9  Subsetting R Objects",
    "section": "9.1 [ operator",
    "text": "9.1 [ operator\nThe [ operator always returns an object of the same class as the original:\n\nx &lt;- c(\"a\", \"b\", \"c\", \"c\", \"d\", \"a\")\n# Extract the first element\nx[1]\n\n[1] \"a\"\n\n\n\n# other examples\nx[1:4]\n\n[1] \"a\" \"b\" \"c\" \"c\"\n\nx[c(1, 3, 49)]\n\n[1] \"a\" \"c\" NA \n\n\n\nx &lt;- 100:110\ni &lt;- c(1, 3 ,2)\nx[i]\n\n[1] 100 102 101\n\n\n\nu &lt;- x &gt; \"a\" \nx[u] ## or x[x &gt; \"a\"]\n\ninteger(0)\n\n\n\nSubsetting a matrix:\nx[1,2] ## Extract the first row, first column \nx[1, ] ## Extract the first row \nx[, 2] ## Extract the second column \nx[1, 2, drop = FALSE] ## keeps the matrix format\nIt can be used to select multiple elements of an object (also in lists or dataframes):\n\n(x &lt;- list(foo = 1:4, bar = 0.6, baz = \"hello\"))\n\n$foo\n[1] 1 2 3 4\n\n$bar\n[1] 0.6\n\n$baz\n[1] \"hello\"\n\nx[c(1, 3)]\n\n$foo\n[1] 1 2 3 4\n\n$baz\n[1] \"hello\"\n\nx[\"foo\"]\n\n$foo\n[1] 1 2 3 4\n\nclass(x[\"foo\"])\n\n[1] \"list\"\n\nx$foo[1]\n\n[1] 1\n\nx$baz[1]\n\n[1] \"hello\"\n\n\n## use of negative subscript removes first element \"3\" \nnum10[-1]"
  },
  {
    "objectID": "subsetting.html#operator-1",
    "href": "subsetting.html#operator-1",
    "title": "9  Subsetting R Objects",
    "section": "9.2 [[ operator",
    "text": "9.2 [[ operator\nThe [[ operator is used to extract elements of a list or a dataframe. It can only be used to extract a single element and the class of the returned object will not necessarily be a list or data frame.\n\nSubsetting a Dataframe:\n# Use the notation [[ ]] to extract columns\nx[[\"var1\"]] = x[, 1] = x$var1 # All are equivalent\nSubsetting a List:\nx[[1]] ## Extract single element from a list \nx[[\"bar\"]] ## Extract named index\nx$bar ## Extract named index \nx[[c(1, 3)]] ## Get the 3rd element of the 1st element of the list \nx[[1]][[3]] ## Get the 3rd element of the 1st element of the list\nNow if we want to get access to the individual elements of list_obj[[2]], we have to use the following command:\n\n(data_2variable &lt;- data.frame(x1=c(2,3,4,5,6), x2=c(5,6,7,8,1)))\n\n  x1 x2\n1  2  5\n2  3  6\n3  4  7\n4  5  8\n5  6  1\n\n(list_obj &lt;- list(dat=data_2variable, vec.obj=c(1,2,3))) \n\n$dat\n  x1 x2\n1  2  5\n2  3  6\n3  4  7\n4  5  8\n5  6  1\n\n$vec.obj\n[1] 1 2 3\n\nlist_obj[[2]][1]\n\n[1] 1"
  },
  {
    "objectID": "subsetting.html#operator-2",
    "href": "subsetting.html#operator-2",
    "title": "9  Subsetting R Objects",
    "section": "9.3 $ operator",
    "text": "9.3 $ operator\nThe $ operator is used to extract elements of a list or data frame by literal name. Its semantics are similar to that of [[.\nExample:\n\nx[[\"bar\"]] ## Extract named index \n\n[1] 0.6\n\nx$bar ## Extract named index\n\n[1] 0.6"
  },
  {
    "objectID": "plyrdplyr.html#plyr",
    "href": "plyrdplyr.html#plyr",
    "title": "10  Plyr & Dplyr",
    "section": "10.1 Plyr",
    "text": "10.1 Plyr\nThe most important utility of the plyr package is that a single line of code can perform all the split(), apply(), and combine() steps.\nThe steps for the split-apply-combine approach of data analysis are as follows:\n\nFirst, we split the dataset into some mutually exclusive groups.\nWe then apply a task on each group and combine all the results to get the desired output.\nThis group-wise task could be generating new variables, summarizing existing variables, or even performing regression analysis on each group.\nFinally, combining approaches helps us get a nice output to compare the results from different groups.\n\nExample:\n\nlibrary(plyr)\nddply(iris, .(Species), function(x) colMeans(x[-5]))\n\n     Species Sepal.Length Sepal.Width Petal.Length Petal.Width\n1     setosa        5.006       3.428        1.462       0.246\n2 versicolor        5.936       2.770        4.260       1.326\n3  virginica        6.588       2.974        5.552       2.026\n\n\n\nThe first argument is the name of the data frame. We put iris, since the iris dataset is in the data frame structure, and we want to work on it.\nThe second argument is for a variable or variables, according to which we want to split our data frame. In this case, we have Species.\nThe third argument is a function that defines what kind of task we want to perform on each subset.\n\nNote that the first letter of the function name specifies the input, and the second letter specifies the output type:\n\n\n\nTable 1: Types of funcctions in the plyr package\n\n\nNote: mapply() can take multiple inputs as separate arguments, whereas a*ply() takes only a single array argument."
  },
  {
    "objectID": "plyrdplyr.html#dplyr",
    "href": "plyrdplyr.html#dplyr",
    "title": "10  Plyr & Dplyr",
    "section": "10.2 Dplyr",
    "text": "10.2 Dplyr\nQuite often, in real-life situations, we start our analysis with a dataframe-type structure. What do we do after getting a dataset and what are the basic data-manipulation tasks we usually perform before starting modeling?:\n\nCheck the validity of a dataset based on conditions.\nSort the dataset based on some variables, in ascending or descending order.\nCreate new variables based on existing variables.\nFinally, summarize them.\n\ndplyr can work with other data frame “backends” such as SQL databases. In fact, there is an SQL interface for relational databases via the DBI package\ndplyr can also be integrated with the data.table package for large fast tables.\n\n10.2.1 dplyr Grammar\nAll dplyr functions have a few common characteristics:\n\nThe first argument is a dataframe.\nThe subsequent arguments typically describe which columns to operate on using the variable names (without quotes).\nThe output is a new datafram (dplyr doesn’t modify the existing original dataset because dplyr functions never modify their inputs).\n\ndplyr’s verbs are organized into four groups based on what they operate on: rows, columns, groups, or tables.\n\nTable 2. Dplyr functions\n\n\nColumns\nRows\n\n\n\n\nselect()\nfilter() (base: subset)\n\n\nrename()\narrange()\n\n\nmutate()\ndistinct()\n\n\nrelocate()\nslice()\n\n\n\nBecause each verb does one thing well, solving complex problems will usually require combining multiple verbs, and we’ll do so with the pipe, |&gt;.\nFor verbs to work at all, dataframes must be properly formatted and annotated. In particular, the data must be tidy, that is, the data comply with the following three interrelated rules:\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\nNote that there’s a specific advantage to placing variables in columns because it allows R’s vectorized nature to shine. dplyr, ggplot2, and all the other packages in the tidyverse are designed to work with tidy data.\n\n\n10.2.2 The Pipe\nThe real power of dplyr arises when you start to combine multiple verbs. For example, imagine that you wanted to find the fastest flights to Houston’s IAH airport: you need to combine filter(), mutate(), select(), and arrange():\nThe pipe takes the thing on its left and passes it along to the function on its right so that x |&gt; f(y) is equivalent to f(x, y), and x |&gt; f(y) |&gt; g(z) is equivalent to g(f(x, y), z). The easiest way to pronounce the pipe is “then”.\nExample:\nflights |&gt; \n  filter(dest == \"IAH\") |&gt; \n  mutate(speed = distance / air_time * 60) |&gt; \n  select(year:day, dep_time, carrier, flight, speed) |&gt; \n  arrange(desc(speed))\nEven though this pipeline has four steps, it’s easy to skim because the verbs come at the start of each line: start with the flights data, then filter, then mutate, then select, then arrange.\nIMPORTANT: Chaining (|&gt;) is a powerful feature of dplyr that allows the output from one verb to be piped into the input of another verb using a short, easy-to-read syntax. To add the pipe to your code, we recommend using the built-in keyboard shortcut Ctrl/Cmd + Shift + M. You’ll need to make one change to your RStudio options to use |&gt; instead of %&gt;%.\n\n\n10.2.3 filter()\nThe filter() function is used to extract subsets of rows from a dataframe. This function is similar to the existing subset().\nExample:\nchic.f &lt;- filter(chicago, pm25tmean2 &gt; 30 & tmpd &gt; 80) \nsummary(chic.f$pm25tmean2)\nThe tidyverse alternative writing:\n# Flights with a departure dely higher than 120 mins\nflights |&gt; \n  filter(dep_delay &gt; 120)\nSometimes, it is more important to subset the dataframe based on values of a variable or multiple variables.\nExample:\nfilter(iris, Species==\"virginica\")\nfilter(iris, Species==\"virginica\" & Sepal.Length &lt;6 & Sepal. Width &lt;=2.7)\nTidyverse:\n# Flights that departed on January 1\nflights |&gt; \n  filter(month == 1 & day == 1)\n# Flights that departed in January or February\nflights |&gt; \n  filter(month == 1 | month == 2)\nThere’s a useful shortcut when you’re combining | and ==: %in%. It keeps rows where the variable equals one of the values on the right:\nflights |&gt; \n  filter(month %in% c(1, 2))\n\n\n10.2.4 arrange()\nThe arrange() function is used to change the order of the rows of a dataframe according to the value of the variables/columns (while preserving corresponding order of other columns).\nExample 1:\nchicago &lt;- arrange(chicago, date) \n## Columns can be arranged in descending order\nchicago &lt;- arrange(chicago, desc(date))\narrange(iris, Sepal.Length, desc(Sepal.Width))\nIt takes a data frame and a set of column names (or more complicated expressions) to order by. If you provide more than one column name, each additional column will be used to break ties in the values of the preceding columns.\nTidyverse:\n# Sorts by the departure time, which is spread over four columns\n# We get the earliest years first, then within a year, the earliest months, etc.\nflights |&gt; \n  arrange(year, month, day, dep_time)\n# Use desc() to re-order the df based on that column in big-to-small order  \nflights |&gt; \n  arrange(desc(dep_delay))\nNote that the number of rows has not changed – we’re only arranging the data, we’re not filtering it.\n\n\n10.2.5 distinct()\nSometimes, we might encounter duplicate observations in a data frame. The distinct() function helps eliminates these observations (finds all the unique rows in a dataset).\nExample:\ndistinct(iris, Species, Petal.Width)\nTidyverse:\nflights |&gt; \n  distinct()\nMost of the time, however, you’ll want the distinct combination of some variables, so you can also optionally supply column names:\nflights |&gt; \n  distinct(origin, dest)\n  \n# Keep other columns when filtering for unique rows\nflights |&gt; \n  distinct(origin, dest, .keep_all = TRUE)\nIf you want to find the number of occurrences instead, you’re better off swapping distinct() for count(). With the sort = TRUE argument, you can arrange them in descending order of the number of occurrences.\nExample:\nflights |&gt;\n  count(origin, dest, sort = TRUE)\n\n\n10.2.6 slice()\nYou can extract the subset of a dataframe using the slice() function.\nExample:\nslice(iris, 95:105)\nThere are five handy functions that allow you to extract specific rows within each group:\n# Takes the first row from each group.\ndf |&gt; slice_head(n = 1)\n# Takes the last row in each group\ndf |&gt; slice_tail(n = 1) .\n# Takes the row with the smallest value of column x\ndf |&gt; slice_min(x, n = 1) \n# Takes the row with the largest value of column x\ndf |&gt; slice_max(x, n = 1) \n# Takes one random row\ndf |&gt; slice_sample(n = 1)\nYou can vary n to select more than one row, or instead of n =, you can use prop = 0.1 to select (e.g.) 10% of the rows in each group. For example, the following code finds the flights that are most delayed upon arrival at each destination:\nExample:\nflights |&gt; \n  group_by(dest) |&gt; \n  slice_max(arr_delay, n = 1) |&gt;\n  relocate(dest)\n\n\n10.2.7 select()\nMost of the time, you do not work on all the variables in a dataframe. Selecting a few columns could make the analysis process less complicated. The select() function can be used to select columns of a data frame that you want to focus on.\nExample:\nchicago &lt;- readRDS(\"chicago.rds\")\nnames(chicago)[1:3] \nselect(chicago, c(\"city\", \"tmpd\"))\nselect(chicago, c(1, 3))\nsubset &lt;- select(chicago, city:dptp)\nTidyverse:\n\nSelect columns by name:\nflights |&gt; \n  select(year, month, day)\nSelect all columns between year and day (inclusive):\nflights |&gt; \n  select(year:day)\nSelect all columns except those from year to day (inclusive):\nflights |&gt; \n  select(!year:day)\nSelect all columns that are characters:\nflights |&gt; \n  select(where(is.character))\n\nThere are a number of helper functions you can use within select():\n\nstarts_with(\"abc\"): matches names that begin with “abc”.\nends_with(\"xyz\"): matches names that end with “xyz”.\ncontains(\"ijk\"): matches names that contain “ijk”.\nnum_range(\"x\", 1:3): matches x1, x2 and x3.\n\nYou can rename variables as you select() them by using =. The new name appears on the left-hand side of the =, and the old variable appears on the right-hand side\nExample:\nflights |&gt; \n  select(tail_num = tailnum)\nYou can also omit variables using the select() function by using the negative sign:\nExample:\nselect(chicago, -(city:dptp))\n\n\n10.2.8 rename()\nRenaming a variable in a dataframe in R is surprisingly hard to do! The rename() function is designed to make this process easier.\nExample:\nchicago &lt;- rename(chicago, dewpoint = dptp, pm25 = pm25tmean2)\nThe syntax inside the rename() function is to have the new name on the left-hand side of the = sign and the old name on the right-hand side.\nTidyverse\nflights |&gt; \n  rename(tail_num = tailnum)\n\n\n10.2.9 mutate()\nThe mutate() function exists to compute transformations of variables in a dataframe. Very often, you want to create new variables that are derived from existing variables and mutate() provides a clean interface for doing that (it adds new columns that are calculated from the existing columns).\nExample:\n# Create a pm25detrend variable that subtracts the mean from the pm25 variable\nchicago &lt;- mutate(chicago, pm25detrend = pm25 - mean(pm25, na.rm = TRUE))\n# Other example\nmutate(iris, SLm=Sepal.Length/100, SWm= Sepal.Width/100, PLm=Petal. Length/100, PWm= Petal.Width/100 )\nTidyverse:\nflights |&gt; \n  mutate(\n    gain = dep_delay - arr_delay,\n    speed = distance / air_time * 60\n  )\nYou can use the .before argument to add the variables to the left-hand side.\nExample:\nflights |&gt; \n  mutate(\n    gain = dep_delay - arr_delay,\n    speed = distance / air_time * 60,\n    .before = 1\n  )\nYou can also use .after to add after a variable, and in both .before and .after you can use the variable name instead of a position.\nNote that since we haven’t assigned the result of the above computation back to flights, the new variables gain, and speed will only be printed but will not be stored in a dataframe.\nAlternatively, you can control which variables are kept with the .keep argument. A particularly useful argument is \"used\" which specifies that we only keep the columns that were used in the “create” step with mutate(). For example, the following output will contain only the variables dep_delay, arr_delay, air_time, gain, hours, and gain_per_hour.\nflights |&gt; \n  mutate(\n    gain = dep_delay - arr_delay,\n    hours = air_time / 60,\n    gain_per_hour = gain / hours,\n    .keep = \"used\"\n  )\nIf you want to keep only the new variables and drop the old ones, we could use the transmute() function:\nExample:\n## Here we detrend the PM10 and ozone (O3) variables\ntransmute(chicago, pm10detrend = pm10tmean2 - mean(pm10tmean2, na.rm = TRUE), \n    o3detrend = o3tmean2 - mean(o3tmean2, na.rm = TRUE)))\n## Note that there are only two columns in the transmuted data frame\n\n\n10.2.10 relocate()\nUse relocate() to move variables around. You might want to collect related variables together or move important variables to the front. By default relocate() moves variables to the front.\nTidyverse\nflights |&gt; \n  relocate(time_hour, air_time)\nYou can also specify where to put them using the .before and .after arguments, just like in mutate():\nflights |&gt; \n  relocate(year:dep_time, .after = time_hour)\nflights |&gt; \n  relocate(starts_with(\"arr\"), .before = dep_time)\n\n\n10.2.11 group_by()\nThe group_by() function is used to divide your dataset into groups meaningful for your analysis. You will usually use the group_by() function in conjunction with the summarize() function.\nExample 1:\nflights |&gt; \n  group_by(month)\nYou can create groups using more than one variable. For example, we could make a group for each date.\nExample:\ndaily &lt;- flights |&gt;  \n  group_by(year, month, day)\nNote that group_by() doesn’t change the data but, if you look closely at the output, you’ll notice that the output indicates that it is “grouped by” month. This means subsequent operations will now work “by month”. group_by() adds this grouped feature (referred to as class) to the dataframe, which changes the behavior of the subsequent verbs applied to the data.\nYou might also want to remove grouping from a dataframe without using summarize(). You can do this with ungroup().\nExample:\ndaily |&gt; \n  ungroup()\n\n\n10.2.12 summarize()\nThe summarize() function is used to calculate a single summary statistic, and reduces the data frame to have a single row for each group.\nExample:\nflights |&gt; \n  group_by(month) |&gt; \n  summarize(\n    avg_delay = mean(dep_delay)\n  )\nYou can create any number of summaries in a single call to summarize(). One very useful summary is n(), which returns the number of rows in each group.\nExample:\nflights |&gt; \n  group_by(month) |&gt; \n  summarize(\n    avg_delay = mean(dep_delay, na.rm = TRUE), \n    n = n()\n  )\n\n\n10.2.13 Chaining (%&gt;%)\nSometimes, it could be necessary to use multiple functions to perform a single task. The pipeline operator %&gt;% is very handy for stringing together multiple dplyr functions in a sequence of operations.\nThis nesting is not a natural way to think about a sequence of operations:\nthird(second(first(x)))\nThe %&gt;% operator allows you to string operations in a left-to-right fashion:\nfirst(x) %&gt;% second(x) %&gt;% third(x)\nThis way we don’t have to create a set of temporary variables along the way or create a massive nested sequence of function calls.\nOnce you travel down the pipeline with %&gt;%, the first argument is taken to be the output of the previous element in the pipeline."
  },
  {
    "objectID": "date.html#date-manipulation",
    "href": "date.html#date-manipulation",
    "title": "11  Date and Text Manipulation",
    "section": "11.1 Date Manipulation",
    "text": "11.1 Date Manipulation\nIn R, dates are stored as the number of days elapsed since January 1, 1970. The built-in R function as.Date() can handle only dates but not time. So if we convert any date object to its internal number, it will show the number of days.\nWe can reformat the number into a date using the date class.\n\nas.numeric(as.Date(\"1970-01-01\")) ## We can also create a date object with other formats\n\n[1] 0\n\nas.numeric(as.Date(\"Jan-01-1970\", format = \"%b-%d-%Y\"))\n\n[1] NA\n\n\n## For the complete list of code that is used to specify date formats\nhelp(strptime)\nThe chron package can handle both date and time. However, it cannot work with time zones.\nUsing the POSIXct and POSIXlt class objects, we can work with time zones.\nThe lubridate package has a much more user-friendly functionality to process date and time, with time zone support:\n\nIt can process date variables in heterogeneous formats.\nNote that the default time zone in the mdy, dmy, or ymd function is Coordinated Universal Time (UTC)\n\n## creating a heterogeneous date object\nlibrary(lubridate)\nhetero_date &lt;- c(\"second chapter due on 2013, august, 24\", \"first chapter submitted on 2013, 08, 18\", \"2013 aug 23\") \nymd(hetero_date)\n\nthe sequence of year, month, and day should be similar across all values within the same object, otherwise during date extraction there will be a missing value that will be generated, along with a warning message."
  },
  {
    "objectID": "date.html#text-manipulation",
    "href": "date.html#text-manipulation",
    "title": "11  Date and Text Manipulation",
    "section": "11.2 Text Manipulation",
    "text": "11.2 Text Manipulation\nBesides default R functionality (paste(), nchar(), substr() …) , there is one contributed package to deal with character data, which is more user friendly and intuitive: stringr package.\nStrings can be arranged into vectors and matrices just like numbers. We can also paste strings together using paste(..., sep). Here sep is an optional input, with default value ” “, that determines which padding character is to be placed between the strings (which are input where … appears).\nExample:\n\nx &lt;- \"Citroen SM\"\ny &lt;- \"Jaguar XK150\"\nz &lt;- \"Ford Falcon GT-HO\"\n(wish.list &lt;- paste(x, y, z, sep = \", \"))\n\n[1] \"Citroen SM, Jaguar XK150, Ford Falcon GT-HO\"\n\n\nSpecial characters can be included in strings using the escape character \\:\n\n\\\" for ”\n\\n for a newline\n\\t for a tab\n\\b for a backspace\n\\\\ for \\\n\nText data can be used to retrieve information in sentiment analysis and even entity recognition.\n\n11.2.1 Sources of Text data\nText data can be found on tweets from any individual, or from any company, Facebook status updates, RSS feeds from any news site, Blog articles, Journal articles, Newspapers, Verbatim transcripts of an in-depth interview.\nFor example, t extract Twitter data, we can use tweetR() and, to extract data from Facebook, we could use facebookR().\n\n\n11.2.2 Getting Text Data\nThe easiest way to get text data is to import from a .csv file where some of the variables contain character data. We have to protect automatic factor conversion by specifying the stringsAsFactors = FALSE argument\nExample 1: The tweets.txt file is the plain text file. We will import this file using the generic readLines() function. It is a vector of characters (not a data.frame).\nExample 2: Html (this is also a character string):\nconURL &lt;- \"http://en.wikipedia.org/wiki/R_%28programming_language%29\"\n# Establish the connection with the URL \nlink2URL &lt;- url(conURL) \n# Reading html code\nhtmlCode &lt;- readLines(link2URL)\n# Closing the connection\nclose(link2URL)\n# Printing the result \nhtmlCode\nThe tm text mining library has some other functions to import text data from various files such as PDF files, plain text files, and even from doc files."
  },
  {
    "objectID": "databases.html",
    "href": "databases.html",
    "title": "12  R and Data Bases",
    "section": "",
    "text": "R stores everything in RAM, and a typical personal computer consists of limited RAM. R is RAM intensive, and for that reason, the size of a dataset should be much smaller than its RAM.\nThere are two principal ways to connect to a database:\n\nthe first uses the ODBC facility available on many computers and,\nthe second uses the DBI package of R along with a specialized package for the particular database needed to be accessed (if there is a specialized package available for a database).\n\n\n\n12.0.1 R and Excel/MSAccess\n\nAn Excel file can be imported into R using ODBC. Remember Excel cannot deal with relational databases.\nWe will now create an ODBC connection with an MS Excel file with the connection string xlopen:\n\nIn our computer: To use the ODBC approach on an Excel file, we firstly need to create the connection string using the system administrator. We need to open the control panel of the operating system and then open Administrative Tools and then choose ODBC. A dialog box will now appear. Click on the Add button and select an appropriate ODBC driver and then locate the desired file and give a data source name. In our case, the data source name is xlopen.\nIn R:\n\n# calling ODBC library into R \nlibrary(RODBC)\n# creating connection with the database using odbc package. \nxldb &lt;- odbcConnect(\"xlopen\") / odbcConnect(\"accessdata\")\n# Now that the connection is created, we will use this connection and import the data xldata&lt;- sqlFetch(xldb, \"CSVanscombe\")\n\n\n\n12.0.2 Relational databases in R\n\nThere are packages to interface between R and different database software packages that use relational database management systems, such as MySQL (RMySQL), PostgreSQL (RPgSQL), and Oracle (ROracle).\nOne of the most popular packages is RMySQL. This package allows us to make connections between R and the MySQL server. In order to install this package properly, we need to download both the MySQL server and RMySQL.\nThere are several R packages available that allow direct interactions with large datasets within R, such as filehash, ff, and bigmemory. The idea is to avoid loading the whole dataset into memory.\n\n\n\n12.0.3 filehash package\n\nIt is used for solving large-data problems. The idea behind the development of this package was to avoid loading the dataset into a computer’s virtual memory. Instead, we dump the large dataset into the hard drive and then assign an environment name for the dumped objects.\n\nlibrary(filehash) \ndbCreate(\"exampledb\")\nfilehash_db&lt;- dbInit(\"exampledb\")  ## db needs to be initialized before accessing\ndbInsert(filehash_db, \"xx\", rnorm(50)) \nvalue&lt;- dbFetch(filehash_db, \"xx\")  ## to retrieve db values\nsummary(value)\n\nThis file connection will remain open until the database is closed via dbDisconnect or the database object in R is removed.\n\n\n\n12.0.4 ff package\nThis package extends the R system and stores data in the form of native binary flat files in persistent storage such as hard disks, CDs, or DVDs rather than in the RAM.\nThis package enables users to work on several large datasets simultaneously. It also allows the allocation of vectors or arrays that are larger than the RAM.\n\n\n12.0.5 sqldf package\nThe sqldf package is an R package that allows users to run SQL statements within R.\nWe can perform any type of data manipulation to an R data frame either in memory or during import.\nIf the dataset is too large and cannot entirely be read into the R environment, we can import a portion of that dataset using sqldf."
  },
  {
    "objectID": "habits.html",
    "href": "habits.html",
    "title": "13  Good Programming Habits",
    "section": "",
    "text": "We find the following to be useful guidelines:\n\nStart each program with some comments giving the name of the program, the author, the date it was written, and what the program does. A description of what a program does should explain what all the inputs and outputs are.\nVariable names should be descriptive, that is, they should give a clue as to what the value of the variable represents. Avoid using reserved names or function names as variable names (in particular t, c, and q are all function names in R). You can find out whether or not your preferred name for an object is already in use by the exists() function.\nUse blank lines to separate sections of code into related parts, and use indenting to distinguish the inside part of an if statement or a for or while loop.\nDocument the programs that you use in detail, ideally with citations for specific algorithms. There is no worse feeling than returning to undocumented code that had been written several years earlier to try to find and then explain an anomaly."
  },
  {
    "objectID": "logic.html#logical-expressions",
    "href": "logic.html#logical-expressions",
    "title": "14  Logic",
    "section": "14.1 Logical expressions",
    "text": "14.1 Logical expressions\nA logical expression is formed using:\n\nthe comparison operators &lt; , &gt; , &lt;=, &gt;=, == (equal to), != (not equal to), &&, || (sequentially evaluated versions of & and |, respectively), and\nthe logical operators & (and), | (or), and ! (not).\n\nThe order of operations can be controlled using parentheses ( ).\nThe value of a logical expression is either TRUE or FALSE (the integers 1 and 0 can also be used to represent TRUE and FALSE, respectively).\nExample:\n\n## Note that A|B is TRUE if A or B or both are TRUE\nc(0,0,1,1)|c(0,1,0,1)\n\n[1] FALSE  TRUE  TRUE  TRUE\n\n## If you want exclusive disjunction, that is either A or B is TRUE but not both, then use xor(A,B)\nxor(c(0,0,1,1), c(0,1,0,1))\n\n[1] FALSE  TRUE  TRUE FALSE\n\n\n\n14.1.1 Sequential && and ||\nTo evaluate x & y, R first evaluates x and y, then returns TRUE if x and y are both TRUE, FALSE otherwise.\nTo evaluate x && y, R first evaluates x. If x is FALSE then R returns FALSE without evaluating y. If x is TRUE then R evaluates y and returns TRUE if y is TRUE, FALSE otherwise.\nSequential evaluation of x and y is useful when y is not always well defined, or when y takes a long time to compute.\nNote that && and || only work on scalars, whereas & and | work on vectors on an element-by-element basis.\n\n\n14.1.2 Index position\nIf you wish to know the index positions of TRUE elements of a logical vector x, then use which(x):\nExample:\n\nx &lt;- c(1, 1, 2, 3, 5, 8, 13)\nwhich(x %% 2 == 0)\n\n[1] 3 6"
  },
  {
    "objectID": "logic.html#if-else",
    "href": "logic.html#if-else",
    "title": "14  Logic",
    "section": "14.2 if-else",
    "text": "14.2 if-else\nA natural extension of the if command includes an else part:\nif (logical_expression) {\n  expression_1 # do something\n  ...\n} else {\n  expression_2 # do something different\n  ...\n}\nBraces { } are used to group together one or more expressions. If there is only one expression then the braces are optional.\nWhen an if expression is evaluated, if logical_expression is TRUE then the first group of expressions is executed and the second group of expressions is not executed. Conversely if logical_expression is FALSE then only the second group of expressions is executed.\n\n14.2.1 elseif\nWhen the else expression contains an if, then it can be written equivalently (and more clearly) as follows:\nif (logical_expression_1) {\n  expression_1\n  ...\n} else if (logical_expression_2) {\n  expression_2\n  ...\n} else {\n  expression_3\n  ...\n}"
  },
  {
    "objectID": "logic.html#loops",
    "href": "logic.html#loops",
    "title": "14  Logic",
    "section": "14.3 Loops",
    "text": "14.3 Loops\nTip: R is set up so that such programming tasks can be accomplished using vector operations rather than looping. Using vector operations is more efficient computationally, as well as more concise literally.\nExample:\n\n# We could find the sum of the first n squares using a loop as follows:\nn &lt;- 100\nS &lt;- 0\nfor (i in 1:n) {\n  S &lt;- S + i^2\n}\nS\n\n[1] 338350\n\n# Alternatively, using vector operations we have:\nsum((1:n)^2)\n\n[1] 338350\n\n\n\n14.3.1 for Loops\nThe for command has the following form, where x is a simple variable and vector is a vector.\nfor (x in vector) {\n  expression_1\n  ...\n}\nExample:\nx &lt;- c(\"a\", \"b\", \"c\", \"d\")\nfor(i in 1:4) / for(i in seq_along(x)) / for(letter in x) { ## set an iterator variable and assign it successive values over the elements of an object (list, vector, etc.) \n  print(x[i])\n  print(letter)\n}\nWhen executed, the for command executes the group of expressions within the braces { } once for each element of vector. The grouped expressions can use x, which takes on each of the values of the elements of vector as the loop is repeated.\n\n\n14.3.2 nested for loops\nExample:\nx &lt;- matrix(1:6, 2, 3) \nfor(i in seq_len(nrow(x))) { \n  for(j in seq_len(ncol(x))) { \n    print(x[i,j]) \n  }\n}\n\n\n14.3.3 while Loops\nOften we do not know beforehand how many times we need to go around a loop. That is, each time we go around the loop, we check some condition to see if we are done yet. In this situation we use a while loop, which has the form:\nwhile (logical_expression) {\n  expression_1\n  ...\n}\nWhen a while command is executed, logical_expression is evaluated first. If it is TRUE then the group of expressions in braces { } is executed. Control is then passed back to the start of the command: if logical_expression is still TRUE then the grouped expressions are executed again, and so on. Clearly, for the loop to stop eventually, logical_expression must eventually be FALSE. To achieve this logical_expression usually depends on a variable that is altered within the grouped expressions.\nExample:\ntime &lt;- 0\ndebt &lt;- debt_initial\nwhile (debt &gt; 0) {\n  time &lt;- time + period\n  debt &lt;- debt*(1 + r*period) - repayments\n}\n\n\n14.3.4 repeat Loops\nrepeat initiates an infinite loop right from the start. The only way to exit a repeat loop is to call break:\nrepeat { \n  if() { \n    break\n  }\n}\n\n\n14.3.5 next, break\nnext is used to skip an iteration of a loop:\nfor(i in 1:100) { \n  if(i &lt;= 20) { \n    next ## Skip the first 20 iterations \n  } \n  ## Do something here\n}\nbreak is used to exit a loop immediately, regardless of what iteration the loop may be on.\nfor(i in 1:100) { \n  print(i) \n  if(i &gt; 20) { \n    break ## Stop loop after 20 iterations \n  }\n}"
  },
  {
    "objectID": "logic.html#loop-functions",
    "href": "logic.html#loop-functions",
    "title": "14  Logic",
    "section": "14.4 Loop Functions",
    "text": "14.4 Loop Functions\nMany R functions are vectorised, meaning that given vector input the function acts on each element separately, and a vector output is returned. This is a very powerful aspect of R that allows for compact, efficient, and readable code. Moreover, for many R functions, applying the function to a vector is much faster than if we were to write a loop to apply it to each element one at a time.\nBesides, writing for and while loops is useful when programming but not particularly easy when working interactively on the command line.\nR has some functions which implement looping in a compact form to make your life easier:\nlapply() ## Apply the function FUN to every element of vector X\nsapply() ## Same as lapply but try to simplify the result \napply() ## Apply a function that takes a vector argument to each of the rows (or columns) of a matrix, \ntapply() ## Apply a function over subsets of a vector\nmapply() ## Multivariate version of lapply (to vectorise over more than one argument)\nX can be a list or an atomic vector, which is a vector that comprises atomic objects (logical, integer, numeric, complex, character and raw). That is, sapply(X, FUN) returns a vector whose i -th element is the value of the expression FUN(X[i]). Note that R performs a loop over the elements of X, so execution of this code is not faster than execution of an equivalent loop.\nIf FUN has arguments other than X[i], then they can be included using the dots protocol as shown above. That is, sapply(X, FUN, ...) returns FUN(X[i], ...) as the i -th element.\n\n14.4.1 lapply\nThe lapply() function does the following simple series of operations:\n\nIt loops over a list, iterating over each element in that list\nIt applies a function to each element of the list (a function that you specify)\nReturns a list (the “l” is for “list”).\n\nlapply() always returns a list, regardless of the class of the input.\n\nstr(lapply)\n\nfunction (X, FUN, ...)  \n\n\nNote that the function() definition is right in the call to lapply():\nlapply(x, function(elt) { elt[,1] }\nExample:\n\nx &lt;- list(a = 1:5, b = rnorm(10)) \nlapply(x, mean)\n\n$a\n[1] 3\n\n$b\n[1] -0.1046629\n\n\nNote that when you pass a function to another function, you do not need to include the open and closed parentheses () like you do when you are calling a function.\nOnce the call to lapply() is finished, the function disappears and does not appear in the workspace.\n\n\n14.4.2 sapply\nThe sapply() function behaves similarly to lapply(), the only real difference being that the return value is a vector or a matrix.\n\nstr(sapply)\n\nfunction (X, FUN, ..., simplify = TRUE, USE.NAMES = TRUE)  \n\n\nsapply() will try to simplify the result of lapply() if possible.\nExample:\n\ns &lt;- split(airquality, airquality$Month)\nmode(s)\n\n[1] \"list\"\n\n(q &lt;- sapply(s, function(x) { \n              colMeans(x[, c(\"Ozone\", \"Solar.R\", \"Wind\")])\n          }\n))\n\n               5         6          7        8        9\nOzone         NA        NA         NA       NA       NA\nSolar.R       NA 190.16667 216.483871       NA 167.4333\nWind    11.62258  10.26667   8.941935 8.793548  10.1800\n\nmode(q)\n\n[1] \"numeric\"\n\n\n\n\n14.4.3 apply\nThe apply() function can take a list, matrix, or array. It is most often used to apply a function to the rows or columns of a matrix (which is just a 2-dimensional array).\n\nstr(apply)\n\nfunction (X, MARGIN, FUN, ..., simplify = TRUE)  \n\n\nUsing apply() is not really faster than writing a loop, but it works in one line and is highly compact.\n\nx &lt;- matrix(rnorm(200), 20, 10)\n# Take the mean of each column\n(k &lt;- apply(x, 2, mean))\n\n [1]  0.08113327 -0.22926080  0.02125778  0.07419069 -0.00420104 -0.03962836\n [7] -0.15912799  0.06912397  0.05453960  0.25070794\n\nmode(k)\n\n[1] \"numeric\"\n\n## Take the mean of each row\napply(x, 1, sum) \n\n [1]  4.0047445 -3.4344409  1.2101137 -5.8504135  2.9326603  3.4725347\n [7]  0.3694524  1.4574741  0.5699734  2.0019296  2.5007197  1.5968432\n[13] -6.9848581  1.3259397 -0.5136034 -4.9109093  4.0569840  3.0089273\n[19] -3.4361370 -1.0032336\n\n\nFor the special case of column/row sums and column/row means of matrices, there are some useful shortcuts:\nrowSums = apply(x, 1, sum) \nrowMeans = apply(x, 1, mean)\ncolSums = apply(x, 2, sum) \ncolMeans = apply(x, 2, mean)\n\n\n14.4.4 tapply\ntapply() is used to apply a function over subsets of a vector. It can be thought of as a combination of split() and sapply() for vectors only.\n\nstr(tapply)\n\nfunction (X, INDEX, FUN = NULL, ..., default = NA, simplify = TRUE)  \n\n\nExample:\n\n(x &lt;- c(rnorm(10), runif(10), rnorm(10, 1)))\n\n [1] -0.49140033  0.26560919  0.73068608  0.07989909  1.20698866 -1.08493080\n [7] -0.99712506  1.09759162  0.45341461 -1.10333945  0.83651970  0.35198336\n[13]  0.60516498  0.24669300  0.52490328  0.80376991  0.48610356  0.21648759\n[19]  0.57310766  0.75591792  0.98986815  0.23731650  1.02851246  0.05916995\n[25]  2.88826799  1.44770682  1.32241379  2.49085743  1.44598649 -0.19760684\n\n# Define some groups with a factor variable\n(f &lt;- gl(3, 10))\n\n [1] 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3\nLevels: 1 2 3\n\ntapply(x, f, mean)\n\n         1          2          3 \n0.01573936 0.54006510 1.17124927 \n\n# We can reduce the noise as follows:\nround(tapply(x, f, mean), digits=1)\n\n  1   2   3 \n0.0 0.5 1.2 \n\n\n\n\n14.4.5 mapply\nThe mapply() function is a multivariate apply of sorts which applies a function in parallel over a set of arguments. Recall that lapply() and friends only iterate over a single R object. What if you want to iterate over multiple R objects in parallel? This is what mapply() is for.\n\nstr(mapply) \n\nfunction (FUN, ..., MoreArgs = NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE)  \n\n\nThe mapply() function has a different argument order from lapply() because the function to apply comes first rather than the object to iterate over. The R objects over which we apply the function are given in the ... argument because we can apply over an arbitrary number of R objects.\nExample 1:\n\nmapply(rep, 1:4, 4:1)\n\n[[1]]\n[1] 1 1 1 1\n\n[[2]]\n[1] 2 2 2\n\n[[3]]\n[1] 3 3\n\n[[4]]\n[1] 4\n\n\nExample 2:\n\nnoise &lt;- function(n, mean, sd) { \n            rnorm(n, mean, sd)\n          }\nmapply(noise, 1:5, 1:5, 2)\n\n[[1]]\n[1] 4.151885\n\n[[2]]\n[1] -0.4352463  1.1679627\n\n[[3]]\n[1] 4.957751 1.046890 3.569792\n\n[[4]]\n[1] 2.650106 1.171613 7.922484 4.511996\n\n[[5]]\n[1] 5.360067 7.668393 6.291863 6.609612 3.375550\n\n\nThe mapply() function can be used to automatically “vectorize” a function. What this means is that it can be used to take a function that typically only takes single arguments and create a new function that can take vector arguments. This is often needed when you want to plot functions.\nExample:\n## Generate some data\nx &lt;- rnorm(100)\n## This is not what we want\nsumsq(1:10, 1:10, x)  \n## Note that the call to sumsq() only produced one value instead of 10 values.\nmapply(sumsq, 1:10, 1:10, MoreArgs = list(x = x))\nThere’s even a function in R called vectorize() that automatically can create a vectorized version of your function. So, we could create a vsumsq() function that is fully vectorized as follows.\nExample:\nvsumsq &lt;- vectorize(sumsq, c(\"mu\", \"sigma\")) \nvsumsq(1:10, 1:10, x)\n\n\n14.4.6 split\nThe split() function takes a vector or other objects and splits it into groups determined by a factor or list of factors.\nThe form of the split() function is this:\n\nstr(split) \n\nfunction (x, f, drop = FALSE, ...)  \n\n\nThe combination of split() and a function like lapply() or sapply() is a common paradigm in R.\nExample 1:\n\nx &lt;- c(rnorm(10), runif(10), rnorm(10, 1))\nf &lt;- gl(3, 10) \n# split() returns a list\n(j &lt;- split(x, f))\n\n$`1`\n [1] -0.70905169  1.24123659 -0.44107335 -1.01246002  1.91092858 -1.72259295\n [7] -1.07246944 -0.01754113  1.10719072  0.42049089\n\n$`2`\n [1] 0.71207134 0.27426389 0.55713616 0.18923331 0.06810646 0.38383678\n [7] 0.49126364 0.18846147 0.69512853 0.40760119\n\n$`3`\n [1] 1.7211506 0.7859766 0.4123125 1.5155080 0.8124862 0.7444470 2.0389070\n [8] 1.3794663 0.8200305 1.7616114\n\nmode(j)\n\n[1] \"list\"\n\n\nExample 2:\n# Splitting a Dataframe\ns &lt;- split(airquality, airquality$Month)"
  },
  {
    "objectID": "functions.html#building-functions",
    "href": "functions.html#building-functions",
    "title": "15  Functions",
    "section": "15.1 Building Functions",
    "text": "15.1 Building Functions\nFunctions are often used to encapsulate a sequence of expressions that need to be executed numerous times, perhaps under slightly different conditions. Functions are also often written when code must be shared with others or the public.\nThe writing of a function allows a developer to create an interface to the code, that is explicitly specified with a set of parameters.\nA function has the general form:\nname &lt;- function(argument_1, argument_2, ...) {\n  expression_1\n  expression_2\n  &lt;some other expressions&gt;\n  return(output)\n}\nHere argument_1, argument_2, etc., are the names of variables and expression_1, expression_2, and output are all regular R expressions.\nname is the name of the function. Because all function arguments have names, they can be specified using their name. Specifying an argument by its name is sometimes useful if a function has many arguments and it may not always be clear which argument is being specified.\nExample:\nf(num = 2)\nSome functions may have no arguments, and that the braces are only necessary if the function comprises more than one expression.\nExample:\n\nf &lt;- function(num = 1) {   \n## if the function is called without the num argument being explicitly specified, then it will print “Hello, world!” to the console once.\n  hello &lt;- \"Hello, world!\\n\" \n  for(i in seq_len(num)) {\n    cat(hello)\n  }\n  chars &lt;- nchar(hello) * num \n  chars\n}\n\nIn R, the return value of a function is always the very last expression that is evaluated (in the example is chars).\nThe formals() function returns a list of all the formal arguments of a function.\nNote that functions have their own class."
  },
  {
    "objectID": "functions.html#call-or-run-a-function",
    "href": "functions.html#call-or-run-a-function",
    "title": "15  Functions",
    "section": "15.2 Call or Run a Function",
    "text": "15.2 Call or Run a Function\nTo call or run the function we type (for example) name(x1, x2). The value of this expression is the value of the expression output. To calculate the value of output the function first copies the value of x1 to argument_1, x2 to argument_2, and so on. The arguments then act as variables within the function. We say that the arguments have been passed to the function. Next the function evaluates the grouped expressions contained in the braces { }; the value of the expression output is returned as the value of the function.\nTo use the function we first load it (using source or by copying and pasting into R), then call it, supplying suitable arguments.\nExample:\nrm(list=ls())\nsource(\"../scripts/quad3.r\")\nquad3(1,0,-1)\nNote that the name of the function does not have to match the name of the program file, but when a program consists of a single function this is conventional."
  },
  {
    "objectID": "functions.html#a-function-return",
    "href": "functions.html#a-function-return",
    "title": "15  Functions",
    "section": "15.3 A Function Return",
    "text": "15.3 A Function Return\nA function always returns a value. For some functions the value returned is unimportant, for example if the function has written its output to a file then there may be no need to return a value as well. In such cases one usually omits the return statement, or returns NULL.\nA function may have more than one return statement, in which case it stops after executing the first one it reaches. If there is no return(output) statement then the value returned by the function is the value of the last expression in the braces.\nIf, when called, the value returned by a function (or any expression) is not assigned to a variable, then it is printed. The expression invisible(x) will return the same value as x, but its value is not printed. For example, some versions of the summary() function use invisible on their returned object."
  },
  {
    "objectID": "functions.html#arguments",
    "href": "functions.html#arguments",
    "title": "15  Functions",
    "section": "15.4 Arguments",
    "text": "15.4 Arguments\nWe think about arguments both when the functions are written and when they are called. The arguments of an existing function can be obtained by calling the formals function.\nIn order to simplify calling functions, some arguments may be assigned default values, which are used in case the argument is not provided in the call to the function:\nExample:\n\ntest3 &lt;- function(x = 1) {\n  return(x)\n  }\ntest3(2)\n\n[1] 2\n\ntest3()\n\n[1] 1\n\n\nSometimes you will want to define arguments so that they can take only a small number of different values, and the function will stop informatively if an inappropriate value is passed.\nExample:\n\nfunk &lt;- function(vibe = c(\"Do\",\"Be\",\"Dooby\",\"Dooo\")) {\n  vibe &lt;- match.arg(vibe)\n  return(vibe)\n  }\nfunk()\n\n[1] \"Do\"\n\n\nfunk(Peter)\nError in match.arg(vibe) (from #2) :\n'arg' should be one of \"Do\", \"Be\", \"Dooby\", \"Dooo\"\n\n15.4.1 Argument Matching\nR functions arguments can be matched (called) positionally or by name:\n\nPositional matching just means that R assigns the first value to the first argument, the second value to second argument, etc.:\nExample:\n# Positional match first by argument, default for 'na.rm'\nsd(mydata)\n# Specify 'x' argument by name, default for 'na.rm'\nsd(x = mydata)\n# Specify both arguments by name\nsd(x = mydata, na.rm = FALSE)\nWhen specifying the function arguments by name, it doesn’t matter in what order you specify them. Named arguments are useful on the command line when you have a long argument list and you want to use the defaults for everything except for an argument near the end of the list:\n## Specify both arguments by name\nsd(na.rm = FALSE, x = mydata)  \nR also provides partial matching of arguments, where doing so is not ambiguous. This means that argument names in the call to the function do not have to be complete. Reliance on partial matching makes code more fragile.\n\ntest6 &lt;- function(a = 1, b.c.d = 1) {\n  return(a + b.c.d)\n  }\ntest6()\n\n[1] 2\n\ntest6(b = 5)\n\n[1] 6\n\n\n\n\n\n15.4.2 The … Argument\n...indicates a variable number of arguments that are usually passed on to other functions, it is often used when extending another function and you don’t want to copy the entire argument list of the original function.\nThe ... argument is necessary when the number of arguments passed to the function cannot be known in advance, e.g. paste(), cat().\nAny arguments that appear after ... on the argument list must be named explicitly and cannot be partially matched or matched positionally.\nR provides a very useful means of passing arguments, unaltered, from the function that is being called to the functions that are called within it. These arguments do not need to be named explicitly in the outer function, hence providing great flexibility. To use this facility you need to include ... in your argument list. These three dots (an ellipsis) act as a placeholder for any extra arguments given to the function.\nExample:\n\ntest4 &lt;- function(x, ...) {\n  return(sd(x, ...))\n  }\ntest4(1:3)\n\n[1] 1\n\n# Arguments that do not match those in test4 are provided, in order, to any function within test4 that has the dots in the list of arguments to the function call.\ntest4(c(1:2,NA), na.rm = TRUE)\n\n[1] 0.7071068\n\n\nUsing the dots in this way means that the user has access to all the function arguments without our needing to list them when we define the function.\nIn general, naming the arguments in the function call is good practice, because it increases the readability and eliminates one potential source of errors."
  },
  {
    "objectID": "functions.html#scoping-rules",
    "href": "functions.html#scoping-rules",
    "title": "15  Functions",
    "section": "15.5 Scoping Rules",
    "text": "15.5 Scoping Rules\nArguments and variables that are defined within a function exist only within that function. That is, if you define and use a variable x inside a function, it does not exist outside the function. If variables with the same name exist inside and outside a function, then they are separate and do not interact at all. You can think of a function as a separate environment that communicates with the outside world only through the values of its arguments and its output expression. For example if you execute the command rm(list=ls()) inside a function (which is only rarely a good idea), you only delete those objects that are defined inside the function.\nExample:\n\ntest &lt;- function(x) {\n  y &lt;- x + 1\n  return(y)\n  }\ntest(1)\n\n[1] 2\n\n\nx\nError: Object \"x\" not found\nThat part of a program in which a variable is defined is called its scope. Restricting the scope of variables within a function provides an assurance that calling the function will not modify variables outside the function, except by assigning the returned value.\nBeware, however, the scope of a variable is not symmetric. That is, variables defined inside a function cannot be seen outside, but variables defined outside the function can be seen inside the function, provided there is not a variable with the same name defined inside.This arrangement makes it possible to write a function whose behavior depends on the context within which it is run.\nExample:\n\ntest2 &lt;- function(x) {\n  y &lt;- x + z\n  return(y)\n  }\nz &lt;- 1\ntest2(1)\n\n[1] 2\n\n\nThe moral of this example is that it is generally advisable to ensure that the variables you use in a function either are declared as arguments, or have been defined in the function."
  },
  {
    "objectID": "debugging.html#show-values",
    "href": "debugging.html#show-values",
    "title": "16  Debugging",
    "section": "16.1 Show values",
    "text": "16.1 Show values\nYou will spend a lot of time correcting errors in your programs. To find an error or bug, you need to be able to see how your variables change as you move through the branches and loops of your code.\nAn effective and simple way of doing this is to include statements like cat(\"var =\", var, \"\\n\") throughout the program, to display the values of variables such as var as the program executes. Once you have the program working you can delete these or just comment them so they are not executed.\nIt is also very helpful to make dry runs of your code, using simple starting conditions for which you know what the answer should be. These dry runs should ideally use short and simple versions of the final program, so that analysis of the output can be kept as simple as possible.\nGraphs and summary statistics of intermediate outcomes can be very revealing, and the code to create them is easily commented out for production runs."
  },
  {
    "objectID": "debugging.html#message",
    "href": "debugging.html#message",
    "title": "16  Debugging",
    "section": "16.2 Message",
    "text": "16.2 Message\nWe can fix this problem by anticipating the possibility of NA values and checking to see if the input is NA with the is.na() function:\nprintmessage3 &lt;- function(x) {\n    if(length(x) &gt; 1L)\n        stop(\"'x' has length &gt; 1\")\n        if(is.na(x))\n            print(\"x is a missing value!\")\n    else if(x &gt; 0) \n        print(\"x is greater than zero\")\n    else\n        print(\"x is less than or equal to zero\") \n    invisible(x)\n}"
  },
  {
    "objectID": "debugging.html#traceback",
    "href": "debugging.html#traceback",
    "title": "16  Debugging",
    "section": "16.3 traceback()",
    "text": "16.3 traceback()\nThe traceback() command prints out the function call stack after an error has occurred. The function call stack is the sequence of functions that was called before the error occurred.\nThe traceback() command must be called immediately after an error occurs. Once another function is called, you lose the traceback.\nlm(y ~ x) \nError in eval(expr, envir, enclos) : object 'y' not found\ntraceback() \n7: eval(expr, envir, enclos)\n6: eval(predvars, data, env)\n5: model.frame.default(formula = y \\~ x, drop.unused.levels = TRUE)\n4: model.frame(formula = y \\~ x, drop.unused.levels = TRUE)\n3: eval(expr, envir, enclos)\n2: eval(mf, parent.frame())\n1: lm(y \\~ x)\nLooking at the traceback is useful for figuring out roughly where an error occurred but it’s not useful for more detailed debugging. For that you might turn to the debug() function."
  },
  {
    "objectID": "debugging.html#debug",
    "href": "debugging.html#debug",
    "title": "16  Debugging",
    "section": "16.4 debug()",
    "text": "16.4 debug()\nThe debug() function takes a function as its first argument. Here is an example of debugging the lm() function.\ndebug(lm)   ## Flag the 'lm()' function for interactive debugging\nlm(y ~ x) debugging\nNow, every time you call the lm() function it will launch the interactive debugger. To turn this behavior off you need to call the undebug() function.\nThe debugger calls the browser at the very top level of the function body. From there you can step through each expression in the body. There are a few special commands you can call in the browser:\n\nn: executes the current expression and moves to the next expression.\nc: continues execution of the function and does not stop until either an error or the function exits.\nQ: quits the browser.\n\nYou can turn off interactive debugging with the undebug() function like this:\nundebug(lm)  ## Unflag the 'lm()' function for debugging"
  },
  {
    "objectID": "debugging.html#recover",
    "href": "debugging.html#recover",
    "title": "16  Debugging",
    "section": "16.5 recover()",
    "text": "16.5 recover()\nThe recover() function can be used to modify the error behavior of R when an error occurs. Normally, when an error occurs in a function, R will print out an error message, exit out of the function, and return you to your workspace to await further commands.\nWith recover() you can tell R that when an error occurs, it should halt execution at the exact point at which the error occurred.\noptions(error = recover)  ## Change default R error behavior\nread.csv(\"nosuchfile\")  ## This code doesn't work"
  },
  {
    "objectID": "profiling.html#system.time",
    "href": "profiling.html#system.time",
    "title": "17  Profiling R Code",
    "section": "17.1 system.time()",
    "text": "17.1 system.time()\nThe system.time() function computes the time (in seconds) needed to execute an expression and if there’s an error, gives the time until the error occurred. Using system.time() allows you to test certain functions or code blocks to see if they are taking excessive amounts of time.\nThe function returns an object of class proc_time which contains two useful bits of information:\n\nuser time: time charged to the CPU(s) for this expression.\nelapsed time: “wall clock” time, the amount of time that passes for you as you’re sitting there.\n\nThe elapsed time may be greater than the user time if the CPU spends a lot of time waiting around. The elapsed time may be smaller than the user time if your machine has multiple cores/processors (and is capable of using them).\nHere’s an example of where the elapsed time is greater than the user time:\n\n## Elapsed time &gt; user time \nsystem.time(readLines(\"http://www.jhsph.edu\")) \n\n   user  system elapsed \n   0.00    0.00    1.68 \n\n\nYou can time longer expressions by wrapping them in curly braces within the call to system.time().\nsystem.time({ \n  ## expression or loop or function\n  })"
  },
  {
    "objectID": "profiling.html#the-r-profiler",
    "href": "profiling.html#the-r-profiler",
    "title": "17  Profiling R Code",
    "section": "17.2 The R Profiler",
    "text": "17.2 The R Profiler\nsystem.time() assumes that you already know where the problem is and can call system.time() on that piece of code. What if you don’t know where to start? This is where the profiler comes in handy.\nIn conjunction with Rprof(), we will use the summaryRprof() function which summarizes the output from Rprof().\nRprof() keeps track of the function call stack at regularly sampled intervals and tabulates how much time is spent inside each function.\nThe profiler is started by calling the Rprof() function:\nRprof()  ## Turn on the profiler\nOnce you call the Rprof() function, everything that you do from then on will be measured by the profiler. Therefore, you usually only want to run a single R function or expression once you turn on the profiler and then immediately turn it off.\nThe profiler can be turned off by passing NULL to Rprof():\nRprof(NULL)  ## Turn off the profiler\nsummaryRprof()\nThe summaryRprof() function tabulates the R profiler output and calculates how much time is spend in which function. There are two methods for normalizing the data:\n\n“by.total”: divides the time spend in each function by the total run time.\n“by.self”: does the same as “by.total” but first subtracts out time spent in functions above the current function in the call stack.\n\nThe final bit of output that summaryRprof() provides is the sampling interval and the total runtime.\n$sample.interval \n[1] 0.02\n$sampling.time \n[1] 7.41"
  },
  {
    "objectID": "descriptive.html#summary-and-str",
    "href": "descriptive.html#summary-and-str",
    "title": "18  Descriptive Statistics",
    "section": "18.1 summary() and str()",
    "text": "18.1 summary() and str()\nThe summary() and str() functions are the fastest ways to get descriptive statistics of the data.\n\nThe summary() function gives the basic descriptive statistics of the data.\nThe str() function gives the structure of the variables."
  },
  {
    "objectID": "descriptive.html#measures-of-centrality",
    "href": "descriptive.html#measures-of-centrality",
    "title": "18  Descriptive Statistics",
    "section": "18.2 Measures of Centrality",
    "text": "18.2 Measures of Centrality\n\n18.2.1 Mode\nThe mode is a value in data that has the highest frequency:\nExample:\n\na &lt;- c(1, 2, 3, 4, 5, 5, 5, 6, 7, 8)\n# To get mode in a vector you create a frequency table\n(y &lt;- table(a)) \n\na\n1 2 3 4 5 6 7 8 \n1 1 1 1 3 1 1 1 \n\nnames(y)[which(y==max(y))]\n\n[1] \"5\"\n\n\n\n\n18.2.2 Median\nThe median is the middle or midpoint of the data and is also the 50 percentile of the data.\nThe median is affected by the outliers and skewness of the data.\n\nmedian(a)\n\n[1] 5\n\n\n\n\n18.2.3 Mean\nThe mean is the average of the data. The mean works best if the data is distributed in a normal distribution or distributed evenly.\n\nmean(a)\n\n[1] 4.6"
  },
  {
    "objectID": "descriptive.html#measures-of-variability",
    "href": "descriptive.html#measures-of-variability",
    "title": "18  Descriptive Statistics",
    "section": "18.3 Measures of Variability",
    "text": "18.3 Measures of Variability\nThe measures of variability are the measures of the spread of the data. These are encompasses:\n\nVariance.\nStandard deviation.\nRange.\nInterquartile range.\nand more.\n\n\n18.3.1 Variance\nThe variance is the average of squared differences from the mean, and it is used to measure the spreadness of the data:\n\nPopulation variance:\n\n\nA &lt;- c(1, 2, 3, 4, 5, 5, 5, 6, 7, 8)\nN &lt;- length(A)\nvar(A) * (N - 1) / N\n\n[1] 4.24\n\n\n\nSample variance:\n\n\nvar(A)\n\n[1] 4.711111\n\n\n\n\n18.3.2 Standard deviation\nThe standard deviation is the square root of a variance and it measures the spread of the data.\n\nPopulation standard deviation:\n\n\nA &lt;- c(1, 2, 3, 4, 5, 5, 5, 6, 7, 8)\nN &lt;- length(A)\nvariance &lt;- var(A) * (N - 1) / N\nsqrt(variance)\n\n[1] 2.059126\n\n\n\nSample standard deviation:\n\n\nsd(A)\n\n[1] 2.170509\n\n\n\n\n18.3.3 range()\nThe range is the difference between the largest and smallest points in the data:\n\nrange(A)    \n\n[1] 1 8\n\nres &lt;- range(A)\ndiff(res)\n\n[1] 7\n\nmin(A)\n\n[1] 1\n\nmax(A)\n\n[1] 8\n\n\n\n\n18.3.4 Interquartile Range\nThe interquartile range is the measure of the difference between the 75 percentile or third quartile and the 25 percentile or first quartile.\n\nIQR(A)\n\n[1] 2.5\n\n\nYou can get the quartiles by using the quantile() function:\n\nquantile(A)\n\n  0%  25%  50%  75% 100% \n1.00 3.25 5.00 5.75 8.00"
  },
  {
    "objectID": "descriptive.html#distributions",
    "href": "descriptive.html#distributions",
    "title": "18  Descriptive Statistics",
    "section": "18.4 Distributions",
    "text": "18.4 Distributions\n\n18.4.1 Normal Distribution\nIf the points do not deviate away from the line, the data is normally distributed.\n\n\n\nFigure 1: The normal distribution\n\n\nTo see whether data is normally distributed, you can use the qqnorm() and qqline() functions:\nqqnorm(data$x) #You must first draw the distribution to draw the line afterwards \nqqline(data$x)\nYou can also use a Shapiro Test to test whether the data is normally distributed. If the p-value is more than 0.05, you can conclude that the data does not deviate from normal distribution:\nshapiro.test(data$x)\n\n\n18.4.2 Modality\nThe modality of a distribution can be seen by the number of peaks when we plot the histogram.\n\n\n\nFigure 2: The modality of a distribution\n\n\n\n\n18.4.3 Skewness\nSkewness is a measure of how symmetric a distribution is and how much the distribution is different from the normal distribution.\nNegative skew is also known as left skewed, and positive skew is also known as right skewed: - A positive skewness indicates that the size of the right-handed tail is larger than the left-handed tail. - A negative skewness indicates that the left-hand tail will typically be longer than the right-hand tail.\n\n\n\nFigure 3: Skewness of a distribution\n\n\nThe Pearson’s Kurtosis measure is used to see whether a dataset is heavy tailed, or light tailed. High kurtosis means heavy tailed, so there are more outliers in the data.\n\nWhen kurtosis is close to 0, then a normal distribution is often assumed. These are called mesokurtic distributions.\n\nWhen kurtosis&gt;0, then the distribution has heavier tails and is called a leptokurtic distribution.\nWhen kurtosis&lt;0, then the distribution is light tails and is called a platykurtic distribution.\n\nTo find the kurtosis and skewness in R, you must install the moments package:\ninstall.packages(\"moments\")\nskewness(data$x)\nkurtosis(data$x)\n\n\n18.4.4 Binomial Distribution\nA binomial distribution has two outcomes, success or failure, and can be thought of as the probability of success or failure in a survey that is repeated various times.\n\ndbinom(32, 100, 0.5)\n\n[1] 0.000112817"
  },
  {
    "objectID": "exploratory.html#formulate-questions",
    "href": "exploratory.html#formulate-questions",
    "title": "19  Exploratory Analysis Pipeline",
    "section": "19.1 Formulate Questions",
    "text": "19.1 Formulate Questions\nA sharp question or hypothesis can serve as a dimension reduction tool that can eliminate variables that are not immediately relevant to the question.\nIt’s usually a good idea to spend a few minutes to figure out what is the question you’re really interested in and narrow it down to be as specific as possible (without becoming uninteresting). One of the most important questions you can answer with an exploratory data analysis is: “Do I have the right data to answer this question?”\nThere is no universal rule about which questions you should ask to guide your research. However, two types of questions will always be useful for making discoveries within your data:\n\nWhat type of variation occurs within my variables?\nWhat type of covariation occurs between my variables?"
  },
  {
    "objectID": "exploratory.html#read-in-your-data",
    "href": "exploratory.html#read-in-your-data",
    "title": "19  Exploratory Analysis Pipeline",
    "section": "19.2 Read In Your Data",
    "text": "19.2 Read In Your Data\nSometimes the data will come in a very messy format, and you’ll need to do some cleaning and transformations.\nThe readr package is a nice package for reading in flat files very fast, or at least much faster than R’s built-in functions"
  },
  {
    "objectID": "exploratory.html#check-the-packaging",
    "href": "exploratory.html#check-the-packaging",
    "title": "19  Exploratory Analysis Pipeline",
    "section": "19.3 Check the “Packaging”",
    "text": "19.3 Check the “Packaging”\nAssuming you don’t get any warnings or errors when reading in the dataset, you should now have an object in your workspace, e.g. named “ozone”. It’s usually a good idea to poke at that object a little bit before we break open the wrapping paper. For example, you can check the number of rows and columns."
  },
  {
    "objectID": "exploratory.html#run-str",
    "href": "exploratory.html#run-str",
    "title": "19  Exploratory Analysis Pipeline",
    "section": "19.4 Run str()",
    "text": "19.4 Run str()\nThis is usually a safe operation in the sense that even with a very large dataset, running str() shouldn’t take too long.\nYou can examine the classes of each of the columns to make sure they are correctly specified (i.e., numbers are numeric, and strings are character, etc.)."
  },
  {
    "objectID": "exploratory.html#look-top-and-bottom-of-your-data",
    "href": "exploratory.html#look-top-and-bottom-of-your-data",
    "title": "19  Exploratory Analysis Pipeline",
    "section": "19.5 Look Top and Bottom of Your Data",
    "text": "19.5 Look Top and Bottom of Your Data\nThis lets me know if the data were read in properly, things are properly formatted, and that everything is there. If your data are time series data, then make sure the dates at the beginning and end of the dataset match what you expect the beginning and ending time period are.\nYou can peek at the top and bottom of the data with the head() and tail() functions. Sometimes there’s weird formatting at the end or some extra comment lines that someone decided to stick at the end."
  },
  {
    "objectID": "exploratory.html#check-ns-frequency",
    "href": "exploratory.html#check-ns-frequency",
    "title": "19  Exploratory Analysis Pipeline",
    "section": "19.6 Check “n”s (frequency)",
    "text": "19.6 Check “n”s (frequency)\nTo do this properly, you need to identify some landmarks that can be used to check against your data. For example, if you are collecting data on people, such as in a survey or clinical trial, then you should know how many people there are in your study (i.e., in an ozone monitoring data system we can take a look at the Time.Local variable to see what time measurements are recorded as being taken.)\ntable(ozone$Time.Local)"
  },
  {
    "objectID": "exploratory.html#validate-with-external-data-source",
    "href": "exploratory.html#validate-with-external-data-source",
    "title": "19  Exploratory Analysis Pipeline",
    "section": "19.7 Validate With External Data Source",
    "text": "19.7 Validate With External Data Source\nExternal validation can often be as simple as checking your data against a single number.Is the data are at least of the right order of magnitude (i.e., the units are correct)? or, is the range of the distribution roughly what we’d expect, given the regulation around ambient pollution levels?"
  },
  {
    "objectID": "exploratory.html#check-the-variation-of-data",
    "href": "exploratory.html#check-the-variation-of-data",
    "title": "19  Exploratory Analysis Pipeline",
    "section": "19.8 Check the Variation of Data",
    "text": "19.8 Check the Variation of Data\nVariation is the tendency of the values of a variable to change from measurement to measurement.\nEvery variable has its own pattern of variation, which can reveal interesting information about how that it varies between measurements on the same observation as well as across observations. The best way to understand a pattern is to visualize the distribution of the variable’s values.\nExample:\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n# Visualizing the carat from the diamonds dataset\nggplot(diamonds, aes(x = carat)) +\n  geom_histogram(binwidth = 0.5)\n\n\n\n\nNow that you can visualize variation, what should you look for in your plot? And what type of follow-up questions should you ask?\nThis histogram suggests several interesting questions:\n\nWhy are there more diamonds at whole carats and common fractions of carats?\nWhy are there more diamonds slightly to the right of each peak than there are slightly to the left of each peak?\n\nVisualizations can also reveal clusters, which suggest that subgroups exist in your data. To understand the subgroups, ask:\n\nHow are the observations within each subgroup similar to each other?\nHow are the observations in separate clusters different from each other?\nHow can you explain or describe the clusters?\n\n\n19.8.1 Typical values\nIn both bar charts and histograms, tall bars show the common values of a variable, and shorter bars show less-common values. Places that do not have bars reveal values that were not seen in your data. Now you can turn this information into useful questions:\n\nWhich values are the most common? Why?\nWhich values are rare? Why? Does that match your expectations?\nCan you see any unusual patterns? What might explain them?\n\nLet’s take a look at the distribution of carat for smaller diamonds.\nExample:\n\nsmaller &lt;- diamonds |&gt; \n              filter(carat &lt; 3)\nggplot(smaller, aes(x = carat)) +\n  geom_histogram(binwidth = 0.01)\n\n\n\n\nThis histogram suggests several interesting questions:\n\nWhy are there more diamonds at whole carats and common fractions of carats?\nWhy are there more diamonds slightly to the right of each peak than there are slightly to the left of each peak?\n\nVisualizations can also reveal clusters, which suggest that subgroups exist in your data. To understand the subgroups, ask:\n\nHow can you explain or describe the clusters?\nHow are the observations within each subgroup similar to each other?\nHow are the observations in separate clusters different from each other?\n\n\n\n19.8.2 Unusual values\nOutliers are observations that are unusual; data points that don’t seem to fit the pattern. Sometimes outliers are data entry errors, sometimes they are simply values at the extremes that happened to be observed in this data collection, and other times they suggest important new discoveries.\nWhen you have a lot of data, outliers are sometimes difficult to see in a histogram. For example, take the distribution of the y variable from the diamonds dataset. The only evidence of outliers is the unusually wide limits on the x-axis.\nExample:\n\nggplot(diamonds, aes(x = y)) + \n  geom_histogram(binwidth = 0.5) \n\n\n\n\nTo make it easy to see unusual values:\ncoord_cartesian(ylim = c(0, 50))\nIf you’ve encountered unusual values in your dataset, and simply want to move on to the rest of your analysis, you have two options.\n\nDrop the entire row with the strange values:\nExample:\n\ndiamonds2 &lt;- diamonds |&gt; \n  filter(between(y, 3, 20))\n\nThis is not recommended because one invalid value doesn’t imply that all the other values for that observation are also invalid.\nInstead, we recommend replacing the unusual values with missing values. The easiest way to do this is to use mutate() to replace the variable with a modified copy.\nExample:\n\ndiamonds2 &lt;- diamonds |&gt; \n    mutate(y = if_else(y &lt; 3 | y &gt; 20, NA, y))\nggplot(diamonds2, aes(x = x, y = y)) + \n    geom_point()\n\nWarning: Removed 9 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\nTo suppress that warning, set na.rm = TRUE."
  },
  {
    "objectID": "exploratory.html#check-the-covariation",
    "href": "exploratory.html#check-the-covariation",
    "title": "19  Exploratory Analysis Pipeline",
    "section": "19.9 Check the Covariation",
    "text": "19.9 Check the Covariation\nIf variation describes the behavior within a variable, covariation describes the behavior between variables. Covariation is the tendency for the values of two or more variables to vary together in a related way.\nThe best way to spot covariation is to visualize the relationship between two or more variables.\n\n19.9.1 Categorical and numerical variable\nFor example, let’s explore how the price of a diamond varies with its quality (measured by cut) using geom_freqpoly():\nExample:\n\nggplot(diamonds, aes(x = price)) + \n  geom_freqpoly(aes(color = cut), binwidth = 500, linewidth = 0.75)\n\n\n\n\nNote that ggplot2 uses an ordered color scale for cut because it’s defined as an ordered factor variable in the data.\nTo make the comparison easier we need to swap what is displayed on the y-axis. Instead of displaying count, we’ll display the density, which is the count standardized so that the area under each frequency polygon is one.\nExample:\n\nggplot(diamonds, aes(x = price, y = after_stat(density))) + \n  geom_freqpoly(aes(color = cut), binwidth = 500, linewidth = 0.75)\n\n\n\n\nThere’s something rather surprising about this plot: it seems that fair diamonds (the lowest quality) have the highest average price!\n\n\n19.9.2 Two categorical variables\nTo visualize the covariation between categorical variables, you’ll need to count the number of observations for each combination of levels of these categorical variables. One way to do that is to rely on the built-in geom_count():\n\nggplot(diamonds, aes(x = cut, y = color)) +\n  geom_count()\n\n\n\n\nThe size of each circle in the plot displays how many observations occurred at each combination of values. Covariation will appear as a strong correlation between specific x values and specific y values.\nAnother approach for exploring the relationship between these variables is computing the counts with dplyr:\n\ndiamonds |&gt; \n  count(color, cut)\n\n# A tibble: 35 × 3\n   color cut           n\n   &lt;ord&gt; &lt;ord&gt;     &lt;int&gt;\n 1 D     Fair        163\n 2 D     Good        662\n 3 D     Very Good  1513\n 4 D     Premium    1603\n 5 D     Ideal      2834\n 6 E     Fair        224\n 7 E     Good        933\n 8 E     Very Good  2400\n 9 E     Premium    2337\n10 E     Ideal      3903\n# ℹ 25 more rows\n\n\nThen visualize with geom_tile() and the fill aesthetic:\n\ndiamonds |&gt; \n  count(color, cut) |&gt;  \n  ggplot(aes(x = color, y = cut)) +\n  geom_tile(aes(fill = n))\n\n\n\n\nFor larger plots, you might want to try the heatmaply package, which creates interactive plots.\n\n\n19.9.3 Two numerical variables\nDrawing a scatterplot with geom_point() is one great way to visualize the covariation between two numerical variables. For example, you can see a positive relationship between the carat size and price of a diamond (diamonds with more carats have a higher price):\nExample:\n\nggplot(smaller, aes(x = carat, y = price)) +\n  geom_point()\n\n\n\n\nScatterplots become less useful as the size of your dataset grows, because points begin to overplot, and pile up into areas of uniform black, making it hard to judge differences in the density of the data across the 2-dimensional space as well as making it hard to spot the trend.\nTwo possible solutions are:\n\nUsing transparency: R     geom_point(alpha = 1 / 100)\nAnother solution is to use bin:\n\nExample:\n\n# geom_bin2d() creates rectangular bins\nggplot(smaller, aes(x = carat, y = price)) +\n  geom_bin2d()\n\n\n\n\n# geom_hex() creates hexagonal bins\n# You will need to install.packages(\"hexbin\")\nggplot(smaller, aes(x = carat, y = price)) +\n  geom_hex()"
  },
  {
    "objectID": "exploratory.html#challenge-your-solutionbootstrap-sample",
    "href": "exploratory.html#challenge-your-solutionbootstrap-sample",
    "title": "19  Exploratory Analysis Pipeline",
    "section": "19.10 Challenge Your Solution/Bootstrap Sample",
    "text": "19.10 Challenge Your Solution/Bootstrap Sample\nThe easy solution is generally nice because it is easy, but you should never allow those results to hold the day. You should always be thinking of ways to challenge the results, especially if those results collide with your prior expectation.\nFor example, How stable are the ozone rankings from year to year? We can imagine that from year to year, the ozone data are somewhat different randomly, but generally follow similar patterns across the country. So, the shuffling process could approximate the data changing from one year to the next. This not an ideal solution, but it could give us a sense of how stable the rankings are:\n\nFirst, we set our random number generator and resample the indices of the rows of the data frame with replacement. The statistical jargon for this approach is a bootstrap sample:\nWe use the resampled indices to create a new dataset, ozone2, that shares many of the same qualities as the original but is randomly perturbed.\nset.seed(10234) \nN &lt;- nrow(ozone) \nidx &lt;- sample(N, N, replace = TRUE) \nozone2 &lt;- ozone[idx, ]\nWe reconstruct our rankings of the counties based on this resampled data:\nranking2 &lt;- group_by(ozone2, State.Name, County.Name) %&gt;% \n  summarize(ozone = mean(Sample.Measurement)) %&gt;%\n  as.data.frame %&gt;%\n  arrange(desc(ozone))\nWe can then compare the top 10 counties from our original ranking and the top 10 counties from our ranking based on the resampled data.\ncbind(head(ranking, 10)\nhead(ranking2, 10))\nWe can see that the rankings based on the resampled data are very close to the original, with the first 7 being identical. Numbers 8 and 9 get flipped in the resampled rankings but that’s about it. This might suggest that the original rankings are somewhat stable."
  },
  {
    "objectID": "exploratory.html#follow-up",
    "href": "exploratory.html#follow-up",
    "title": "19  Exploratory Analysis Pipeline",
    "section": "19.11 Follow Up",
    "text": "19.11 Follow Up\nAt this point it’s useful to consider a few follow up questions:\n\nDo you have the right data?\nDo you need other data?\nDo you have the right question?\n\nThe goal of exploratory data analysis is to get you thinking about your data and reasoning about your question. At this point, we can refine our question or collect new data, all in an iterative process to get at the truth."
  },
  {
    "objectID": "inferential.html#correlation",
    "href": "inferential.html#correlation",
    "title": "20  Inferential Statistics",
    "section": "20.1 Correlation",
    "text": "20.1 Correlation\nCorrelations are statistical associations to find how close two variables are and to derive the linear relationships between them.\nYou can use correlation to find which variables are more related to the target variable and use this to reduce the number of variables.\nCorrelation does not mean a causal relationship, it does not tell you the how and why of the relationship.\ncor(data$var1, data$var2)\nThe correlation has a range from -1.0 to 1.0."
  },
  {
    "objectID": "inferential.html#covariance",
    "href": "inferential.html#covariance",
    "title": "20  Inferential Statistics",
    "section": "20.2 Covariance",
    "text": "20.2 Covariance\nCovariance is a measure of variability between two variables.\nThe greater the value of one variable and the greater of other variable means it will result in a covariance that is positive.\ncov(data$var1, data$var2)\nCovariance does not have a range. When two variables are independent of each other, the covariance is zero."
  },
  {
    "objectID": "inferential.html#hypothesis-testing-and-p-value",
    "href": "inferential.html#hypothesis-testing-and-p-value",
    "title": "20  Inferential Statistics",
    "section": "20.3 Hypothesis Testing and P-Value",
    "text": "20.3 Hypothesis Testing and P-Value\nBased on the research question, the hypothesis can be a null hypothesis, H0 (μ1= μ2) and an alternate hypothesis, Ha (μ1 ≠ μ2).\nFor data normally distributed:\n\np-value:\n\nA small p-value &lt;= alpha, which is usually 0.05, indicates that the observed data is sufficiently inconsistent with the null hypothesis, so the null hypothesis may be rejected. The alternate hypothesis is true at the 95% confidence interval.\nA larger p-value means that you failed to reject null hypothesis.\n\nt-test continuous variables of data.\nchi-square test for categorical variables or data.\nANOVA\n\nFor data not normally distributed:\n\nnon-parametric tests."
  },
  {
    "objectID": "inferential.html#t-test",
    "href": "inferential.html#t-test",
    "title": "20  Inferential Statistics",
    "section": "20.4 T-Test",
    "text": "20.4 T-Test\nA t-test is used to determine whether the mean between two data points or samples are equal to each other.\n\n\\(H_0\\) (\\(μ_1\\) = \\(μ_2\\)): The null hypothesis means that the two means are equal.\n\\(H_a\\) (\\(μ_1\\) ≠ \\(μ_2\\)): The alternative means that the two means are different.\n\nIn t-test there are two assumptions:\n\nThe population is normally distributed.\nThe samples are randomly sampled from their population.\n\nType I and Type II Errors:\n\nA Type I error is a rejection of the null hypothesis when it is really true.\nA Type II error is a failure to reject a null hypothesis that is false.\n\n\n20.4.1 One-Sample T-Test\nA one-sample t-test is used to test whether the mean of a population is equal to a specified mean.\nYou can use the t statistics and the degree of freedom (\\(df = n -1\\)) to estimate the p-value using a t-table.\nt.test(data$var1, mu=0.6) \n\n\n20.4.2 Two-Sample Independent T-Test (unpaired, paired = FALSE)\nThe two-sample unpaired t-test is when you compare two means of two independent samples. The degrees of freedom formula is \\(df = nA – nB – 2\\)\nIn the two-sample unpaired t-test, when the variance is unequal, you use the Welch t-test.\nt.test(data$var1, data\\$var2, var.equal=FALSE, paired=FALSE) \n\n\n20.4.3 Two-Sample Dependent T-Test (paired = TRUE)\nA two-sample paired t-test is used to test the mean of two samples that depend on each other. The degree of freedom formula is \\(df = n-1\\)\nt.test(data$var1, data$var2, paired=TRUE)"
  },
  {
    "objectID": "inferential.html#chi-square-test",
    "href": "inferential.html#chi-square-test",
    "title": "20  Inferential Statistics",
    "section": "20.5 Chi-Square Test",
    "text": "20.5 Chi-Square Test\nThe chi-square test is used to compare the relationships between two categorical variables.\nThe null hypothesis means that there is no relationship between the categorical variables.\n\n20.5.1 Goodness of Fit Test\nWhen you have only one categorical variable from a population and you want to compare whether the sample is consistent with a hypothesized distribution, you can use the goodness of fit test.\n\n\\(H_0\\): No significant difference between the observed and expected values.\n\\(H_A\\): There is a significant difference between the observed and expected values.\n\nTo use the goodness of fit chi-square test in R, you can use the chisq.test() function:\ndata &lt;- c(B=200, c=300, D=400)\nchisq.test(data)\n\n\n20.5.2 Contingency Test\nIf you have two categorical variables and you want to compare whether there is a relationship between two variables, you can use the contingency test.\n\n\\(H_0\\): the two categorical variables have no relationship. The two variables are independent.\n\\(H_A\\): the two categorical variables have a relationship. The two variables are not independent.\n\nvar1 &lt;- c(\"Male\", \"Female\", \"Male\", \"Female\", \"Male\")\nvar2 &lt;- c(\"chocolate\", \"strawberry\", \"strawberry\", \"strawberry\", \"chocolate\")\ndata &lt;- data.frame(var1, var2)\ndata.table &lt;- table(data$var1, data$var2)\ndata.table &gt; chisq.test(data.table)"
  },
  {
    "objectID": "inferential.html#anova",
    "href": "inferential.html#anova",
    "title": "20  Inferential Statistics",
    "section": "20.6 ANOVA",
    "text": "20.6 ANOVA\nANOVA is the process of testing the means of two or more groups. ANOVA also checks the impact of factors by comparing the means of different samples.\nIn ANOVA, you use two kinds of means:\n\nSample means.\nGrand mean (the mean of all of the samples’ means).\n\nHypothesis: - \\(H_0\\): \\(μ_1\\)= \\(μ_2\\) = … = \\(μ_L\\) ; the sample means are equal or do not have significant differences. - \\(H_A\\): \\(μ_1\\) ≠ \\(μ_m\\); is when the sample means are not equal.\nYou assume that the variables are sampled, independent, and selected or sampled from a population that is normally distributed with unknown but equal variances.\n\n20.6.1 Between Group Variability\nThe distribution of two samples, when they overlap, their means are not significantly different. Hence, the difference between their individual mean and the grand mean is not significantly different.\n\n\n\nFigure 6: Between group variability\n\n\nThis variability is called the between-group variability, which refers to the variations between the distributions of the groups or levels.\n\n\n20.6.2 Within Group Variability\nFor the following distributions of samples, as their variance increases, they overlap each other and become part of a population.\n\n\n\nFigure 7: Within group variability\n\n\nThe F-statistics are the measures if the means of samples are significantly different. The lower the F-statistics, the more the means are equal, so you cannot reject the null hypothesis.\n\n\n20.6.3 One-Way ANOVA\nOne-way ANOVA is used when you have only one independent variable.\n\nlibrary(graphics)\nset.seed(123) \nvar1 &lt;- rnorm(12, 2, 1) \nvar2 &lt;- c(\"B\", \"B\", \"B\", \"B\", \"C\", \"C\", \"C\", \"C\", \"C\", \"D\", \"D\", \"B\")\ndata &lt;- data.frame(var1, var2) \nfit &lt;- aov(data$var1 ~ data$var2, data = data)\nfit \n\nCall:\n   aov(formula = data$var1 ~ data$var2, data = data)\n\nTerms:\n                data$var2 Residuals\nSum of Squares   0.162695  9.255706\nDeg. of Freedom         2         9\n\nResidual standard error: 1.014106\nEstimated effects may be unbalanced\n\nsummary(fit)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\ndata$var2    2  0.163  0.0813   0.079  0.925\nResiduals    9  9.256  1.0284               \n\n\n\n\n20.6.4 Two-Way ANOVA\nTwo-way ANOVA is used when you have two independent variables. (continuar en el ejemplo anterior):\n\nvar3 &lt;- c(\"D\", \"D\", \"D\", \"D\", \"E\", \"E\", \"E\", \"E\", \"E\", \"F\", \"F\", \"F\")\ndata &lt;- data.frame(var1, var2, var3) \nfit &lt;- aov(data$var1 ~ data$var2 + data$var3, data=data)\nfit\n\nCall:\n   aov(formula = data$var1 ~ data$var2 + data$var3, data = data)\n\nTerms:\n                data$var2 data$var3 Residuals\nSum of Squares   0.162695  0.018042  9.237664\nDeg. of Freedom         2         1         8\n\nResidual standard error: 1.074573\n1 out of 5 effects not estimable\nEstimated effects may be unbalanced\n\nsummary(fit)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\ndata$var2    2  0.163  0.0813   0.070  0.933\ndata$var3    1  0.018  0.0180   0.016  0.904\nResiduals    8  9.238  1.1547               \n\n## var1 does not depend on var2's mean and var3's mean\n\n\n\n20.6.5 MANOVA\nThe multivariate analysis of variance is when there are multiple response variables that you want to test.\nExample:\n\nres &lt;- manova(cbind(iris$Sepal.Length, iris$Petal.Length) ~ iris$Species, data=iris) \nsummary(res)\n\n              Df Pillai approx F num Df den Df    Pr(&gt;F)    \niris$Species   2 0.9885   71.829      4    294 &lt; 2.2e-16 ***\nResiduals    147                                            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.aov(res)\n\n Response 1 :\n              Df Sum Sq Mean Sq F value    Pr(&gt;F)    \niris$Species   2 63.212  31.606  119.26 &lt; 2.2e-16 ***\nResiduals    147 38.956   0.265                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response 2 :\n              Df Sum Sq Mean Sq F value    Pr(&gt;F)    \niris$Species   2 437.10 218.551  1180.2 &lt; 2.2e-16 ***\nResiduals    147  27.22   0.185                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## Hence, you have two response variables, Sepal.Length and Petal.Length\n\nThe p-value is 2.2e-16, which is less than 0.05. Hence, you reject the null hypothesis"
  },
  {
    "objectID": "inferential.html#nonparametric-test",
    "href": "inferential.html#nonparametric-test",
    "title": "20  Inferential Statistics",
    "section": "20.7 Nonparametric Test",
    "text": "20.7 Nonparametric Test\nThe nonparametric test is a test that does not require the variable and sample to be normally distributed.\nYou use nonparametric tests when you do not have normally distributed data and the sample data is big.\n\nTable 3: Types of nonparametric tests\n\n\n\n\n\n\n\nNonparametric Test\nFunction\nMethod replaced\n\n\n\n\nWilcoxon Signed Rank Test\nwilcox.test(data[,1], mu=0, alternatives=\"two.sided\")\none-sample\n\n\nt-testundefined\n\n\n\n\nWilcoxon-Mann-Whitney Test\nwilcox.test(data[,1], data[,2], correct=FALSE)\nsubstitute\n\n\nto the two-sample t-test\n\n\n\n\nKruskal-Wallis Test\nkruskal.test(airquality$Ozone ~ airquality$Month)\none-way\n\n\n\n\n20.7.1 Wilcoxon Signed Rank Test\nThe Wilcoxon signed rank test is used to replace the one-sample t-test.\nHypothesis:\n\n\n\\(H_0\\): \\(μ_1\\)= \\(μ_o\\); the null hypothesis is that the population median has the specified value of \\(μ_0\\)\n\\(H_a\\): \\(μ_1\\) ≠ \\(μ_o\\)\n\nTo use the Wilcoxon signed rank test in R, you can first generate the data set using random.org packages, so that the variables are not normally distributed.\nExample:\ninstall.packages(\"random\") \nlibrary(random) \nvar1 &lt;- randomNumbers(n=100, min=1, max=1000, col=1)\nvar2 &lt;- randomNumbers(n=100, min=1, max=1000, col=1) \nvar3 &lt;- randomNumbers(n=100, min=1, max=1000, col=1) \ndata &lt;- data.frame(var1[,1], var2[,1], var3[,1]) \nwilcox.test(data[,1], mu=0, alternatives=\"two.sided\")\n\n\n20.7.2 Wilcoxon-Mann-Whitney Test\nThe Wilcoxon-Mann-Whitney test is a nonparametric test to compare two samples. It is a powerful substitute to the two-sample t-test.\nTo use the Wilcoxon-Matt-Whitney test (or the Wilcoxon rank sum test or the Mann-Whitney test) in R, you can use the wilcox.test() function:\nwilcox.test(data[,1], data[,2], correct=FALSE)\nThere are not significant differences in the median for first variable median and second variable median.\n\n\n20.7.3 Kruskal-Wallis Test\nThe Kruskal-Wallis test is a nonparametric test that is an extension of the Mann-Whitney U test for three or more samples.\nThe test requires samples to be identically distributed.\nKruskal-Wallis is an alternative to one-way ANOVA.\nThe Kruskal-Wallis test tests the differences between scores of k independent samples of unequal sizes with the ith sample containing li rows:\n\n\\(H_0\\): \\(μ_o\\) = \\(μ_1\\)= \\(μ_2\\) = … = \\(μ_k\\); The null hypothesis is that all the medians are the same.\n\\(H_a\\): \\(μ_1\\) ≠ \\(μ_k\\); The alternate hypothesis is that at least one median is different.\n\n\nkruskal.test(airquality$Ozone ~ airquality$Month)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  airquality$Ozone by airquality$Month\nKruskal-Wallis chi-squared = 29.267, df = 4, p-value = 6.901e-06"
  },
  {
    "objectID": "regression.html#linear-regressions",
    "href": "regression.html#linear-regressions",
    "title": "21  Regression Analysis",
    "section": "21.1 Linear Regressions",
    "text": "21.1 Linear Regressions\nThe linear regression equation is \\(y = b_0 + b_1 x\\), where y is the dependent variable, x is the independent variable, \\(b_0\\) is the intercept, and \\(b_1\\) is the slope.\nTo use linear regression in R, you use the lm() function:\n\nset.seed(123)\nx &lt;- rnorm(100, mean=1, sd=1)\ny &lt;- rnorm(100, mean=2, sd=2)\ndata &lt;- data.frame(x, y);\nmod &lt;- lm(data$y ~ data$x, data=data)\nmod\n\n\nCall:\nlm(formula = data$y ~ data$x, data = data)\n\nCoefficients:\n(Intercept)       data$x  \n     1.8993      -0.1049  \n\nsummary(mod)\n\n\nCall:\nlm(formula = data$y ~ data$x, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.815 -1.367 -0.175  1.161  6.581 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.8993     0.3033   6.261 1.01e-08 ***\ndata$x       -0.1049     0.2138  -0.491    0.625    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.941 on 98 degrees of freedom\nMultiple R-squared:  0.002453,  Adjusted R-squared:  -0.007726 \nF-statistic: 0.241 on 1 and 98 DF,  p-value: 0.6246\n\n\nWhen the p-value is less than 0.05, the model is significant:\n\n\\(H_0\\): Coefficient associated with the variable is equal to zero\n\\(H_a\\): Coefficient is not equal to zero (there is a relationship)\n\nFurthermore:\n\nThe higher the R-squared and the adjusted R-squared, the better the linear model.\nThe lower the standard error, the better the model"
  },
  {
    "objectID": "regression.html#multiple-linear-regressions",
    "href": "regression.html#multiple-linear-regressions",
    "title": "21  Regression Analysis",
    "section": "21.2 Multiple Linear Regressions",
    "text": "21.2 Multiple Linear Regressions\nMultiple linear regression is used when you have more than one independent variable.\nThe equation of a multiple linear regression is:\n\\(y = b_0 + b_1 x_1 + b_2 x_2 + ... + b_ k x_k + ϵ\\)\nWhen you have n observations or rows in the data set, you have the following model:\n\n\n\nFigure 4: Model for multiple regressions\n\n\nUsing a matrix, you can represent the equations as: \\(y = Xb + ϵ\\)\n\n\n\nFigure 5: Representation of equations as a matrix\n\n\nTo calculate the coefficients: ^b = (X’ X)-1 X’ y\n\nset.seed(123)\nx &lt;- rnorm(100, mean=1, sd=1)\nx2 &lt;- rnorm(100, mean=2, sd=5)\ny &lt;- rnorm(100, mean=2, sd=2)\ndata &lt;- data.frame(x, x2, y)\nmod &lt;- lm(data$y ~ data$x + data$x2, data=data)\nmod\n\n\nCall:\nlm(formula = data$y ~ data$x + data$x2, data = data)\n\nCoefficients:\n(Intercept)       data$x      data$x2  \n   2.517425    -0.266343     0.009525  \n\nsummary(mod)\n\n\nCall:\nlm(formula = data$y ~ data$x + data$x2, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7460 -1.3215 -0.2489  1.2427  4.1597 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.517425   0.305233   8.248 7.97e-13 ***\ndata$x      -0.266343   0.209739  -1.270    0.207    \ndata$x2      0.009525   0.039598   0.241    0.810    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.903 on 97 degrees of freedom\nMultiple R-squared:  0.01727,   Adjusted R-squared:  -0.00299 \nF-statistic: 0.8524 on 2 and 97 DF,  p-value: 0.4295"
  },
  {
    "objectID": "graphs.html#principles-of-graphics",
    "href": "graphs.html#principles-of-graphics",
    "title": "22  Graphs",
    "section": "22.1 Principles of Graphics",
    "text": "22.1 Principles of Graphics\n1. Show comparisons\nYou should always be comparing at least two things.\n2. Show causality, mechanism, explanation, systematic structure\nGenerally, it’s difficult to prove that one thing causes another thing even with the most carefully collected data. But it’s still often useful for your data graphics to indicate what you are thinking about in terms of cause.\n3. Show multivariate data\nThe point is that data graphics should attempt to show this information as much as possible, rather than reduce things down to one or two features that we can plot on a page… From the plot it seems that there is a slight negative relationship between the two variables….This example illustrates just one of many reasons why it can be useful to plot multivariate data and to show as many features as intelligently possible. In some cases, you may uncover unexpected relationships depending on how they are plotted or visualized.\n4. Integrate evidence\nData graphics should make use of many modes of data presentation simultaneously, not just the ones that are familiar to you or that the software can handle. One should never let the tools available drive the analysis; one should integrate as much evidence as possible on to a graphic as possible.\n5. Describe and document the evidence\nData graphics should be appropriately documented with labels, scales, and sources. A general rule for me is that a data graphic should tell a complete story all by itself. Is there enough information on that graphic for the person to get the story? While it is certainly possible to be too detailed, I tend to err on the side of more information rather than less.\n6. Content, Content, Content\nAnalytical presentations ultimately stand or fall depending on the quality, relevance, and integrity of their content. This includes the question being asked and the evidence presented in favor of certain hypotheses. Starting with a good question, developing a sound approach, and only presenting information that is necessary for answering that question, is essential to every data graphic."
  },
  {
    "objectID": "graphs.html#types-of-graphs",
    "href": "graphs.html#types-of-graphs",
    "title": "22  Graphs",
    "section": "22.2 Types of Graphs",
    "text": "22.2 Types of Graphs\nVisualizing the data via graphics can be important at the beginning stages of data analysis to understand basic properties of the data, to find simple patterns in data, and to suggest possible modeling strategies. In later stages of an analysis, graphics can be used to “debug” an analysis, if an unexpected (but not necessarily wrong) result occurs, or ultimately, to communicate your findings to others.\nWe will make a distinction between exploratory graphs and final graphs:\nExploratory graphs are usually made very quickly and a lot of them are made in the process of checking out the data, developing a personal understanding of the data and to prioritize tasks for follow up. Details like axis orientation or legends, while present, are generally cleaned up and prettified if the graph is going to be used for communication later.\n\n22.2.1 One-dimension graphs\n\n\n\n\n\n\n\n\nGraph\nCode\nUse\n\n\n\n\nFive-number summary\nfivenum(), summary()\n\n\n\nBoxplots\nboxplot(pollution$pm25, col = \"blue\")\nCommonly plot outliers that go beyond the bulk of the data\n\n\nBarplot\ntable(pollution$region) %&gt;% barplot(col = \"wheat\")\nFor visualizing categorical data, with the number of entries for each category being proportional to the height of the bar\n\n\nHistogram\nhist(pollution$pm25, col = \"green\", breaks = 100)\nrug(pollution$pm25)\nabline(v = 12, lwd = 2)\nabline(v = median(pollution$pm25), col = \"magenta\", lwd = 4)\nCheck skewness of the data, symmetry, multi-modality, and other features\n\n\nDensity plot\na &lt;- density(airquality$Ozone, na.rm = TRUE)\nplot(a)\nComputes a non-parametric estimate of the distribution of a variables\n\n\n\n\n\n22.2.2 Multi-dimension graphs\n\n\n\n\n\n\n\n\nGraph\nCode\nUse\n\n\n\n\nMultiple 1-D plots\nboxplot(pm25 ~ region, data = pollution, col = \"red\")\npar(mfrow = c(2, 1), mar = c(4, 4, 2, 1))\nhist(subset(pollution, region == \"east\")$pm25, col = \"green\")\nwith(subset(airquality, Month == 5), points(Wind, Ozone, col = \"blue\"))\nhist(subset(pollution, region == \"west\")$pm25, col = \"green\")&gt; boxplot(pm25 ~ region, data = pollution, col = \"red\")\npar(mfrow = c(2, 1), mar = c(4, 4, 2, 1))\nhist(subset(pollution, region == \"east\")$pm25, col = \"green\")\nwith(subset(airquality, Month == 5), points(Wind, Ozone, col = \"blue\"))\nhist(subset(pollution, region == \"west\")$pm25, col = \"green\")\nFor seeing the relationship between two variables, especially when one is naturally categorical\n\n\nScatterplots\nwith(pollution, plot(latitude, pm25, col = region)) &gt; abline(h = 12, lwd = 2, lty = 2)\nlevels(pollution$region)\npar(mfrow = c(2, 1), mar = c(4, 4, 2, 1))\nhist(subset(pollution, region == \"east\")$pm25, col = \"green\")\nhist(subset(pollution, region == \"west\")$pm25, col = \"green\")\nVisualizing two continuous variables\n\n\nScatter plot matrix\npairs(~var1+var2+var3+var4+var5, data=data, main=\"scatterplot matrix\")\nIt is used to find the correlation between a variable and other variables, and to select the important variables, which is also known as variable selection\n\n\nSmooth scatterplots\nwith(airquality, {plot(Temp, Ozone), lines(loess.smooth(Temp, Ozone))\nSimilar to scatterplots but rather plots a 2-D histogram. Can be useful for scatterplots with many many data points"
  },
  {
    "objectID": "graphs.html#the-base-plotting-system",
    "href": "graphs.html#the-base-plotting-system",
    "title": "22  Graphs",
    "section": "22.3 The Base Plotting System",
    "text": "22.3 The Base Plotting System\nThe base plotting system is the original plotting system for R. The basic model is sometimes referred to as the “artist’s palette” model. The idea is you start with blank canvas and build up from there.\nYou will typically start with a plot function (or similar plot creating function) to initiate a plot and then annotate the plot. If you don’t have a completely well-formed idea of how you want your data to look, you’ll often start by “throwing some data on the page” and then slowly add more information to it as our thought process evolves.\nThere are a few downsides though:\n\nYou can’t go backwards once the plot has started.\nWhile the base plotting system is nice in that it gives you the flexibility to specify these kinds of details to painstaking accuracy, sometimes it would be nice if the system could just figure it out for you.\nIt’s difficult to describe or translate a plot to others because there’s no clear graphical language or grammar that can be used to communicate what you’ve done.\n\n\n22.3.1 How to Create a Base Plot?\n\nFirst, you must read the data into R with read.csv(). For example, the avgpm25.csv dataset contains the annual mean PM2.5 averaged over the period 2008 through 2010\nclass &lt;- c(\"numeric\", \"character\", \"factor\", \"numeric\", \"numeric\")\npollution &lt;- read.csv(\"data/avgpm25.csv\", colClasses = class)  \nExplicitly launch a graphics device.\nCall a plotting function to make a plot (Note: if you are using a file device, no plot will appear on the screen).\nAnnotate the plot if necessary.\nExplicitly close graphics device with dev.off() (this is very important!).\n\nExample:\n# Open PDF device; create 'myplot.pdf' in my working directory\npdf(file = \"myplot.pdf\", width = 4, height = 3) # The height and width arguments are in units of inches.\n# Create plot and send to a file (no plot appears on screen) \nwith(faithful, plot(eruptions, waiting))\n# Annotate plot; still nothing on screen \ntitle(main = \"Old Faithful Geyser data\") \n# Close the PDF file device \ndev.off() \n# Now you can view the file 'myplot.pdf' on your computer\nLet’s further detail each of the steps involved in creating a base plot.\n\n\n22.3.2 Graphics Devices\nWe can think of a graphics device as being a platform upon which the plot is created. If we create a plot, then a default graphics device is automatically opened for the plot to appear upon. In other words, when you make a plot in R, it has to be “sent” to a specific graphics device.\nThe most common place for a plot to be “sent” is the screen device. Functions like plot() in base, xyplot() in lattice, or qplot in ggplot2 will default to sending a plot to the screen device. Therefore, when making a plot, you need to consider how the plot will be used to determine what device the plot should be sent to.\nThe list of devices supported by your installation of R is found in ?Devices.\n\nUsing the commands pdf, postscript, jpeg, png, or bmp, we can also produce graphics in the formats that correspond to these names.\nThe jpeg, png, and bmp graphics are all raster-style graphics, which may translate poorly when included in a document.\nIn contrast, the pdf, postscript, and windows metafile (win.metafile, available on Windows) formats allow for vector-style graphics, which are scaleable, and better suited to integration in documents.\n\n\n\n\n\n\n\n\nVector\nBitmap\n\n\n\n\nGood for line drawings and plots with solid colors using a modest number of points\ngood for plots with a large number of points, natural scenes or web-based plots\n\n\npdf (line-type graphics, resizes well, usually portable)\npng (good for line drawings or images with solid colors)\n\n\nsvg (XML-based scalable vector graphics; supports animation and interactivity)\njpeg (good for plotting many many many points, does not resize well)\n\n\nwin.metafile\ntiff\n\n\npostscript\nbmp\n\n\n\nIt is possible to open multiple graphics devices (screen, file, or both), for example when viewing multiple plots at once. Plotting can only occur on one graphics device at a time, though.\nOne way of having more than one plot visible is to open additional graphics devices. In a Windows environment this is done by using the command windows() before each additional plot.\nTo create a graphics device without a plot, we call the function that is specific to our operating system (that is, windows for Windows, quartz for Mac, and X11 for Unix).\nThe currently active graphics device can be found by calling dev.cur()\nEvery open graphics device is assigned an integer starting with 2 (there is no graphics device 1). You can change the active graphics device with dev.set(&lt;integer&gt;).\n\n\n22.3.3 Graphics Parameters\nGraphics parameters control how output appears on graphics devices. To get a complete list with their current values, type par(). Some of the parameters mentioned above, namely pch, lwd and col, are examples of graphics parameters. To get the value of a specific parameter, for example pch, type par(\"pch\").\nSome graphics parameters can apply to one or more plots, and others only make sense when applied to graphics devices. For example, to change the symbol for a single plot, we could include the argument pch = 2 in the call to the plot function. However, we could also make this change for all graphics that are produced on the device.\nTo change a graphics parameter for the graphics device, we use the par command. These are some examples:\n\npar(mfrow = c(a,b)) where a and b are integers, will create a matrix of plots on one page, with a rows and b columns. These will be filled by rows; use mfcol if you wish to fill them by columns.\nExample:\n\npar(mfrow = c(2, 2), mar=c(5, 4, 2, 1))\ncurve(x*sin(x), from = 0, to = 100, n = 1001)\ncurve(x*sin(x), from = 0, to = 10, n = 1001)\ncurve(x*sin(x), from = 0, to = 1, n = 1001)\ncurve(x*sin(x), from = 0, to = 0.1, n = 1001)\n\n\n\n\npar(mar = c(bottom, left, top, right)) will create space around each plot, in which to write axis labels and titles. Measurements are in units of character widths.\npar(oma = c(bottom, left, top, right)) will create space around the matrix of plots (an outer margin). Measurements are in units of character widths.\npar(las = 1) rotates labels on the y-axis to be horizontal rather than vertical.\npar(pty = \"s\") forces the plot shape to be square. The alternative is that the plot shape is mutable, which is the default, and corresponds to pty =\"m\".\n\n\n\n22.3.4 Make a Plot with plot(x,y)\nplot(x, y) is used to plot one vector against another, with the x values on the x-axis and the y values on the y-axis.\nThe plot command offers a wide variety of options for customizing the graphic. Each of the following arguments can be used within the plot statement, singly or together, separated by commas\n\ntype = \"p\": Determines the type of plot, with options:\n\n“p” for points (the default);\n“l” for lines;\n“b” for both, with gaps in the lines for the points; …\n\nExample:\n\nwith(airquality, plot(Wind, Ozone, \n    main = \"Ozone and Wind in New York City\",\n    type = \"p\")\n)\n\n\n\n\npoints(x, y): To add points (x[1], y[1]), (x[2], y[2]), … to the current plot.\nlines(x, y): To add lines.\nabline(v = xpos) and abline(h = ypos): to draw vertical or horizontal lines. - col: Both points and lines take the optional input from col (e.g. “red”, “blue”, etc.). The complete list of available colours can be obtained by the colours function (or colors).\nExample: # Plot with a Regression Line\n\nFirst make the plot (as above).\nFit a simple linear regression model using the lm() function.\nTake the output of lm() and pass it to the abline() function which automatically takes the information from the model object and calculates the corresponding regression line\n\ntext(x, y, labels): To add the text labels [i] at the point (x[i], y[i]). The optional input pos is used to indicate where to position the labels in relation to the points.\ntitle(text): To add a title, where text is a character string.\nmain = \"Plot title goes in here\": provides the plot title.\nxlab = \" \" / ylab = \" \": To add axis labels.\nNote: In a call to plot, the arguments for main, sub, and xlab and ylab can be character strings or expressions that contain a mathematical expression. For mathematical typesetting you can use the functions expression(require graphics) and bquote(base):\nExample:\n...\nxlab = expression(alpha),\nylab = expression(100 %*% (alpha^3 - alpha^2) + 15),\n# Mix of mathematical expressions and character strings into a single expression by using the paste function\nmain = expression(paste(\"Function: \",\n   f(alpha) == 100 %*% (alpha^3 - alpha^2) + 15)),\n...\nxlab = expression(alpha) tells R to interpret alpha in the context of the MML (mathematical markup language), producing an α as the label for the x-axis.\npch = k: Determines the shape of points, with k taking a value from 1 to 25.\nlwd = 1: line width, default 1.\nxlim = c(a,b)/ `ylim = c(a,b): Will set the lower and upper limits of the x-axis/y-axis to be a and b, respectively.\n\nAs an example we plot part of the parabola y2 = 4x, as well as its focus and directrix. We make use of the surprisingly useful input type = \"n\", which results in the graph dimensions being established, and the axes being drawn, but nothing else.\nExample:\n\nx &lt;- seq(0, 5, by = 0.01)\ny.upper &lt;- 2*sqrt(x)\ny.lower &lt;- -2*sqrt(x)\ny.max &lt;- max(y.upper)\ny.min &lt;- min(y.lower)\nplot(c(-2, 5), c(y.min, y.max), type = \"n\", xlab = \"x\", ylab = \"y\")\nlines(x, y.upper)\nlines(x, y.lower)\nabline(v=-1)\npoints(1, 0)\ntext(1, 0, \"focus (1, 0)\", pos=4)\ntext(-1, y.min, \"directrix x = -1\", pos = 4)\ntitle(\"The parabola y^2 = 4*x\")\n\n\n\n\n\n\n22.3.5 Augmenting a Plot\nA traditional plot can be augmented using any of a number of different tools after its creation. A number of these different steps are detailed below:\n\nStart by creating the plot object, which sets up the dimensions of the space, but omit any plot objects for the moment:\nExample:\nopar1 &lt;- par(las = 1, mar=c(4,4,3,2))   \nplot(ufc$dbh.cm, ufc$height.m, axes=FALSE, \n    xlab=\"\",\n    ylab=\"\",\n    type=\"n\"\n)\nNext, we add the points. Here we use different colours and symbols for different heights of trees: those that are realistic, and those that are not, which may reflect measurement errors. We use the vectorised ifelse() function:\nExample:\npoints(ufc$dbh.cm, ufc$height.m,     \n  col = ifelse(ufc$height.m &gt; 4.9, \"darkseagreen4\", \"red\"),\n  pch = ifelse(ufc$height.m &gt; 4.9, 1, 3)\n)\nThen we add axes. The following are the simplest possible calls. We can also control the locations of the tickmarks, and their labels; we can overlay different axes, change colour, and so on. ?axis provides the details:\nExample:\naxis(1)\naxis(2)\nWe can next add axis labels using margin text (switching back to vertical direction for the y-axis text):\nExample:\nopar2 &lt;- par(las=0)\nmtext(\"Diameter (cm)\", side=1, line=3)\nmtext(\"Height (m)\", side=2, line=3)\nWrap the plot in the traditional frame. As before, we can opt to use different line types and different colours:\nExample:\nbox()\nFinally, we add a legend:\nExample:\nlegend(x = 60, y = 15, c(\"Normal trees\", \"A weird tree\"),\n    col=c(\"darkseagreen3\", \"red\"),\n    pch=c(1, 3),\n    bty=\"n\"\n)\nNote the first two arguments: the location of the legend can also be expressed relative to the graph components, for example, by “bottomright”.\nIf we wish, we can return the graphics environment to a previous state:\npar(opar1)\n\nCheck out the playwith package, which provides interaction with graphical objects at a level unattainable in base R.\n\n\n22.3.6 Color in Plots\nTypically we add color to a plot, not to improve its artistic value, but to add another dimension to the visualization. It makes sense that the range and palette of colors you use will depend on the kind of data you are plotting. Careful choices of plotting color can have an impact on how people interpret your data and draw conclusions from them.\nThe function colors() lists the names of (657) colors you can use in any plotting function. Typically, you would specify the color in a (base) plotting function via the col argument.\nThe grDevices package has two functions, they differ only in the type of object that they return:\n\ncolorRamp: Take a palette of colors and return a function that takes values between 0 and 1.\npal &lt;- colorRamp(c(\"red\", \"blue\")) \npal(0)\nThe numbers in the matrix will range from 0 to 255 and indicate the quantities of red, green, and blue (RGB) in columns 1, 2, and 3 respectively. there are over 16 million colors that can be expressed in this way.\nThe idea here is that you do not have to provide just two colors in your initial color palette; you can start with multiple colors and colorRamp() will interpolate between all of them.\ncolorRampPalette: Takes a palette of colors and returns a function that takes integer arguments and returns a vector of colors interpolating the palette (like heat.colors() or topo.colors()).\npal &lt;- colorRampPalette(c(\"red\", \"yellow\"))\npal(3)\nReturns 3 colors in between red and yellow. Note that the colors are represented as hexadecimal strings.\nNote that the rgb() function can be used to produce any color via red, green, blue proportions and return a hexadecimal representation:\nrgb(0, 0, 234, maxColorValue = 255)\nPart of the art of creating good color schemes in data graphics is to start with an appropriate color palette that you can then interpolate with a function like colorRamp() or colorRampPalette().\n\nFor improved color palettes you can use the RColorBrewer package. Here is a display of all the color palettes available from this package:\n\nlibrary(RColorBrewer) \ndisplay.brewer.all()\n\n\n\n\n\nThe brewer.pal() function creates nice looking color palettes especially for thematic maps:\n\nlibrary(RColorBrewer)\n(cols &lt;- brewer.pal(3, \"BuGn\"))\n\n[1] \"#E5F5F9\" \"#99D8C9\" \"#2CA25F\"\n\n\nThese three colors make up your initial palette. Then you can pass them to colorRampPalette() to create my interpolating function.\n\npal &lt;- colorRampPalette(cols)\n\nNow you can plot your data using this newly created color (ramp) palette:\nimage(volcano, col = pal(20))\nThe smoothScatter() function is very useful for making scatterplots of very large datasets:\n\nset.seed(1)\nx &lt;- rnorm(10000)\ny &lt;- rnorm(10000) \nsmoothScatter(x, y)\n\n\n\n\nAdding transparency: Color transparency can be added via the alpha parameter to rgb() to produce color specifications with varying levels of transparency.\n\nrgb(1, 0, 0, 0.1)\n\n[1] \"#FF00001A\"\n\n\nTransparency can be useful when you have plots with a high density of points or lines. If you add some transparency to the black circles, you can get a better sense of the varying density of the points in the plot.\n\n# x, y from the previous example\nplot(x, y, pch = 19, col = rgb(0, 0, 0, 0.15)) \n\n\n\n\n\n\n\n22.3.7 Copying Plots\nNote that copying a plot is not an exact operation, so the result may not be identical to the original:\n## Copy my plot to a PNG file  (follows the previous code from pdf creation)\ndev.copy(png, file = \"geyserplot.png\") \n\n## Don't forget to close the PNG device! \ndev.off()\n\n\n22.3.8 Complementary Packages\n\ngraphics: contains plotting functions for the “base” graphing systems, including plot, hist, boxplot and many others\ngrdevices: contains all the code implementing the various graphics devices, including X11, PDF, PostScript, PNG, etc."
  },
  {
    "objectID": "graphs.html#the-lattice-system",
    "href": "graphs.html#the-lattice-system",
    "title": "22  Graphs",
    "section": "22.4 The Lattice System",
    "text": "22.4 The Lattice System\nTrellis graphics are a data visualisation framework developed at the Bell Labs, which have been implemented in R as the lattice package.\nTrellis graphics are a set of techniques for displaying multidimensional data. They allow great flexibility for producing conditioning plots; that is, plots obtained by conditioning on the value of one of the variables.\nTo use Trellis graphics you must first you must load the lattice package with the library function: library(lattice).\n\n22.4.1 When to Use Trellis Plots\n\nLattice plots tend to be most useful for conditioning types of plots, i.e. looking at how y changes with x across levels of z. These types of plots are useful for looking at multidimensional data and often allow you to squeeze a lot of information into a single window or page.\nWith the lattice system, plots are created with a single function call, such as xyplot() or bwplot().\nNote that there is no real distinction between the functions that create or initiate plots and the functions that annotate plots because it all happens at once.\nAnother difference from base plotting is that things like margins and spacing are set automatically. This is possible because entire plot is specified at once via a single function call.\nThe notion of panels comes up a lot with lattice plots because you typically have many panels in a lattice plot (each panel typically represents a condition, like “region”).\nOnce a plot is created, you cannot “add” to the plot (but of course you can just make it again with modifications).\n\n\n\n22.4.2 How to Use Lattice\nTo illustrate the use of the lattice package we use the ufc dataset, where dbh (diameter at breast height) and height vary by species. That is, the plots are conditioned on the value of the variable species.\nExample:\n# These plots display a distribution of values taken on by the variable dbh, divided up according to the value of the variable species\ndensityplot(~ dbh.cm | species, data = ufc) # A density plot\nbwplot(~ dbh.cm | species, data = ufc) # # A box and whiskers plot\nhistogram(~ dbh.cm | species, data = ufc) # A histogram\n# We plot height as a function of dbh.\nxyplot(height.m ~ dbh.cm | species, data = ufc) # A scatterplot\nAll four commands require a formula object, which is described using ~and |.\n\nIf a dataframe is passed to the function, using the argument data, then the column names of the dataframe can be used for describing the model.\nWe interpret y ~ x | a as saying we want y as a function of x, divided up by the different levels of a.\nIf `a is not a factor then a factor will be created by coercion.\nIf we are just interested in x we still include the ~ symbol, so that R knows that we are specifying a model.\nIf we wish to provide within-panel conditioning on a second variable, then we use the group argument.\n\nIn order to display numerous lattice objects on a graphics device, we call the print function with the split and more arguments.\nExample:\n# Place a lattice object (called my.lat) in the top-right corner of a 3-row, 2-column graphics device, and allow for more objects\nprint(my.lat, split = c(2,1,2,3), more = TRUE)\nSee ?print.trellis for further details.\nGraphics produced by lattice are highly customisable.\nExample:\nxyplot(height.m ~ dbh.cm | species,\n    data = ufc,\n    subset = species %in% list(\"WC\", \"GF\"),\n    panel = function(x, y, ...) {\n        panel.xyplot(x, y, ...),       \n        panel.abline(lm(y~x), ...)\n    },\n    xlab = \"Diameter (cm)\",\n    ylab = \"Height (m)\"\n    )\nNote that the subset argument is a logical expression or a vector of integers, and is used to restrict the data that are plotted. We can also change the order of the panels using the index.cond argument (see ?xyplot for more details). The panels are plotted bottom left to top right by default, or top left to bottom right if the argument as.table = TRUE is supplied.\nThe panel argument accepts a function, the purpose of which is to control the appearance of the plot in each panel. The panel function should have one input argument for each variable in the model, not including the conditioning variable."
  },
  {
    "objectID": "graphs.html#d-plots",
    "href": "graphs.html#d-plots",
    "title": "22  Graphs",
    "section": "22.5 3D Plots",
    "text": "22.5 3D Plots\nR provides considerable functionality for constructing 3D graphics, using either the base graphics engine or the lattice package. However, its is recommended to use the lattice package, because the data can be supplied to the lattice functions in a familiar structure: observations in rows and variables in columns, unlike that required by the base 3D graphics engine (observations in a grid).\nExample:\nufc.plots &lt;- read.csv(\"../data/ufc-plots.csv\")\nstr(ufc.plots)\nlibrary(lattice)\nwireframe(vol.m3.ha ~ east * north,\n    main = expression(paste(\"Volume (\", m^3, ha^{-1}, \")\", sep =        \"\")),\n    xlab = \"East (m)\", ylab = \"North (m)\",\n    data = ufc.plots\n    )\nTo learn more about base-graphics 3D plots, run the demonstrations demo(persp) and demo(image) and look at the examples presented."
  },
  {
    "objectID": "graphs.html#ggplot2",
    "href": "graphs.html#ggplot2",
    "title": "22  Graphs",
    "section": "22.6 ggplot2",
    "text": "22.6 ggplot2\nggplot2 is one of the most elegant and most versatile systems for making graphs. ggplot2 implements the grammar of graphics, a coherent system for describing and building graphs.\nggplot2 splits the difference between base and lattice in a number of ways. Taking cues from lattice, the ggplot2 system automatically deals with spacings, text, titles but also allows you to annotate by “adding” to a plot.\nYou can find more information in the Graphs section here.\n\n22.6.1 Installation\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\nA typical plot with the ggplot package looks as follows:\ndata(mpg)\nqplot(displ, hwy, data = mpg)\nIn ggplot2, aesthetics are the things we can see (e.g. position, color, fill, shape, line type, size). You can use aesthetics in ggplot2 via the aes() function:\nggplot(data, aes(x=var1, y=var2))\n\n\n22.6.2 Geometric objects\nGeometric objects are the plots or graphs you want to put in the chart.\nYou can use geom_point() to create a scatterplot, geom_line() to create a line plot, and geom_boxplot() to create a boxplot in the chart.\nhelp.search(\"geom_\", package=\"ggplot2\")\nIn ggplot2, geom is also the layers of the chart. You can add in one geom object after another, just like adding one layer after another layer.\nggplot(data, aes(x=var1, y=var2)) + \n  geom_point(aes(color=\"red\"))\n\n\n22.6.3 Plotly\nPlotly JS allows you to create interactive, publication-quality charts. You can create a Plotly chart using ggplot:\ninstall.packages(\"plotly\")\nset.seed(12)\nvar1 &lt;- rnorm(100, mean=1, sd=1)\nvar2 &lt;- rnorm(100, mean=2, sd=1)\ndata &lt;- data.frame(var1, var2)\ngg &lt;- ggplot(data) + geom_line(aes(x=var1, y=var2))\ng &lt;- ggplotly(gg)"
  },
  {
    "objectID": "ggplot.html#the-layered-grammar-of-ggraphics",
    "href": "ggplot.html#the-layered-grammar-of-ggraphics",
    "title": "23  Creating a ggplot",
    "section": "23.1 The Layered Grammar of Ggraphics",
    "text": "23.1 The Layered Grammar of Ggraphics\nThe grammar of graphics is a template made of seven parameters (the bracketed words that appear in the template) for building plots. The grammar of graphics is based on the insight that you can uniquely describe any plot as a combination of a dataset, a geom, a set of mappings, a stat, a position adjustment, a coordinate system, a faceting scheme, and a theme.\nggplot(data = &lt;DATA&gt;) + \n  &lt;GEOM_FUNCTION&gt;(\n     mapping = aes(&lt;MAPPINGS&gt;),\n     stat = &lt;STAT&gt;, \n     position = &lt;POSITION&gt;\n  ) +\n  &lt;COORDINATE_FUNCTION&gt; +\n  &lt;FACET_FUNCTION&gt;\nTo build a basic plot from scratch:\n\nYou could start with a dataset and then transform it into the information that you want to display (with a stat).\nNext, you could choose a geometric object to represent each observation in the transformed data.\nYou could then use the aesthetic properties of the geoms to represent variables in the data. You would map the values of each variable to the levels of an aesthetic.\nYou’d then select a coordinate system to place the geoms into, using the location of the objects (which is itself an aesthetic property) to display the values of the x and y variables.\nYou could further adjust the positions of the geoms within the coordinate system (a position adjustment) or split the graph into subplots (faceting).\nYou could also extend the plot by adding one or more additional layers, where each additional layer uses a dataset, a geom, a set of mappings, a stat, and a position adjustment.\n\nTo learn more about the theoretical underpinnings that describes the theory of ggplot2 in detail, click here: The Layered Grammar of Graphics."
  },
  {
    "objectID": "ggplot.html#define-a-plot-object",
    "href": "ggplot.html#define-a-plot-object",
    "title": "23  Creating a ggplot",
    "section": "23.2 Define a Plot Object",
    "text": "23.2 Define a Plot Object\nWith ggplot2, you begin a plot with the function ggplot(), defining a plot object that you then add layers to.\nThe first argument of ggplot() is the dataset to use in the graph and so ggplot(data = penguins) creates an empty graph that is primed to display the penguins data, but since we haven’t told it how to visualize it yet, for now it’s empty (it’s like an empty canvas you’ll paint the remaining layers of your plot onto).\nExample:\n\nggplot(data = penguins)"
  },
  {
    "objectID": "ggplot.html#map-the-data",
    "href": "ggplot.html#map-the-data",
    "title": "23  Creating a ggplot",
    "section": "23.3 Map the Data",
    "text": "23.3 Map the Data\nNext, we need to tell ggplot() how the information from our data will be visually represented. The mapping argument of the ggplot() function defines how variables in your dataset are mapped to visual properties (aesthetics) of your plot.\nThe mapping argument is always defined in the aes() function, and the x and yarguments of aes() specify which variables to map to the x and y axes.\nExample:\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n)\n\n\n\n\nOur empty canvas now has more structure – it’s clear where flipper lengths will be displayed (on the x-axis) and where body masses will be displayed (on the y-axis). But the penguins themselves are not yet on the plot. This is because we have not yet determine how to represent the observations from our dataframe on our plot."
  },
  {
    "objectID": "ggplot.html#represent-the-data-geoms",
    "href": "ggplot.html#represent-the-data-geoms",
    "title": "23  Creating a ggplot",
    "section": "23.4 Represent the Data (geoms)",
    "text": "23.4 Represent the Data (geoms)\nTo do so, we need to define a geom: the geometrical object that a plot uses to represent data. These geometric objects are made available in ggplot2 with functions that start with geom_.\nFor example, bar charts use geom_bar(), line charts use geom_line(), boxplots use geom_boxplot(), scatterplots use geom_point(), and so on.\nExample:\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nNote: One common problem when creating ggplot2 graphics is to put the + in the wrong place: it has to come at the end of the line, not the start.\nYou can rewrite the previous plot more concisely:\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nOr using the pipe, |&gt;:\n\npenguins |&gt; \n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nNow we have something that looks like what we might think of as a “scatterplot”.\nYou can also set the visual properties of your geom manually as an argument of your geom function (outside of aes()) instead of relying on a variable mapping to determine the appearance. For example, we can make all of the points in our plot blue:\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point(color = \"blue\")\n\n\n\n\n\n23.4.1 Help with geoms\nThe best place to get a comprehensive overview of all of the geoms ggplot2 offers, as well as all functions in the package, is this page: https://ggplot2.tidyverse.org/reference.\nTo learn more about any single geom, use the help (e.g., ?geom_smooth).\nSee also https://exts.ggplot2.tidyverse.org/gallery/ for a sampling of community made geoms."
  },
  {
    "objectID": "ggplot.html#add-aesthetics-and-layers",
    "href": "ggplot.html#add-aesthetics-and-layers",
    "title": "23  Creating a ggplot",
    "section": "23.5 Add Aesthetics and Layers",
    "text": "23.5 Add Aesthetics and Layers\nYou will we need to modify the aesthetic mapping, inside of aes().\nExample:\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nOnce you map an aesthetic, ggplot2 takes care of the rest. It selects a reasonable scale to use with the aesthetic, and it constructs a legend that explains the mapping between levels and values. For x and y aesthetics, ggplot2 does not create a legend, but it creates an axis line with tick marks and a label. The axis line provides the same information as a legend; it explains the mapping between locations and values.\nWhen a categorical variable is mapped to an aesthetic, ggplot2 will automatically assign a unique value of the aesthetic (here a unique color) to each unique level of the variable (each of the three species), a process known as scaling. ggplot2 will also add a legend that explains which values correspond to which levels.\nNow let’s add one more layer: a smooth curve displaying the relationship between body mass and flipper length. Since this is a new geometric object representing our data, we will add a new geom as a layer on top of our point geom: geom_smooth(). And we will specify that we want to draw the line of best fit based on a linear model with method = \"lm\".\nExample:\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nWhen aesthetic mappings are defined in ggplot(), at the global level, they’re passed down to each of the subsequent geom layers of the plot. However, each geom function in ggplot2 can also take a mapping argument, which allows for aesthetic mappings at the local level that are added to those inherited from the global level. In other words, if you place mappings in a geom function, ggplot2 will treat them as local mappings for the layer. It will use these mappings to extend or overwrite the global mappings for that layer only. This makes it possible to display different aesthetics in different layers.\nExample:\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species)) +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nNote that since we want points to be colored based on species but don’t want the lines to be separated out for them, we should specify color = species for geom_point() only.\nYou can specify different data for each layer. Here, we use red points as well as open circles to highlight two-seater cars. The local data argument in geom_point() overrides the global data argument in ggplot() for that layer only.\nExample:\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  geom_point(\n    data = mpg |&gt; filter(class == \"2seater\"), \n    color = \"red\"\n  ) +\n  geom_point(\n    data = mpg |&gt; filter(class == \"2seater\"), \n    shape = \"circle open\", size = 3, color = \"red\"\n  )\n\n\n\n\nIt’s generally not a good idea to represent information using only colors on a plot, as people perceive colors differently due to color blindness or other color vision differences. Therefore, in addition to color, we can also map species to the shape aesthetic.\nExample:\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n23.5.1 Help with aes\nYou can learn more about all possible aesthetic mappings in the aesthetic specifications vignette at https://ggplot2.tidyverse.org/articles/ggplot2-specs.html. Remember that the specific aesthetics you can use for a plot depend on the geom you use to represent the data."
  },
  {
    "objectID": "ggplot.html#statistical-transformation",
    "href": "ggplot.html#statistical-transformation",
    "title": "23  Creating a ggplot",
    "section": "23.6 Statistical Transformation",
    "text": "23.6 Statistical Transformation\nMany graphs, like scatterplots, plot the raw values of a dataset. Other graphs, like bar charts, calculate new values to plot. The algorithm used to calculate new values for a graph is called a stat, short for statistical transformation.\nYou can figure out which stat a geom uses by inspecting the default value for the stat argument. For example, ?geom_bar shows that the default value for stat is “count”, which means that geom_bar() uses stat_count(). To find the possible variables that can be computed by the stat, look for the section titled “computed variables” in the help for geom_bar().\nEvery geom has a default stat; and every stat has a default geom. This means that you can typically use geoms without worrying about the underlying statistical transformation. However, there are three reasons why you might need to use a stat explicitly:\n\nYou might want to override the default stat. In the following example we change the stat of geom_bar() from count (the default) to identity. This lets us map the height of the bars to the raw values of a y variable.\nExample:\n\ndiamonds |&gt;\n  count(cut) |&gt;\n  ggplot(aes(x = cut, y = n)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\nYou might want to override the default mapping from transformed variables to aesthetics.\nExample:\n\n# Display a bar chart of proportions, rather than counts\nggplot(diamonds, aes(x = cut, y = after_stat(prop), group = 1)) + \n  geom_bar()\n\n\n\n\nYou might want to focus on the statistical transformation in your code. For example, you might use stat_summary(), which summarizes the y values for each unique x value, to draw attention to the summary that you’re computing:\nExample:\n\nggplot(diamonds) + \n  stat_summary(\n    aes(x = cut, y = depth),\n    fun.min = min,\n    fun.max = max,\n    fun = median\n  )"
  },
  {
    "objectID": "ggplot.html#position-adjustments",
    "href": "ggplot.html#position-adjustments",
    "title": "23  Creating a ggplot",
    "section": "23.7 Position Adjustments",
    "text": "23.7 Position Adjustments\nYou can color a bar chart using either the color aesthetic, or, more usefully, the fill aesthetic:\nExample:\n\nggplot(mpg, aes(x = drv, color = drv)) + \n  geom_bar()\n\n\n\n\nExample:\n\nggplot(mpg, aes(x = drv, fill = drv)) + \n  geom_bar()\n\n\n\n\nNote that if you map the fill aesthetic to another variable, like class, the bars are automatically stacked. In our example, each colored rectangle represents a combination of drv and class.\nExample:\n\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar()\n\n\n\n\nThe stacking is performed automatically using the position adjustment specified by the position argument. If you don’t want a stacked bar chart, you can use one of four other options:\n\nposition = \"identity\": will place each object exactly where it falls in the context of the graph. See how bars overlap and why the identity position adjustment is more useful for 2d geoms, like points, where it is the default.\n\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar(alpha = 1/5, position = \"identity\")\n\n\n\n\nposition = \"fill\": works like stacking, but makes each set of stacked bars the same height. This makes it easier to compare proportions across groups.\nExample:\n\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar(position = \"fill\")\n\n\n\n\nposition = \"dodge\": places overlapping objects directly beside one another. This makes it easier to compare individual values.\nExample:\n\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\nposition = \"jitter\": adds a small amount of random noise to each point in a scatterplot. This spreads the points out to solve the problem of overplotting, which makes it difficult to see the distribution of the data.\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point(position = \"jitter\")"
  },
  {
    "objectID": "ggplot.html#coordinate-system",
    "href": "ggplot.html#coordinate-system",
    "title": "23  Creating a ggplot",
    "section": "23.8 Coordinate System",
    "text": "23.8 Coordinate System\nThe default coordinate system is the Cartesian coordinate system where the x and y positions act independently to determine the location of each point.\nThere are two other coordinate systems that are occasionally helpful.\n\ncoord_quickmap(): sets the aspect ratio correctly for plotting spatial data with ggplot2. See more in the Maps chapter of the ggplot2 book.\nExample:\n\nlibrary(maps)\n\nnz &lt;- map_data(\"nz\")\n\nggplot(nz, aes(x = long, y = lat, group = group)) +\n  geom_polygon(fill = \"white\", color = \"black\")\n\n\n\nggplot(nz, aes(x = long, y = lat, group = group)) +\n  geom_polygon(fill = \"white\", color = \"black\") +\n  coord_quickmap()\n\n\n\n\ncoord_polar(): uses polar coordinates. Polar coordinates can reveal interesting connections between a bar chart and a Coxcomb chart.\nExample:\n\nbar &lt;- ggplot(data = diamonds) + \n  geom_bar(\n    mapping = aes(x = clarity, fill = clarity), \n    show.legend = FALSE,\n    width = 1\n  ) + \n  theme(aspect.ratio = 1)\n\nbar + coord_flip()\n\n\n\nbar + coord_polar()"
  },
  {
    "objectID": "ggplot.html#facets",
    "href": "ggplot.html#facets",
    "title": "23  Creating a ggplot",
    "section": "23.9 Facets",
    "text": "23.9 Facets\nFacets splits a plot into subplots that each display one subset of the data based on a categorical variable.\nExample:\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_wrap(~cyl)\n\n\n\n\nTo facet your plot with the combination of two variables, switch from facet_wrap() to facet_grid(). The first argument of facet_grid() is also a formula, but now it’s a double sided formula: rows ~ cols.\nExample:\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_grid(drv ~ cyl)\n\n\n\n\nBy default each of the facets share the same scale and range for x and y axes. This is useful when you want to compare data across facets but it can be limiting when you want to visualize the relationship within each facet better. Setting the scales argument in a faceting function to free_x will allow for different scales of x-axis across columns, free_y will allow for different scales on y-axis across rows, and free will allow both.\nExample:\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_grid(drv ~ cyl, scales = \"free\")"
  },
  {
    "objectID": "ggplot.html#improve-labels",
    "href": "ggplot.html#improve-labels",
    "title": "23  Creating a ggplot",
    "section": "23.10 Improve Labels",
    "text": "23.10 Improve Labels\nAnd finally, we can improve the labels of our plot using the labs() function in a new layer:\n\ntitle, adds a title.\nsubtitle, adds a subtitle to the plot.\nx, is the x-axis label.\ny, is the y-axis label.\ncolor and shape define the label for the legend.\n\nIn addition, we can improve the color palette to be colorblind safe with the scale_color_colorblind() function from the ggthemes package.\nExample:\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Body mass and flipper length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n    color = \"Species\", shape = \"Species\"\n  ) +\n  scale_color_colorblind()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nThe purpose of a plot title is to summarize the main finding. Avoid titles that just describe what the plot is (e.g. “Fuel efficcieny generally decreases with engine size”).\n\n23.10.1 Mathematical labels\nIt’s possible to use mathematical equations instead of text strings. Just switch ” ” out for quote() and read about the available options in ?plotmath:\nExample:\nggplot(df, aes(x, y)) +\n  geom_point() +\n  labs(\n    x = quote(x[i]),\n    y = quote(sum(x[i] ^ 2, i == 1, n))\n  )"
  },
  {
    "objectID": "ggplot.html#save-your-plot",
    "href": "ggplot.html#save-your-plot",
    "title": "23  Creating a ggplot",
    "section": "23.13 Save Your Plot",
    "text": "23.13 Save Your Plot\nOnce you’ve made a plot, you might want to get it out of R by saving it as an image that you can use elsewhere. That’s the job of ggsave(), which will save the plot most recently created to disk:\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\nggsave(filename = \"penguin-plot.png\")\n\nSaving 7 x 5 in image\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nThis will save your plot to your working directory."
  },
  {
    "objectID": "typesofgraphs.html#visualizing-categorial-variables",
    "href": "typesofgraphs.html#visualizing-categorial-variables",
    "title": "24  Types of Graphs",
    "section": "24.1 Visualizing Categorial Variables",
    "text": "24.1 Visualizing Categorial Variables\nA variable is categorical if it can only take one of a small set of values.\n\n24.1.1 Bar Chart\nTo examine the distribution of a categorical variable, you can use a bar chart. The height of the bars displays how many observations occurred with each x value.\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nExample:\n\nggplot(penguins, aes(x = species)) +\n  geom_bar()\n\n\n\n\n\n\n24.1.2 Bar Plots\nIn bar plots of categorical variables with non-ordered levels, like the penguin species above, it’s often preferable to reorder the bars based on their frequencies. Doing so requires transforming the variable to a factor (how R handles categorical data) and then reordering the levels of that factor.\nExample:\n\nggplot(penguins, aes(x = fct_infreq(species))) +\n  geom_bar()"
  },
  {
    "objectID": "typesofgraphs.html#visualizing-numerical-variables",
    "href": "typesofgraphs.html#visualizing-numerical-variables",
    "title": "24  Types of Graphs",
    "section": "24.2 Visualizing Numerical Variables",
    "text": "24.2 Visualizing Numerical Variables\nA variable is numerical (or quantitative) if it can take on a wide range of numerical values, and it is sensible to add, subtract, or take averages with those values. Numerical variables can be continuous or discrete.\n\n24.2.1 Histogram\nOne commonly used visualization for distributions of continuous variables is a histogram.\nExample:\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 200)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nA histogram divides the x-axis into equally spaced bins and then uses the height of a bar to display the number of observations that fall in each bin. In the graph above, the tallest bar shows that 39 observations have a body_mass_g value between 3,500 and 3,700 grams, which are the left and right edges of the bar.\nYou can set the width of the intervals in a histogram with the binwidth argument, which is measured in the units of the x variable.\nExample:\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 20)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 2000)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nAn alternative visualization for distributions of numerical variables is a density plot. A density plot is a smoothed-out version of a histogram and a practical alternative, particularly for continuous data that comes from an underlying smooth distribution.\nExample:\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_density()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`)."
  },
  {
    "objectID": "typesofgraphs.html#visualizing-relationships",
    "href": "typesofgraphs.html#visualizing-relationships",
    "title": "24  Types of Graphs",
    "section": "24.3 Visualizing Relationships",
    "text": "24.3 Visualizing Relationships\nTo visualize a relationship we need to have at least two variables mapped to aesthetics of a plot.\n\n24.3.1 Numerical and Categorical Variable\nYou can use side-by-side box plots. A boxplot is a type of visual shorthand for measures of position (percentiles) that describe a distribution. It is also useful for identifying potential outliers.\nExample:\n\nggplot(penguins, aes(x = species, y = body_mass_g)) +\n  geom_boxplot()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nAlternatively, we can make density plots with geom_density().\nExample:\n\nggplot(penguins, aes(x = body_mass_g, color = species)) +\n  geom_density(linewidth = 0.75)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\nAdditionally, we can map species to both color and fill aesthetics and use the alpha aesthetic to add transparency to the filled density curves. This aesthetic takes values between 0 (completely transparent) and 1 (completely opaque). In the following plot it’s set to 0.5.\n\nggplot(penguins, aes(x = body_mass_g, color = species, fill = species)) +\n  geom_density(alpha = 0.5)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n24.3.2 Two Categorical Variables\nWe can use stacked bar plots to visualize the relationship between two categorical variables. For example, the following two stacked bar plots both display the relationship between island and species, or specifically, visualizing the distribution of species within each island.\nExample:\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar()\n\n\n\n\nThe following plot, a relative frequency plot created by setting position = \"fill\" in the geom, is more useful for comparing species distributions across islands since it’s not affected by the unequal numbers of penguins across the islands.\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar(position = \"fill\")\n\n\n\n\nIn creating these bar charts, we map the variable that will be separated into bars to the x aesthetic, and the variable that will change the colors inside the bars to the fill aesthetic.\n\n\n24.3.3 Two Numerical Variables\nA scatterplot is probably the most commonly used plot for visualizing the relationship between two numerical variables.\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n24.3.4 Three or More Variables\nWe can incorporate more variables into a plot by mapping them to additional aesthetics. For example, in the following scatterplot the colors of points represent species and the shapes of points represent islands.\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = island))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n24.3.5 Facets\nAdding too many aesthetic mappings to a plot makes it cluttered and difficult to make sense of. Another way, which is particularly useful for categorical variables, is to split your plot into facets, subplots that each display one subset of the data.\nTo facet your plot by a single variable, use facet_wrap(). The first argument of facet_wrap() is a formula3, which you create with ~ followed by a variable name. The variable that you pass to facet_wrap() should be categorical.\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = species)) +\n  facet_wrap(~island)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nIf you don’t specify the width and height they will be taken from the dimensions of the current plotting device. For reproducible code, you’ll want to specify them."
  },
  {
    "objectID": "random.html#always-remember-to-set-your-seed",
    "href": "random.html#always-remember-to-set-your-seed",
    "title": "25  Random Numbers",
    "section": "25.1 Always Remember to Set Your Seed!",
    "text": "25.1 Always Remember to Set Your Seed!\nWhen simulating any random numbers it is essential to set the random number seed. Setting the random number seed with set.seed() ensures reproducibility of the sequence of random numbers.\nExample:\n\nset.seed(1)\nrnorm(5)\n\n[1] -0.6264538  0.1836433 -0.8356286  1.5952808  0.3295078\n\n\nExample: Simulating a Linear Model\nSuppose we want to simulate from the following linear model \\(y=β_0+β_1x+ε\\) where \\(ε ∼ N(0, 22)\\)\nAssume \\(x ∼ N(0, 12)\\), \\(β_0 = 0.5\\) and \\(β_1 = 2\\). The variable x might represent an important predictor of the outcome y.\nHere’s how you could do that in R:\n\n## Always set your seed! \nset.seed(20) \n## Simulate predictor variable \nx &lt;- rnorm(100) \n## Simulate the error term \ne &lt;- rnorm(100, 0, 2) \n## Compute the outcome via the model \ny &lt;- 0.5 + 2 * x + e \nsummary(y)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-6.4084 -1.5402  0.6789  0.6893  2.9303  6.5052 \n\nplot(x, y)"
  },
  {
    "objectID": "random.html#random-sampling",
    "href": "random.html#random-sampling",
    "title": "25  Random Numbers",
    "section": "25.2 Random Sampling",
    "text": "25.2 Random Sampling\nThe sample() function draws randomly from a specified set of (scalar) objects allowing you to sample from arbitrary distributions of numbers.\nExample:\n\nset.seed(1)\nsample(1:10,4)\n\n[1] 9 4 7 1\n\n# Doesn't have to be numbers\nsample(letters, 5)\n\n[1] \"b\" \"w\" \"k\" \"n\" \"r\"\n\n\nTo sample more complicated things, such as rows from a dataframe or a list, you can sample the indices into an object rather than the elements of the object itself:\n\nset.seed(20)\nidx &lt;- seq_len(nrow(airquality))## Create index vector \nsamp &lt;- sample(idx, 6) ##Sample from the index vector \nairquality[samp, ]\n\n    Ozone Solar.R Wind Temp Month Day\n107    NA      64 11.5   79     8  15\n120    76     203  9.7   97     8  28\n130    20     252 10.9   80     9   7\n98     66      NA  4.6   87     8   6\n29     45     252 14.9   81     5  29\n45     NA     332 13.8   80     6  14"
  },
  {
    "objectID": "products.html#manipulate",
    "href": "products.html#manipulate",
    "title": "26  Packages for Data Products",
    "section": "26.1 Manipulate",
    "text": "26.1 Manipulate\nThe manipulate package creates a quick interactive graphic that offers simple controls, including sliders, pickers and checkboxes.\nlibrary(manipulate) \nmanipulate(plot(1:x), x = slider(1, 100))\nWith manipulate you can have more than one set of controls by simply adding more arguments to the manipulate function.\nNote that it’s difficult to share a manipulate interactive graphic.\nThis is a link on how manipulate actually works: link"
  },
  {
    "objectID": "products.html#shiny",
    "href": "products.html#shiny",
    "title": "26  Packages for Data Products",
    "section": "26.2 Shiny",
    "text": "26.2 Shiny\nIt is described by RStudio as “A web application framework for R”. “Turn your analyses into interactive web applications No HTML, CSS, or JavaScript knowledge required”.\nOnly those who have shiny installed and have access to your code could run your web page. However, RStudio offers a service for hosting shiny apps (their servers) on a platform called shinyapps.io.\nIf on Windows, make sure that you have Rtools installed. Then, you can install shiny with:\ninstall.packages(\"shiny\") \nlibrary(shiny)\nA shiny app consists of two files. First, a file called ui.R that controls the User Interface (hence the ui in the filename) and secondly, a file server.R that controls the shiny server (hence the server in the filename).\n\n26.2.1 Create ui.R file:\nlibrary(shiny)\nshinyUI(\n  pageWithSidebar( \n    headerPanel(\"Hello Shiny!\"), \n    sidebarPanel( \n      h3('Sidebar text')\n    ),\n    mainPanel( \n      h3('Main Panel text')\n    )\n  )\n)\n\n\n26.2.2 Create server.R file:\nlibrary(shiny) \nshinyServer( \n  function(input, output) {\n  }\n)\nThe current version of Rstudio has a “Run app” button in the upper right hand corner of the editor if a ui.R or server.R file is open.\nTo get used to programming shiny apps, you need to throw away a little of your thinking about R; it’s a different style of programming.\n\n\n26.2.3 Types of inputs\n\nNumeric input:\nnumericInput('id1', 'Numeric input, labeled id1', 0, min = 0, max = 10, step= 1)\nCheckbox input:\ncheckboxGroupInput(\"id2\", \"Checkbox\", c(\"Value 1\" = \"1\", \"Value 2\" = \"2\", \"Value 3\" = \"3\"))\nDate input:\ndateInput(\"date\", \"Date:\")\n\n\n\n26.2.4 Sharing your app\nNow that we have a working app we’d like to share it with the world. It’s much nicer to have it display as a standalone web application.\nThis requires running a shiny server to host the app. Instead of creating and deploying our own shiny server, we’ll rely on RStudio’s service: link\n\nAfter login in shinyapps:\ninstall.packages(\"devtools\")\ninstall.packages(\"shinyapps\")\nRun code:\nshinyapps::setAccountInfo(name='&lt;ACCOUNT NAME&gt;', \n      token='&lt;TOKEN&gt;', \n      secret='&lt;SECRET&gt;')\nSubmit code:\ndeployApp(appName = \"myFirstApp\")\n\n\n\n26.2.5 Build a GUI with html\nCheck out this link"
  },
  {
    "objectID": "products.html#reproducible-presentations",
    "href": "products.html#reproducible-presentations",
    "title": "26  Packages for Data Products",
    "section": "26.3 Reproducible presentations",
    "text": "26.3 Reproducible presentations\n\n26.3.1 Slidify\nSlidify is for building reproducible presentations.\ninstall.packages(\"devtools\") \nlibrary(devtools)/ require(devtools)\ninstall_github('ramnathv/slidify')\ninstall_github('ramnathv'/'slidifyLibraries') o devtools::install_github(pkgs, force = TRUE)\nlibrary(slidify)\n\n\n26.3.2 R studio presenter\nFor more information about R studio presenter, chek out this link\nTip: if you’re sort of a hacker type and you like to tinker with things, use slidify. If you just want to get it done and not worry about it, use RPres. Either way, you really can’t go wrong."
  },
  {
    "objectID": "products.html#interactive-graphs",
    "href": "products.html#interactive-graphs",
    "title": "26  Packages for Data Products",
    "section": "26.4 Interactive Graphs",
    "text": "26.4 Interactive Graphs\n\n26.4.1 rCharts\nlink to rCharts\nrCharts is a way to create interactive javascript visualizations using R.\nrequire(devtools) \ninstall_github('rCharts', 'ramnathv')\n\n\n26.4.2 googleVis\nlink to googlevis\nGoogle has some nice visualization tools built into their products (e.g. Google Maps). These include maps and interactive graphs.\ninstall.packages(\"googleVis\")\nlibrary(googlevis)\n\n\n26.4.3 leaflet\nlink to leaflet\nleaflet seems to be emerging as the most popular R package for creating interactive maps.\nThe map widget (the leaflet() command) starts out a map and then you add elements or modify the map by passing it as arguments to mapping functions.\n\n\n26.4.4 plot.ly\nlink to plot.ly\nplotly relies on the platform/website plot.ly for creating interactive graphics.\nrequire(devtools) \ninstall_github(\"ropensci/plotly\")\nPlotly will give you the json data and gives a tremendous number of options for publishing the graph.\nNotably, plotly allows for integration with ggplot2.\nFor interactive graphics, learning some javascript and following that up with D3 would be the logical next step"
  },
  {
    "objectID": "reproducible.html#goal",
    "href": "reproducible.html#goal",
    "title": "27  Reproducible Reporting",
    "section": "27.1 Goal",
    "text": "27.1 Goal\nThe goal is to have independent people to do independent things with different data, different methods, and different laboratories and see if you get the same result. But the problem is that it’s becoming more and more challenging to do replication or to replicate other studies. Part of the reason is because studies are getting bigger and bigger.\nThe idea behind a reproducible reporting is to create a kind of minimum standard or a middle ground where we won’t be replicating a study, but maybe we can do something in between.\nYou need to make the data available for the original study and the computational methods available so that other people can look at your data and run the kind of analysis that you’ve run, and come to the same findings that you found. If you can take someone’s data and reproduce their findings, then you can, in some sense, validate the data analysis.\nUnderstanding what someone did in a data analysis now requires looking at code and scrutinizing the computer programs that people used."
  },
  {
    "objectID": "reproducible.html#the-data-science-pipeline",
    "href": "reproducible.html#the-data-science-pipeline",
    "title": "27  Reproducible Reporting",
    "section": "27.2 The Data Science Pipeline",
    "text": "27.2 The Data Science Pipeline\nThe basic idea behind reproducibility is to focus on the elements in the blue blox: the analytic data and the computational results.\n\n\n\nFigure 8: The Data Science Pipeline\n\n\nWith reproducibility the goal is to allow the author of a report and the reader of that report to “meet in the middle”."
  },
  {
    "objectID": "reproducible.html#organizing-a-data-analysis",
    "href": "reproducible.html#organizing-a-data-analysis",
    "title": "27  Reproducible Reporting",
    "section": "27.3 Organizing a Data Analysis",
    "text": "27.3 Organizing a Data Analysis\n\nRaw Data:\n\nYou want to store this raw data in your analysis folder.\nIf the data were accessed from the web you want to include things like the URL, where you got the data, what the data set is, a brief description of what it’s for, the date that you accessed the URL on, the website, etc.\nYou may want this in a README file.\nIf you’re using git to track things that are going on in your project, add your raw data, if possible. In the log message, when you add it you can talk about what the website was where you got it, what the URL was, etc.\n\nProcessed Data:\n\nYour processed data should be named so that you can easily see what script generated what data.\nIn any README file or any sort of documentation, it’s important to document what code files were used to transform the raw data into the processed data.\n\nFigures:\n\nExploratory figures.\nFinal figures. The final figures usually make a very small subset of the set of exploratory figures that you might generate. You typically don’t want to inundate people with a lot of figures because then the ultimate message of what you’re trying to communicate tends to get lost in a pile of figures.\n\nScripts:\n\nFinal scripts will be much more clearly commented. You’ll likely have bigger comment blocks for whole sections of code.\n\nR Markdown files:\n\nThey may not be exactly required, but they can be very useful to summarize parts of an analysis or an entire analysis.\nYou can embed code and text into the same document and then you process the document into something readable like a webpage or a PDF file.\n\nFinal Report:\n\nThe point of this is to tell the final story of what you generated here.\nYou’ll have a title, an introduction that motivates your problem, the methods that you used to refine, the results and any measures of uncertainty, and then any conclusions that you might draw from the data analysis that you did, including any pitfalls or potential problems."
  },
  {
    "objectID": "reproducible.html#structure-of-a-data-analysis",
    "href": "reproducible.html#structure-of-a-data-analysis",
    "title": "27  Reproducible Reporting",
    "section": "27.4 Structure of a Data Analysis",
    "text": "27.4 Structure of a Data Analysis\n1. Defining the question:\n\nA proper data analysis has a scientific context, and at least some general question that we’re trying to investigate which will narrow down the kind of dimensionality of the problem. Then we’ll apply the appropriate statistical methods to the appropriate data.\nDefining a question is the most powerful dimension reduction tool you can ever employ.\nThe idea is, if you can narrow down your question as specifically as possible, you’ll reduce the kind of noise that you’ll have to deal with when you’re going through a potentially very large data set.\nThink about what type of question you’re interested in answering before you go delving into all the details of your data set. That will lead you to the data. Which may lead you to applied statistics, which you use to analyze the data.\n\n2. Defining the ideal dataset/Determining what data you can access (the real data set):\n\nsometimes you have to go for something that is not quite the ideal data set.\nYou might be able to find free data on the web. You might need to buy some data from a provider.\nIf the data simply does not exist out there, you may need to generate the data yourself in some way.\n\n3. Obtaining the data:\n\nYou have to be careful to reference the source, so wherever you get the data from, you should always reference and keep track of where it came from.\nIf you get data from an Internet source, you should always make sure at the very minimum to record the URL, which is the web site indicator of where you got the data, and the time and date that you accessed it.\n\n4. Cleaning the data:\n\nRaw data typically needs to be processed in some way to get it into a form where you can model it or feed it into a modeling program.\nIf the data is already pre-processed, it’s important that you understand how it was done. Try to get some documentation about what the pre-processing was and how the sampling was done.\nIt is very important that anything you do to clean the data is recorded.\nOnce you have cleaned the data and you have gotten a basic look at it, it is important to determine if the data are good enough to solve your problems.\nIf you determine the data are not good enough for your question, then you’ve got to quit, try again, change the data, or try a different question. It is important to not simply push on with the data you have, just because that’s all that you’ve got, because that can lead to inappropriate inferences or conclusions.\n\n5. Exploratory data analysis:\n\nIt would be useful to look at what are the data, what did the data look like, what’s the distribution of the data, what are the relationships between the variables.\nYou want to look at basic summaries, one dimensional, two dimensional summaries of the data and we want to check for is there any missing data, why is there missing data, if there is, create some exploratory plots and do a little exploratory analyses. o Split the data set into Train and Test data sets:\nlibrary(kernlab) \ndata(spam)\nset.seed(3435) \ntrainIndicator = rbinom(4601, size = 1, prob = 0.5) table(trainIndicator)\ntrainSpam = spam[trainIndicator == 1, ] \ntestSpam = spam[trainIndicator == 0, ]\nWe can make some plots and we can compare, what are the frequencies of certain characteristics between the spam and the non spam emails:\nboxplot(capitalAve ~ type, data = trainSpam)\npairs(log10(trainSpam[, 1:4] + 1))   ## pairs plot of the first four variables\nYou can see that some of them are correlated, some of them are not particularly correlated, and that’s useful to know.\nExplore the predictors space a little bit more by doing a hierarchical cluster analysis, e.g. the Dendrogram just to see how what predictors or what words or characteristics tend to cluster together\nhCluster = hclust(dist(t(trainSpam[, 1:57]))) \nplot(hCluster)\n\n6. Statistical prediction/modeling:\n\nAny statistical modeling that you engage in should be informed by questions that you’re interested in, of course, and the results of any exploratory analysis. The exact methods that you employ will depend on the question of interest.\nwe’re just going to cycle through all the variables in this data set using this for-loop to build a logistic regression model, and then subsequently calculate the cross validated error rate of predicting spam emails from a single variable.\nOnce we’ve done this, we’re going to try to figure out which of the individual variables has the minimum cross validated error rate. It turns out that the predictor that has the minimum cross validated error rate is this variable called charDollar. This is an indicator of the number of dollar signs in the email.\nWe can actually make predictions now from the model on the test data (now we’re going to predict the outcome on the test data set to see how well we do).\nwe can take a look at the predicted values from our model, and then compare them with the actual values from the test data set, because we know which was spam, and which was not. Now we can just calculate the error rate.\n\n7. Interpretation of results:\n\nThink carefully about what kind of language you use to interpret your results. It’s also good to give an explanation for why certain models predict better than others, if possible.\nIf there are coefficients in the model that you need to interpret, you can do that here.\nAnd in particular it’s useful to bring in measures of uncertainty, to calibrate your interpretation of the final results.\n\n8. Challenging of results:\n\nIt’s good to challenge everything, the whole process by which you’ve gone through this problem. Is the question even a valid question to ask? Where did the data come from? How did you get the data? How did you process the data? How did you do the analysis and draw any conclusions?\nAnd if you built models, why is your model the best model? Why is it an appropriate model for this problem? How do you choose the things to include in your model?\nAll these things are questions that you should ask yourself and should have a reasonable answer to, so that when someone else asks you, you can respond in kind.\n\n9. Synthesis and write up:\n\nTypically in any data analysis, there are going to be many, many, many things that you did. And when you present them to another person or to a group you’re going to want to have winnowed it down to the most important aspects to tell a coherent story.\nTypically you want to lead with the question that you were trying to address.\nIt’s important that you don’t include every analysis that you ever did, but only if its needed for telling a coherent story. Talk about the analyses of your data set in the order that’s appropriate for the story you’re trying to tell.\nInclude very well done figures so that people can understand what you’re trying to say in one picture or two.\n\n10. Creating reproducible code:\n\nYou can use tools like RMarkdown and knitr and RStudio to document your analyses as you do them.\nYou can preserve the R code as well as any kind of a written summary of your analysis in a single document using knitr.\nIf someone cannot reproduce your data analysis then the conclusions that you draw will be not as worthy as an analysis where the results are reproducible."
  },
  {
    "objectID": "reproducible.html#r-markdown",
    "href": "reproducible.html#r-markdown",
    "title": "27  Reproducible Reporting",
    "section": "27.5 R Markdown",
    "text": "27.5 R Markdown\nlink to R Markdown guide\nThe benefit of Markdown for writers is that it allows one to focus on writing as opposed to formatting. It has simple and minimal yet intuitive formatting elements and can be easily converted to valid HTML (and other formats) using existing tools.\nR markdown is the integration of R code with Markdown. Documents written in R Markdown have R coded nested inside of them, which allows one to create documents containing “live” R code.\nR markdown can be converted to standard markdown using the knitr package in R. Markdown can subsequently be converted to HTML using the markdown package in R."
  },
  {
    "objectID": "reproducible.html#knitr",
    "href": "reproducible.html#knitr",
    "title": "27  Reproducible Reporting",
    "section": "27.6 Knitr",
    "text": "27.6 Knitr\nFor literate statistical programming, the idea is that a report is viewed as a stream of text and code.\nAnalysis code is divided into code chunks with text surrounding the code chunks explaining what is going on: - In general, literate programs are weaved to produce human-readable documents - and tangled to produce machine- readable documents\nThe requirements for writing literate programs are a documentation language (Markdown) and a programming language (R).\nMy First knitr Document:\n\nOpen an R Markdown document.\nRStudio will prompt you with a dialog box to set some of the metadata for the document.\nWhen you are ready to process and view your R Markdown document the easiest thing to do is click on the Knit HTML button that appears at the top of the editor window.\nNote here that the the code is echoed in the document in a grey background box and the output is shown just below it in a white background box. Notice also that the output is prepended with two pound symbols.\nCode chunks begin with {r} and end with just. Any R code that you include in a document must be contained within these delimiters, unless you have inline code.\nHiding code:\n{r pressure, echo=FALSE}\nHiding results:\n{r pressure, echo=FALSE, results = “hide”}\nRather than try to copy and paste the result into the paragraph, it’s better to just do the computation right there in the text:\nMy favourite random number is r rnorm(1)\nTables can be made in R Markdown documents with the help of the xtable package.\nThe opts_chunk variable sets an option that applies to all chunks in your document. For example, if we wanted the default to be that all chunks do NOT echo their code and always hide their results, we could set:\nknitr::opts_chunk$set(echo = FALSE, results = “hide”)\nGlobal options can always be overridden by any specific options that are set in at the chunk level:\n{r pressure, echo=FALSE, results = “asis”}\nChunk caching. If you have a long document or one involving lengthy computations, then every time you want to view your document in the pretty formatted version, you need to re-compile the document, meaning you need to re- run all the computations. Chunk caching is one way to avoid these lengthy computations.\ncache = TRUE\nIncluding a call to sessionInfo() at the end of each report written in R (perhaps with markdown or knitr) can be useful for communicating to the reader what type of environment is needed to reproduce the contents of the report."
  },
  {
    "objectID": "ggplot.html#themes",
    "href": "ggplot.html#themes",
    "title": "23  Creating a ggplot",
    "section": "23.11 Themes",
    "text": "23.11 Themes\ngplot2 includes the eight themes, with theme_gray() as the default. Many more are included in add-on packages like ggthemes (https://jrnold.github.io/ggthemes). You can also create your own themes, if you are trying to match a particular corporate or journal style.\nExample:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(se = FALSE) +\n  theme_bw()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nIt’s also possible to control individual components of each theme, like the size and color of the font used for the y axis.\nExample:\n  ...\n  theme(\n    legend.position = c(0.6, 0.7),\n    legend.direction = \"horizontal\",\n    legend.box.background = element_rect(color = \"black\"),\n    plot.title = element_text(face = \"bold\"),\n    plot.title.position = \"plot\",\n    plot.caption.position = \"plot\",\n    plot.caption = element_text(hjust = 0)\n  )\nFor an overview of all theme() components, see help with ?theme."
  },
  {
    "objectID": "ggplot.html#layout",
    "href": "ggplot.html#layout",
    "title": "23  Creating a ggplot",
    "section": "23.12 Layout",
    "text": "23.12 Layout\nWhat if you have multiple plots you want to lay out in a certain way? The patchwork package allows you to combine separate plots into the same graphic.\nTo place two plots next to each other, you can simply add them to each other. Note that you first need to create the plots and save them as objects (in the following example they’re called p1 and p2). Then, you place them next to each other with +.\n\nlibrary(patchwork)\np1 &lt;- ggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  labs(title = \"Plot 1\")\np2 &lt;- ggplot(mpg, aes(x = drv, y = hwy)) + \n  geom_boxplot() + \n  labs(title = \"Plot 2\")\np1 + p2\n\n\n\n\nYou can also create complex plot layouts with patchwork. In the following, | places the p1 and p3 next to each other and / moves p2 to the next line.\n\np3 &lt;- ggplot(mpg, aes(x = cty, y = hwy)) + \n  geom_point() + \n  labs(title = \"Plot 3\")\n(p1 | p3) / p2"
  },
  {
    "objectID": "reading.html#excel-spreadsheets",
    "href": "reading.html#excel-spreadsheets",
    "title": "5  Reading Data",
    "section": "5.4 Excel Spreadsheets",
    "text": "5.4 Excel Spreadsheets\nIf the dataset is stored in the .xls or .xlsx format you can use the readxl package. This package is non-core tidyverse, so you need to load it explicitly, but it is installed automatically when you install the tidyverse package.\nMost of readxl’s functions allow you to load Excel spreadsheets into R:\n\nread_xls() reads Excel files with xls format.\nread_xlsx() read Excel files with xlsx format.\nread_excel() can read files with both xls and xlsx format. It guesses the file type based on the input.\n\nExample:\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(writexl)\n(students &lt;- read_excel(\"data/students.xlsx\"))\n\n# A tibble: 6 × 5\n  `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4.0  \n2            2 Barclay Lynn     French fries       Lunch only          5.0  \n3            3 Jayendra Lyne    N/A                Breakfast and lunch 7.0  \n4            4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6            6 Güvenç Attila    Ice cream          Lunch only          6.0  \n\n\nAn important feature that distinguishes spreadsheets from flat files is the notion of multiple sheets, called worksheets. You can read a single worksheet from a spreadsheet with the sheet argument in read_excel(). The default, which we’ve been relying on up until now, is the first sheet.\nread_excel(\"data/penguins.xlsx\", sheet = \"Torgersen Island\")\nAlternatively, you can use excel_sheets() to get information on all worksheets in an Excel spreadsheet, and then read the one(s) you’re interested in.\nexcel_sheets(\"data/penguins.xlsx\")\n\n# Read a worksheet individually\npenguins_biscoe &lt;- read_excel(\"data/penguins.xlsx\", sheet = \"Biscoe Island\", na = \"NA\") \nTo know the number of columns and rows of each worksheet you can use the function dim().\ndim(penguins_torgersen)\nWhen working with spreadsheet data, it’s important to keep in mind that the underlying data can be very different than what you see in the cell. For example, Excel has no notion of an integer. All numbers are stored as floating points, but you can choose to display the data with a customizable number of decimal points. Similarly, dates are actually stored as numbers, specifically the number of seconds since January 1, 1970. You can customize how you display the date by applying formatting in Excel. Confusingly, it’s also possible to have something that looks like a number but is actually a string (e.g., type ’10 into a cell in Excel).\n\n5.4.1 Reading part of a sheet\nIt’s quite common to find cell entries in a spreadsheet that are not part of the data you want to read into R. You can use the readxl_example() function to locate the spreadsheet on your system in the directory where the package is installed. This function returns the path to the spreadsheet, which you can use in read_excel()as usual.\nstudents_path &lt;- readxl_example(\"students.xlsx\")\nread_excel(students_path, range = \"A1:C4\")"
  },
  {
    "objectID": "writing.html#writing-to-excel",
    "href": "writing.html#writing-to-excel",
    "title": "6  Writing Data",
    "section": "6.1 Writing to Excel",
    "text": "6.1 Writing to Excel\nYou can write data back to disk as an Excel file using the write_xlsx() from the writexl package:\nwrite_xlsx(bake_sale, path = \"data/bake-sale.xlsx\") \nIf you’re interested in additional features like writing to sheets within a spreadsheet and styling, you will want to use the openxlsx package."
  },
  {
    "objectID": "reading.html#reading-excel-spreadsheets",
    "href": "reading.html#reading-excel-spreadsheets",
    "title": "5  Reading Data",
    "section": "5.4 Reading Excel Spreadsheets",
    "text": "5.4 Reading Excel Spreadsheets\nIf the dataset is stored in the .xls or .xlsx format you can use the readxl package. This package is non-core tidyverse, so you need to load it explicitly, but it is installed automatically when you install the tidyverse package.\nMost of readxl’s functions allow you to load Excel spreadsheets into R:\n\nread_xls() reads Excel files with xls format.\nread_xlsx() read Excel files with xlsx format.\nread_excel() can read files with both xls and xlsx format. It guesses the file type based on the input.\n\nExample:\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(writexl)\n(students &lt;- read_excel(\"data/students.xlsx\"))\n\n# A tibble: 6 × 5\n  `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4.0  \n2            2 Barclay Lynn     French fries       Lunch only          5.0  \n3            3 Jayendra Lyne    N/A                Breakfast and lunch 7.0  \n4            4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6            6 Güvenç Attila    Ice cream          Lunch only          6.0  \n\n\nAn important feature that distinguishes spreadsheets from flat files is the notion of multiple sheets, called worksheets. You can read a single worksheet from a spreadsheet with the sheet argument in read_excel(). The default, which we’ve been relying on up until now, is the first sheet.\nread_excel(\"data/penguins.xlsx\", sheet = \"Torgersen Island\")\nAlternatively, you can use excel_sheets() to get information on all worksheets in an Excel spreadsheet, and then read the one(s) you’re interested in.\nexcel_sheets(\"data/penguins.xlsx\")\n\n# Read a worksheet individually\npenguins_biscoe &lt;- read_excel(\"data/penguins.xlsx\", sheet = \"Biscoe Island\", na = \"NA\") \nTo know the number of columns and rows of each worksheet you can use the function dim().\ndim(penguins_torgersen)\nWhen working with spreadsheet data, it’s important to keep in mind that the underlying data can be very different than what you see in the cell. For example, Excel has no notion of an integer. All numbers are stored as floating points, but you can choose to display the data with a customizable number of decimal points. Similarly, dates are actually stored as numbers, specifically the number of seconds since January 1, 1970. You can customize how you display the date by applying formatting in Excel. Confusingly, it’s also possible to have something that looks like a number but is actually a string (e.g., type ’10 into a cell in Excel).\n\n5.4.1 Reading part of a sheet\nIt’s quite common to find cell entries in a spreadsheet that are not part of the data you want to read into R. You can use the readxl_example() function to locate the spreadsheet on your system in the directory where the package is installed. This function returns the path to the spreadsheet, which you can use in read_excel()as usual.\nstudents_path &lt;- readxl_example(\"students.xlsx\")\nread_excel(students_path, range = \"A1:C4\")"
  },
  {
    "objectID": "reading.html#reading-google-sheets",
    "href": "reading.html#reading-google-sheets",
    "title": "5  Reading Data",
    "section": "5.5 Reading Google Sheets",
    "text": "5.5 Reading Google Sheets\nFor loading data from a Google Sheet you will be using the googlesheets4 package. This package is non-core tidyverse, so you need to load it explicitly.\n\nlibrary(googlesheets4)\nlibrary(tidyverse)\n\nThe main function of the googlesheets4 package is read_sheet(), which reads a Google Sheet from a URL or a file id. This function also goes by the name range_read(). You can also create a brand new sheet with gs4_create() or write to an existing sheet with sheet_write() and friends.\nThe first argument to read_sheet()is the URL of the file to read, and it returns a tibble:\nExample:\n\ngs4_deauth()\nstudents &lt;- read_sheet('https://docs.google.com/spreadsheets/d/1V1nPp1tzOuutXFLb3G9Eyxi3qxeEhnOXUzL5_BcCQ0w')\n\n✔ Reading from \"students\".\n\n\n✔ Range 'Sheet1'.\n\nstudents\n\n# A tibble: 6 × 5\n  `Student ID` `Full Name`      favourite.food     mealPlan            AGE      \n         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;list&gt;   \n1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          &lt;dbl [1]&gt;\n2            2 Barclay Lynn     French fries       Lunch only          &lt;dbl [1]&gt;\n3            3 Jayendra Lyne    N/A                Breakfast and lunch &lt;dbl [1]&gt;\n4            4 Leon Rossini     Anchovies          Lunch only          &lt;NULL&gt;   \n5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch &lt;chr [1]&gt;\n6            6 Güvenç Attila    Ice cream          Lunch only          &lt;dbl [1]&gt;\n\n\nIt’s also possible to read individual sheets from Google Sheets as well. Let’s read the “Torgersen Island” sheet from the penguins Google Sheet:\n\npenguins_sheet_id &lt;- \"1aFu8lnD_g0yjF5O-K6SFgSEWiHPpgvFCF0NY9D6LXnY\"\nread_sheet(penguins_sheet_id, sheet = \"Torgersen Island\")\n\n✔ Reading from \"penguins\".\n\n\n✔ Range ''Torgersen Island''.\n\n\n# A tibble: 52 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;     &lt;list&gt;         &lt;list&gt;        &lt;list&gt;            &lt;list&gt;     \n 1 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n 2 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n 3 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n 4 Adelie  Torgersen &lt;chr [1]&gt;      &lt;chr [1]&gt;     &lt;chr [1]&gt;         &lt;chr [1]&gt;  \n 5 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n 6 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n 7 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n 8 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n 9 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n10 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n# ℹ 42 more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\nYou can obtain a list of all sheets within a Google Sheet with sheet_names():\n\nsheet_names(penguins_sheet_id)\n\n[1] \"Torgersen Island\" \"Biscoe Island\"    \"Dream Island\"    \n\n\nNote that we’re also using the gs4_example() function below to locate an example Google Sheet that comes with the googlesheets4 package.\n\ndeaths_url &lt;- gs4_example(\"deaths\")\ndeaths &lt;- read_sheet(deaths_url, range = \"A5:F15\")\n\n✔ Reading from \"deaths\".\n\n\n✔ Range 'A5:F15'.\n\ndeaths\n\n# A tibble: 10 × 6\n   Name      Profession   Age `Has kids` `Date of birth`     `Date of death`    \n   &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;lgl&gt;      &lt;dttm&gt;              &lt;dttm&gt;             \n 1 David Bo… musician      69 TRUE       1947-01-08 00:00:00 2016-01-10 00:00:00\n 2 Carrie F… actor         60 TRUE       1956-10-21 00:00:00 2016-12-27 00:00:00\n 3 Chuck Be… musician      90 TRUE       1926-10-18 00:00:00 2017-03-18 00:00:00\n 4 Bill Pax… actor         61 TRUE       1955-05-17 00:00:00 2017-02-25 00:00:00\n 5 Prince    musician      57 TRUE       1958-06-07 00:00:00 2016-04-21 00:00:00\n 6 Alan Ric… actor         69 FALSE      1946-02-21 00:00:00 2016-01-14 00:00:00\n 7 Florence… actor         82 TRUE       1934-02-14 00:00:00 2016-11-24 00:00:00\n 8 Harper L… author        89 FALSE      1926-04-28 00:00:00 2016-02-19 00:00:00\n 9 Zsa Zsa … actor         99 TRUE       1917-02-06 00:00:00 2016-12-18 00:00:00\n10 George M… musician      53 FALSE      1963-06-25 00:00:00 2016-12-25 00:00:00\n\n\nWhen you attempt to read in a sheet that requires authentication, googlesheets4 will direct you to a web browser with a prompt to sign in to your Google account and grant permission to operate on your behalf with Google Sheets. However, if you want to specify a specific Google account, authentication scope, etc. you can do so with gs4_auth(), e.g., gs4_auth(email = \"mine@example.com\"), which will force the use of a token associated with a specific email."
  },
  {
    "objectID": "writing.html#writing-to-google-sheets",
    "href": "writing.html#writing-to-google-sheets",
    "title": "6  Writing Data",
    "section": "6.2 Writing to Google Sheets",
    "text": "6.2 Writing to Google Sheets\nYou can write from R to Google Sheets with write_sheet(). The first argument is the dataframe to write, and the second argument is the name (or other identifier) of the Google Sheet to write to:\nwrite_sheet(bake_sale, ss = \"bake-sale\")\nIf you’d like to write your data to a specific (work)sheet inside a Google Sheet, you can specify that with the sheet argument as well.\nwrite_sheet(bake_sale, ss = \"bake-sale\", sheet = \"Sales\")"
  }
]