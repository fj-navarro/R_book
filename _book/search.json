[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for Research in Action",
    "section": "",
    "text": "Preface\nWelcome to R for Research in Action, a guide designed to empower learners at all levels in mastering the R programming language for their research endeavors. Whether you are a beginner just starting with R or an advanced user seeking to refine your skills, this book will serve as a comprehensive companion to help you elevate the quality of your research, making it more robust, in-depth, and reproducible.\nResearch today demands more than just data collection—it requires the ability to analyze, visualize, and interpret findings in ways that are reliable and transparent. R, with its powerful ecosystem of packages and its open-source nature, has become an essential tool across various fields for achieving these goals. From data wrangling to advanced statistical modeling, R offers unparalleled flexibility to researchers, allowing them to explore their data deeply and communicate results with precision.\nThis book covers a broad range of topics, including data manipulation, visualization, statistical analysis, programming, and reproducible research practices. We begin with the basics, progressively introducing more advanced topics to ensure that users at every skill level can benefit. Along the way, you will find practical examples, real-world applications, and step-by-step guidance aimed at bridging the gap between theory and actionable results.\nBy the end of this book, my hope is that you will not only be proficient in R but also more confident in using it to conduct meaningful, impactful research. May this journey enhance the quality of your work and inspire further exploration.\n\n\nLearning Resources\nThese are resources that provide a wide range of learning options, from interactive tutorials to advanced textbooks, making them valuable for R learners at any stage.\n\nRWeekly. A valuable resource for learning and mastering R programming with its curated, up-to-date content on R tutorials, new packages, and advanced topics every week. Its community-driven approach fosters engagement and offers real-world applications, making it easier for users to stay informed and actively participate in the broader R ecosystem. Website\nThe RStudio Education Resources. Offers tutorials, webinars, and cheat sheets, particularly focusing on using R with the RStudio IDE. It’s great for learners who prefer structured courses. Website\nSwirl. An interactive R learning platform that teaches R directly within RStudio. It offers hands-on experience with guided lessons on various R topics. Website\nTidyverse.org. This website focuses on learning and using the tidyverse, a collection of R packages designed for data science. It offers excellent tutorials and documentation. Website\nStack Overflow (R Tag). A large and active community where you can ask questions, browse solutions, and learn from real-world R programming issues faced by others. Website\nR-bloggers. A blog aggregator for R tutorials, tips, and news from various contributors. It covers a wide range of topics and helps you stay up-to-date with the latest trends. Website\nRDocumentation. A centralized source for searching through R packages and functions, providing detailed documentation and examples. Website\n\n\n\nOnline Books\n\nR for Data Science (R4DS) by Hadley Wickham. A highly recommended free book that covers the essentials of R, data manipulation, and visualization with the tidyverse packages. Website\nAdvanced R by Hadley Wickham. Great for those looking to deepen their understanding of R programming. Website\nHands on Programming with R by Garrett Grolemund. An introduction to R as a programming language and is a great place to start if R is your first programming language. Website\nTidy Modeling with R by Max Kuhn and Julia Silge. This book is a guide to using a collection of software in the R programming language for model building called tidymodels. Website\nR Packages. Essential for learning how to build your own R packages. Website"
  },
  {
    "objectID": "intro_objects.html#classes",
    "href": "intro_objects.html#classes",
    "title": "1  R Objects",
    "section": "1.1 Classes",
    "text": "1.1 Classes\nR has five basic or “atomic” classes of objects:\n\ncharacter\nnumeric (real numbers)\ninteger\ncomplex\nlogical (True/False)\n\nEntering 1 in R gives you a numeric object; entering 1L explicitly gives you an integer object."
  },
  {
    "objectID": "intro_objects.html#attributes",
    "href": "intro_objects.html#attributes",
    "title": "1  R Objects",
    "section": "1.2 Attributes",
    "text": "1.2 Attributes\nAttributes of an object (if any) can be accessed using the attributes() function:\n\nnames, dimnames\ndimensions (e.g. matrices, arrays)\nclass (e.g. integer, numeric)\nlength\n\nThe mode() of an object tells us how it’s stored. It could happen that two different objects are stored in the same mode with different classes.\n\nFor vectors the class and mode will always be numeric, logical, or character.\nFor matrices and arrays a class is always a matrix or array, but its mode can be numeric, character, or logical.\n\nThe primary purpose of the class() function is to know how different functions, including generic functions, work (e.g. print, or plot). There is a collection of R commands used to assess whether a particular object belongs to a certain class, these start with is.; for example, is.numeric(), is.logical(), is.character(), is.list(), is.factor(), and is.data.frame()"
  },
  {
    "objectID": "intro_objects.html#mixing-objects",
    "href": "intro_objects.html#mixing-objects",
    "title": "1  R Objects",
    "section": "1.3 Mixing Objects",
    "text": "1.3 Mixing Objects\nThis is not allowed! When different objects are mixed in a vector, coercion occurs so that every element in the vector is of the same class.\n\ny &lt;- c(1.7, \"a\")\nclass(y) \n\n[1] \"character\"\n\n\n\ny &lt;- c(TRUE, 2)\nclass(y) \n\n[1] \"numeric\"\n\n\n\ny &lt;- c(\"a\", TRUE)\nclass(y) \n\n[1] \"character\""
  },
  {
    "objectID": "intro_objects.html#explicit-coercion",
    "href": "intro_objects.html#explicit-coercion",
    "title": "1  R Objects",
    "section": "1.4 Explicit Coercion",
    "text": "1.4 Explicit Coercion\nObjects can be explicitly coerced from one class to another using the as. functions, if available.\n\nx &lt;- 0:6 \nclass(x) \n\n[1] \"integer\"\n\n\n\nas.numeric(x)\n\n[1] 0 1 2 3 4 5 6"
  },
  {
    "objectID": "intro_objects.html#names",
    "href": "intro_objects.html#names",
    "title": "1  R Objects",
    "section": "1.5 Names",
    "text": "1.5 Names\nObject names must start with a letter and can only contain letters, numbers, _, and ..\nYou want your object names to be descriptive, so you’ll need to adopt a convention for multiple words.\nExample:\ni_use_snake_case\notherPeopleUseCamelCase\nsome.people.use.periods"
  },
  {
    "objectID": "intro_objects.html#r-workspace",
    "href": "intro_objects.html#r-workspace",
    "title": "1  R Objects",
    "section": "1.6 R Workspace",
    "text": "1.6 R Workspace\nThe objects that you create using R remain in existence until you explicitly delete them or you conclude the session.\n\nTo list all currently defined objects, use ls() or objects().\nTo remove object x, use rm(x). To remove all currently defined objects, use rm(list = ls()).\nTo save all of your existing objects to a file called fname in the current working directory, use save.image(file = \"fname\").\nTo save specific objects (say x and y) use save(x, y, file = \"fname\").\nTo load a set of saved objects use load(file = \"fname\").\nTo save this history to the file fname use savehistory(file = \"fname\") and to load the history file fname use loadhistory(file = \"fname\")."
  },
  {
    "objectID": "intro_objects.html#expressions-and-assignments",
    "href": "intro_objects.html#expressions-and-assignments",
    "title": "1  R Objects",
    "section": "1.7 Expressions and Assignments",
    "text": "1.7 Expressions and Assignments\nIn R an expression is used to denote a phrase of code that can be executed.\nExample:\n## An expression\nseq(10, 20, 3)\nThe combination of expressions that are saved for evaluation is called an assignment:\n## An assignment\nobject_name &lt;- value\nWhen reading that code, say “object name gets value” in your head. You will make lots of assignments, and &lt;- is a pain to type.\nYou can save time with RStudio’s keyboard shortcut: Alt + - (the minus sign)."
  },
  {
    "objectID": "intro_types.html#vectors",
    "href": "intro_types.html#vectors",
    "title": "2  Data Types",
    "section": "2.1 Vectors",
    "text": "2.1 Vectors\nA vector is the most convenient way to store more than one data value.\nA vector is a contiguous cell that contains data, where each cell can be accessed by an index. In other words, a vector is an indexed set of objects.\nAll the elements of an atomic vector have to be of the same type —numeric, character, or logical— which is called the mode of the vector.\n\n2.1.1 How to create a vector?\nThere are many ways to create a vector, but these are four basic functions for constructing vectors:\n\nThe c() (combine) function can be used to create vectors of objects by concatenating things together.\nExamples:\n\nx &lt;- c(0.5, 0.6) \nclass(x) \n\n[1] \"numeric\"\n\n\n\nx &lt;- c(TRUE, FALSE) \nclass(x) \n\n[1] \"logical\"\n\n\n\nx &lt;- c(T, F)  \nclass(x) \n\n[1] \"logical\"\n\n\n\nx &lt;- c(\"a\", \"b\", \"c\")   \nclass(x) \n\n[1] \"character\"\n\n\n\nx &lt;- 9:29   \nclass(x) \n\n[1] \"integer\"\n\n\n\nx &lt;- c(1+0i, 2+4i)  \nclass(x) \n\n[1] \"complex\"\n\n\nseq(from, to, by):\n\n(x &lt;- seq(1, 20, 2))\n\n [1]  1  3  5  7  9 11 13 15 17 19\n\n\nrep(x, times):\n\n(y &lt;- rep(3, 4))\n\n[1] 3 3 3 3\n\n\nYou can also use the vector() function to initialize vectors:\nExample:\n\nx &lt;- vector(\"numeric\", length = 10) \nclass(x) \n\n[1] \"numeric\"\n\n\n\n\n\n2.1.2 Add a value to a variable\nExample:\nx[4] &lt;- 9\n\n\n2.1.3 Add names to a vector\n\n# Create a vector with the name of each element\nnamed.num.vec &lt;- c(x1=1, x2=3, x3=5) \nnamed.num.vec \n\nx1 x2 x3 \n 1  3  5 \n\n\nThis is another option to add names:\nnames(x) &lt;- c(\"a\", \"b\", \"c)\n\n\n2.1.4 length()\nThe function length(x) gives the number of elements of ‘x’.\n\nx &lt;- 100:100\nlength(x)\n\n[1] 1\n\n\nIt is possible to have a vector with no elements\n\nx &lt;- c()\nlength(x)\n\n[1] 0"
  },
  {
    "objectID": "intro_types.html#factors",
    "href": "intro_types.html#factors",
    "title": "2  Data Types",
    "section": "2.2 Factors",
    "text": "2.2 Factors\nStatisticians typically recognise three basic types of variable: numeric, ordinal, and categorical. In R the data type for ordinal and categorical vectors is factor. The possible values of a factor are referred to as its levels.\nIn practice, a factor is not much different from a character vector, except that the elements of a factor can take only a limited number of values (of which R keeps a record), and in statistical routines R is able to treat a factor differently than a character vector.\nTo create a factor we apply the function factor() to some vector x. By default the distinct values of x become the levels, or we can specify them using the optional levels argument.\nExample:\n\nx &lt;- factor(c(\"yes\", \"yes\", \"no\", \"yes\", \"no\")) \ntable(x)\n\nx\n no yes \n  2   3 \n\nlevels(x)\n\n[1] \"no\"  \"yes\"\n\n\nNote the use of the function table() to calculate the number of times each level of the factor appears. table() can be applied to other modes of vectors as well as factors. The output of the table() function is a one-dimensional array (as opposed to a vector). If more than one vector is passed to table(), then it produces a multidimensional array.\nThe order of the levels of a factor can be set using the levels argument to factor(). By default R arranges the levels of a factor alphabetically. If you specify the levels yourself, then R uses the ordering that you provide.\nExample:\n\nx &lt;- factor(c(\"yes\", \"yes\", \"no\", \"yes\", \"no\"), levels = c(\"yes\", \"no\"), ordered=TRUE) \nx\n\n[1] yes yes no  yes no \nLevels: yes &lt; no\n\n\nUsing factors with labels is better than using integers because factors are self-describing.\nExample:\nphys.act &lt;- factor(phys.act, levels = c(\"L\", \"M\", \"H\"),\n  labels = c(\"Low\", \"Medium\", \"High\"),\n  ordered = TRUE)\nWe check whether or not an object x is a factor using is.factor(x).\nExample:\n\nis.factor(x)\n\n[1] TRUE\n\n\nUsually it is convenient to transform a numeric variable into a data.frame:\nairquality &lt;- transform(airquality, Month = factor(Month))\ncut() is a generic command to create factor variables from numeric variables:\nExample:\n\nnumvar &lt;- rnorm(100) \nnum2factor &lt;- cut(numvar, breaks=5) ## the levels are produced using the actual range of values\nnum2factor\n\n  [1] (-0.365,0.718] (-0.365,0.718] (0.718,1.8]    (-1.45,-0.365] (-2.54,-1.45] \n  [6] (0.718,1.8]    (-1.45,-0.365] (-0.365,0.718] (-1.45,-0.365] (-1.45,-0.365]\n [11] (0.718,1.8]    (1.8,2.89]     (0.718,1.8]    (-2.54,-1.45]  (-0.365,0.718]\n [16] (-0.365,0.718] (-0.365,0.718] (-0.365,0.718] (-1.45,-0.365] (0.718,1.8]   \n [21] (-1.45,-0.365] (-1.45,-0.365] (1.8,2.89]     (-0.365,0.718] (-0.365,0.718]\n [26] (-0.365,0.718] (-0.365,0.718] (0.718,1.8]    (-0.365,0.718] (0.718,1.8]   \n [31] (-1.45,-0.365] (-1.45,-0.365] (-1.45,-0.365] (-0.365,0.718] (-2.54,-1.45] \n [36] (-0.365,0.718] (-1.45,-0.365] (-0.365,0.718] (0.718,1.8]    (-0.365,0.718]\n [41] (-2.54,-1.45]  (-0.365,0.718] (-0.365,0.718] (-1.45,-0.365] (0.718,1.8]   \n [46] (0.718,1.8]    (-0.365,0.718] (-1.45,-0.365] (-0.365,0.718] (0.718,1.8]   \n [51] (-0.365,0.718] (-0.365,0.718] (-1.45,-0.365] (0.718,1.8]    (-1.45,-0.365]\n [56] (-1.45,-0.365] (-0.365,0.718] (-1.45,-0.365] (-1.45,-0.365] (-0.365,0.718]\n [61] (1.8,2.89]     (-1.45,-0.365] (-1.45,-0.365] (0.718,1.8]    (-0.365,0.718]\n [66] (-1.45,-0.365] (1.8,2.89]     (0.718,1.8]    (-0.365,0.718] (0.718,1.8]   \n [71] (-2.54,-1.45]  (-0.365,0.718] (-0.365,0.718] (-1.45,-0.365] (0.718,1.8]   \n [76] (-1.45,-0.365] (0.718,1.8]    (1.8,2.89]     (-2.54,-1.45]  (-0.365,0.718]\n [81] (-0.365,0.718] (-1.45,-0.365] (-0.365,0.718] (1.8,2.89]     (0.718,1.8]   \n [86] (-0.365,0.718] (-1.45,-0.365] (-1.45,-0.365] (-0.365,0.718] (1.8,2.89]    \n [91] (-0.365,0.718] (0.718,1.8]    (-2.54,-1.45]  (-1.45,-0.365] (-0.365,0.718]\n [96] (-0.365,0.718] (-0.365,0.718] (-0.365,0.718] (0.718,1.8]    (-0.365,0.718]\n5 Levels: (-2.54,-1.45] (-1.45,-0.365] (-0.365,0.718] ... (1.8,2.89]\n\n\n\nnum2factor &lt;- cut(numvar, breaks=5, labels= c(\"lowest group\", \"lower middle group\", \"middle group\", \"upper middle\", \"highest group\"))\ndata.frame(table(num2factor)) ## displaying the data in tabular form\n\n          num2factor Freq\n1       lowest group    7\n2 lower middle group   27\n3       middle group   39\n4       upper middle   20\n5      highest group    7"
  },
  {
    "objectID": "intro_types.html#matrices",
    "href": "intro_types.html#matrices",
    "title": "2  Data Types",
    "section": "2.3 Matrices",
    "text": "2.3 Matrices\nMatrices are stored as vectors with an added dimension attribute. The dimension attribute is itself an integer vector of length 2, which gives the number of rows and columns.\nThe matrix elements are stored column-wise in the vector. This means that it is possible to access the matrix elements using a single index:\n\n(A &lt;- matrix(c(3,5,2,3), 2, 2))\n\n     [,1] [,2]\n[1,]    3    2\n[2,]    5    3\n\nA[2]\n\n[1] 5\n\nA[,2]\n\n[1] 2 3\n\n\n\n2.3.1 How to create a matrix?\nMatrices are constructed column-wise, so entries can be thought of starting in the “upper left” corner and running down the columns.\nExample:\n\n(m &lt;- matrix(1:6, nrow = 2, ncol = 3, byrow = FALSE))\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\ndim(m) \n\n[1] 2 3\n\n\nMatrices can be created by column-binding or row-binding with the cbind() and rbind() functions:\nExamples:\n\nx &lt;- 1:3\ny &lt;- 10:12\ncbind(x, y)\n\n     x  y\n[1,] 1 10\n[2,] 2 11\n[3,] 3 12\n\nrbind(x, y)\n\n  [,1] [,2] [,3]\nx    1    2    3\ny   10   11   12\n\n\nMatrices can also be created directly from vectors by adding a dimension attribute:\nExample:\n\nm &lt;- 1:10\ndim(m) &lt;- c(2, 5)\nm\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    5    7    9\n[2,]    2    4    6    8   10\n\n\nTo create a matrixA with one column from a vector x, we use:\nA &lt;- as.matrix(x)\nTo create a vector from a matrix A, we use:\nx &lt;- as.vector(A)\n\n\n2.3.2 Create a diagonal matrix\nTo create a diagonal matrix use diag(x):\n\nB &lt;- diag(c(1,2,3))\n\n\n\n2.3.3 Add names to matrices\nMatrices can have names:\nExample:\ndimnames(m) &lt;- list(c(\"a\", \"b\"), c(\"c\", \"d\", \"e\")) \nm\nThese are other options for column and row names:\ncolnames(m) &lt;- c(\"c\", \"d\", \"e\")\nrownames(m) &lt;- c(\"a\", \"b\")\n\n\n2.3.4 Operations with matrices\nTo perform matrix multiplication we use the operator %*%. Remember that * acts element wise on matrices.\nOther functions for using with matrices are:\n\nnrow(x)\nncol(x)\ndet(x) (the determinant)\nt(x) (the transpose)\nsolve(A, B) (returns x such that A %*% x == B).\nIf A is invertible then solve(A) returns the matrix inverse of A."
  },
  {
    "objectID": "intro_types.html#data-frames",
    "href": "intro_types.html#data-frames",
    "title": "2  Data Types",
    "section": "2.4 Data Frames",
    "text": "2.4 Data Frames\nIt is a list of vectors restricted to be of equal length. Each vector —or column— corresponds to a variable in an experiment, and each row corresponds to a single observation or experimental unit. Each vector can be of any of the basic modes of object.\nThe dataframe is like a matrix but extended to allow for different object modes in different columns. Unlike matrices, data frames can store different classes of objects in each column (matrices must have every element be the same class, e.g. all integers or all numeric). Obviously to work with datasets from real experiments we need a way to group data of differing modes.\nData frames are used to store tabular data in R. Data frames are represented as a special type of list where every element of the list has to have the same length. Each element of the list can be thought of as a column and the length of each element of the list is the number of rows.\nData frames are usually created by:\n\nreading in a dataset using the read.table() or read.csv()\ncreating a dataframe with data.frame():\nExample:\n\n(x &lt;- data.frame(foo = 1:4, bar = c(T, T, F, F)))\n\n  foo   bar\n1   1  TRUE\n2   2  TRUE\n3   3 FALSE\n4   4 FALSE\n\n# To summarise the structure of a list (or dataframe), use str()\nstr(x)\n\n'data.frame':   4 obs. of  2 variables:\n $ foo: int  1 2 3 4\n $ bar: logi  TRUE TRUE FALSE FALSE\n\n\ncoerced from other types of objects like lists:\nx &lt;- as.data.frame(x)\n\nDataframes can be converted to a matrix by calling data.matrix().\nThe dplyr package has an optimized set of functions designed to work efficiently with dataframes.\n\n2.4.1 Columns and Rows\nYou can construct a dataframe from a collection of vectors and/or existing dataframes using the function data.frame, which has the form: data.frame(col1 = x1, col2 = x2, ..., df1, df2, ...). Here col1, col2, etc., are the column names (given as character strings without quotes) and x1, x2, etc., are vectors of equal length. df1, df2, etc., are dataframes, whose columns must be the same length as the vectors x1, x2, etc. Column names may be omitted, in which case R will choose a name for you.\nColumn names indicate the names of the variables or predictors names(). We can also create a new variable within a dataframe, by naming it and assigning it a value:\nExample:\nufc$volume.m3 &lt;- pi * (ufc$dbh.cm / 200)^2 * ufc$height.m / 2\nEquivalently one could assign to ufc[6] or ufc[\"volume.m3\"] or ufc[[6]] or ufc[[\"volume.m3\"]].\nThe command names(df) will return the names of the dataframe df as a vector of character strings.\nExample:\nufc.names &lt;- names(ufc)\n# To change the names of df you pass a vector of character strings to `names(df)`\nnames(ufc) &lt;- c(\"P\", \"T\", \"S\", \"D\", \"H\", \"V\")\nWhen you create dataframes and any one of the column’s classes is a character, it automatically gets converted to factor, which is a default R operation. However, there is one argument, stringsAsFactors=FALSE, that allows us to prevent the automatic conversion of character to factor during data frame creation.\nDataframes have a special attribute called row.names() which indicate information about each row of the data frame. You can change the row names of df by making an assignment to row.names(df).\n\n\n2.4.2 subset()\nThe function subset() is a convenient tool for selecting the rows of a dataframe, especially when combined with the operator %in%.\nExample:\n# Suppose you are only interested in the height of trees of species DF (Douglas Fir) or GF (Grand Fir)\nfir.height &lt;- subset(ufc, subset = species %in% c(\"DF\", \"GF\"),\n                    select = c(plot, tree, height.m))\nhead(fir.height)\n\n\n2.4.3 attach()\nR allows you to attach a dataframe to the workspace. When attached, the variables in the dataframe can be referred to without being prefixed by the name of the dataframe.\nExample:\nattach(ufc)\nmax(height.m[species == \"GF\"])\nWhen you attach a dataframe R actually makes a copy of each variable, which is deleted when the dataframe is detached. Thus, if you change an attached variable you do not change the dataframe. After we use the attach() command, we need to use detach() to remove individual variables from the working environment.\nNonetheless, note that the with() and transform()functions provide a safer alternative."
  },
  {
    "objectID": "intro_types.html#lists",
    "href": "intro_types.html#lists",
    "title": "2  Data Types",
    "section": "2.5 Lists",
    "text": "2.5 Lists\nLists are a special type of vector that can contain elements of different type (we can store single constants, vectors of numeric values, factors, data frames, matrices, and even arrays), namely, a list is a general data storage object that can house pretty much any other kind of R object.\nLike a vector, a list is an indexed set of objects (and so has a length), but unlike a vector the elements of a list can be of different types, including other lists! The mode of a list is list.\nThe power and utility of lists comes from this generality. A list might contain an individual measurement, a vector of observations on a single response variable, a dataframe, or even a list of dataframes containing the results of several experiments.\nIn R lists are often used for collecting and storing complicated function output. Dataframes are special kinds of lists.\n\n2.5.1 How to Create a List?\nLists can be explicitly created using the list() function, which takes an arbitrary number of arguments:\nExample 1:\n\n(x &lt;- list(1, \"a\", TRUE, 1 + 4i))\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] \"a\"\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] 1+4i\n\n\nExample 2:\n\n(my.list &lt;- list(\"one\", TRUE, 3, c(\"f\",\"o\",\"u\",\"r\")))\n\n[[1]]\n[1] \"one\"\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] 3\n\n[[4]]\n[1] \"f\" \"o\" \"u\" \"r\"\n\nmy.list[[2]]\n\n[1] TRUE\n\nmode(my.list[[2]])\n\n[1] \"logical\"\n\nmy.list[[4]][1]\n\n[1] \"f\"\n\nmy.list[4][1]\n\n[[1]]\n[1] \"f\" \"o\" \"u\" \"r\"\n\n\nWe can also create an empty list of a pre-specified length with the vector() function:\nExample:\n\n(x &lt;- vector(mode = \"list\", length = 5)) # the elements are NULL\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n[[5]]\nNULL\n\n\nTo flatten a list x, that is convert it to a vector, we use unlist(x).\nExample:\n\nx &lt;- list(1, c(2, 3), c(4, 5, 6))\nunlist(x)\n\n[1] 1 2 3 4 5 6\n\n\nMany functions produce list objects as their output. For example, when we fit a least squares regression, the regression object itself is a list, and can be manipulated using list operations.\nExample:\n\nlm.xy &lt;- lm(y ~ x, data = data.frame(x = 1:5, y = 1:5))\nmode(lm.xy)\n\n[1] \"list\"\n\nnames(lm.xy)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n\n\n\n2.5.2 Names\nThe elements of a list can be named when the list is created, using arguments of the form name1 = x1, name2 = x2, etc., or they can be named later by assigning a value to the names attribute.\nExample:\n\nmy.list &lt;- list(first = \"one\", second = TRUE, third = 3,\n                fourth = c(\"f\",\"o\",\"u\",\"r\"))\nnames(my.list)\n\n[1] \"first\"  \"second\" \"third\"  \"fourth\"\n\n\nUnlike a dataframe, the elements of a list do not have to be named. Names can be used (within quotes) when indexing with single or double square brackets, or they can be used (with or without quotes) after a dollar sign to extract a list element."
  },
  {
    "objectID": "intro_types.html#arrays",
    "href": "intro_types.html#arrays",
    "title": "2  Data Types",
    "section": "2.6 Arrays",
    "text": "2.6 Arrays\nSometimes, you need to store multiple matrices or data frames into a single object; in this case, we can use arrays to store this data.\nData frames and matrices are of two dimensions only, but an array can be of any number of dimensions.\nHere is a simple example to store three matrices of order 2 x 2 in a single array object:\nExamples:\n\n(mat.array &lt;- array(dim=c(2,2,3)))\n\n, , 1\n\n     [,1] [,2]\n[1,]   NA   NA\n[2,]   NA   NA\n\n, , 2\n\n     [,1] [,2]\n[1,]   NA   NA\n[2,]   NA   NA\n\n, , 3\n\n     [,1] [,2]\n[1,]   NA   NA\n[2,]   NA   NA\n\n(mat.array[,,1] &lt;- rnorm(4))\n\n[1]  1.00376090 -0.23405502 -1.07585795  0.02910741\n\n(mat.array[,,1] &lt;- rnorm(4))\n\n[1] -0.60771815  0.55092259  0.02501520 -0.01856388\n\n(mat.array[,,2] &lt;- rnorm(4))\n\n[1]  1.0958681  2.5708866 -0.3630206  1.8325596\n\n(mat.array[,,3] &lt;- rnorm(4))\n\n[1] -0.09184286  0.33612445  0.63677317  1.08605567"
  },
  {
    "objectID": "intro_packages.html",
    "href": "intro_packages.html",
    "title": "3  Packages",
    "section": "",
    "text": "A package is an archive of files that conforms to a certain format and structure and that provides extra functionality, usually extending R in a particular direction. The R community has produced many high-quality R packages for performing specific tasks,\nAny package is in one of three states:\n\nInstalled and loaded. A package that is loaded is directly available to your R session. Find out which packages are loaded using sessionInfo().\nInstalled but not loaded. A package that is installed is available for loading but its contents are not available until it is loaded. The function help.start() gives details of the packages that are installed on your computer.\nNot installed. These packages cannot be loaded. If a package is not installed then the library() function produces an error. If the install status is uncertain at the time of calling library (for example if you are writing a function that requires the package), then use the require() function, which returns FALSE if the package is not installed, rather than an error.\n\nInstalling all available packages would be a waste of space and time, as you would never use most of them. Similarly, loading all installed packages every time you start R would take some time, so by default R only loads the base packages when it starts and requires the user to load any others as and when they are needed.\nPackages are divided into three groups:\n\nBase. Base packages are installed along with R, and their objects are always available.\nRecommended. Recommended packages are installed along with R but must be loaded before they can be used.\nOther. Other packages are not installed by default, and must be installed separately.\n\nThe command to find out what packages are available for loading is installed.packages. The output of the function is quite verbose, but we only need the first column:\nExample:\n\ninstalled.packages()[1:5, 1] # Returns only the first five packages\n\n    anytime     askpass   backports   base64enc          BH \n  \"anytime\"   \"askpass\" \"backports\" \"base64enc\"        \"BH\" \n\n\nAll the packages that are available at a repository, and whose requirements are matched by the currently running version of R, can be listed using the command available.packages(). A package that is available in the repository but has not yet been installed may be installed using the install.packages() function. If we include the argument dependencies = TRUE, then the function will also install packages that are necessary to run the package or packages of interest; such packages are called dependencies.\nMost of the packages frequently get updates. This includes new features, improvements, and error fixing. The packageVersion function returns the version details of the input package:\nExample:\n\npackageVersion(\"tidyverse\")\n\n[1] '2.0.0'\n\n\nThe status of the packages that are installed can be compared with the repository using the old.packages() function, and easily updated using the update.packages() function.\nR can be easily updated with installr package. To update R on MacOS, you need to use updateR” package instead.\nExample:\ninstall.packages(\"installr\")\nlibrary(installr)\nupdateR()"
  },
  {
    "objectID": "intro_style.html#names",
    "href": "intro_style.html#names",
    "title": "4  Code Style",
    "section": "4.1 Names",
    "text": "4.1 Names\nRemember that variable names should use only lowercase letters, numbers, and _. Use _to separate words within a name.\nAs a general rule of thumb, it’s better to prefer long, descriptive names that are easy to understand rather than concise names that are fast to type.\nIn general, if you have a bunch of variables that are a variation on a theme, you’re better off giving them a common prefix rather than a common suffix because autocomplete works best on the start of a variable."
  },
  {
    "objectID": "intro_style.html#spaces",
    "href": "intro_style.html#spaces",
    "title": "4  Code Style",
    "section": "4.2 Spaces",
    "text": "4.2 Spaces\nPut spaces on either side of mathematical operators apart from ^ (i.e. +, -, ==, &lt;, …), and around the assignment operator (&lt;-).\nExample:\n# Strive for\nz &lt;- (a + b)^2 / d\n\n# Avoid\nz&lt;-( a + b ) ^ 2/d\nDon’t put spaces inside or outside parentheses for regular function calls. Always put a space after a comma, just like in standard English.\n# Strive for\nmean(x, na.rm = TRUE)\n\n# Avoid\nmean (x ,na.rm=TRUE)"
  },
  {
    "objectID": "intro_style.html#pipes",
    "href": "intro_style.html#pipes",
    "title": "4  Code Style",
    "section": "4.3 Pipes",
    "text": "4.3 Pipes\n|&gt; should always have a space before it and should typically be the last thing on a line. This makes it easier to add new steps, rearrange existing steps, modify elements within a step, and get a sky view by skimming the verbs on the left-hand side.\n# Strive for \nflights |&gt;  \n  filter(!is.na(arr_delay), !is.na(tailnum)) |&gt; \n  count(dest)\n\n# Avoid\nflights|&gt;filter(!is.na(arr_delay), !is.na(tailnum))|&gt;count(dest)\nIf the function you’re piping into has named arguments (like mutate() or summarize()), put each argument on a new line. If the function doesn’t have named arguments (like select() or filter()), keep everything on one line unless it doesn’t fit, in which case you should put each argument on its own line.\n# Strive for\nflights |&gt;  \n  group_by(tailnum) |&gt; \n  summarize(\n    delay = mean(arr_delay, na.rm = TRUE),\n    n = n()\n  )\n\n# Avoid\nflights |&gt;\n  group_by(\n    tailnum\n  ) |&gt; \n  summarize(delay = mean(arr_delay, na.rm = TRUE), n = n())\nAfter the first step of the pipeline, indent each line by two spaces. RStudio will automatically put the spaces in for you after a line break following a |&gt; . If you’re putting each argument on its own line, indent by an extra two spaces. Make sure )is on its own line, and un-indented to match the horizontal position of the function name.\n# Strive for \nflights |&gt;  \n  group_by(tailnum) |&gt; \n  summarize(\n    delay = mean(arr_delay, na.rm = TRUE),\n    n = n()\n  )\nBe wary of writing very long pipes, say longer than 10-15 lines. Try to break them up into smaller sub-tasks, giving each task an informative name.\nThe same basic rules that apply to the pipe also apply to ggplot2; just treat + the same way as |&gt;."
  },
  {
    "objectID": "manipu_reading.html#tabular-data",
    "href": "manipu_reading.html#tabular-data",
    "title": "5  Reading Data",
    "section": "5.1 Tabular Data",
    "text": "5.1 Tabular Data\nIt is common for data to be arranged in tables, with columns corresponding to variables and rows corresponding to separate observations. These dataframes are usually read into R using the function read.table(), which has the form:\nread.table(file, header = FALSE, sep = \"\") \n# read.table() returns a dataframe\nread_table() reads a common variation of fixed-width files where columns are separated by white space.\nThere are two commonly used variants of read.table():\n\n5.1.1 read.csv()\nread.csv() is for comma-separated data and is equivalent to read.table(file, header = TRUE, sep = \",\").\nSometimes, it could happen that the file extension is .csv, but the data is not comma separated. In that case, we can still use the read.csv() function, but in this case we have to specify the separator:\nExample:\nread.csv(\"iris_semicolon.csv\", stringsAsFactors = FALSE, sep=\";\")\nanscombe_tab_2 &lt;- read.table(\"anscombe.txt\", header=TRUE)\nUsually, read_csv() uses the first line of the data for the column names, which is a very common convention. But it’s not uncommon for a few lines of metadata to be included at the top of the file. You can use skip = n to skip the first n lines or use comment = \"#\" to drop all lines that start with (e.g.) #.\nExample:\nread_csv(\"data/students.csv,\n  skip = 2,\n  comment = \"#\"\n)\nIn other cases, the data might not have column names. You can use col_names = FALSE to tell read_csv() not to treat the first row as headings and instead label them sequentially from \\(X_1\\) to \\(X_n\\). Alternatively, you can pass col_names a character vector which will be used as the column names (e.g. col_names = c(\"x\", \"y\", \"z\")):\nNote that by default read_csv() only recognizes empty strings (““) in as NAs, however you surely want it to also recognize the character string “N/A”.\nExample:\nstudents &lt;- read_csv(\"data/students.csv\", na = c(\"N/A\", \"\"))\nIf a .csv file contains both numeric and character variables, and we use read.csv(), the character variables get automatically converted to the factor type. We can prevent character variables from this automatic conversion to factor, by specifying stringsAsFactors=FALSE within the read.csv() function.\nExample:\nread.csv(\"iris.csv\", stringsAsFactors=F)\nreadr provides a total of nine column types for you to use: col_logical(), col_logical(), col_factor()… Find more information here.\n\n\n5.1.2 read.delim()\nread_delim() reads in files with any delimiter, attempting to automatically guess the delimiter if you don’t specify it. For example, read.delim()is for tab-delimited data and is equivalent to read.table(file, header = TRUE, sep = \"\\t\").\nExample:\n## skips the first 2 lines\nanscombe &lt;- read.csv(\"CSVanscombe.csv\", skip=2) \n\n\n5.1.3 Other file types\nOnce you’ve mastered read_csv(), using readr’s other functions is straightforward; it’s just a matter of knowing which function to reach for:\n\nread_csv2(): reads semicolon-separated files. These use ; instead of , to separate fields and are common in countries that use , as the decimal marker.\nread_tsv(): reads tab-delimited files.\nread_fwf(): reads fixed-width files. You can specify fields by their widths with fwf_widths() or by their positions with fwf_positions().\nread_log(): reads Apache-style log files."
  },
  {
    "objectID": "manipu_reading.html#reading-multiple-files",
    "href": "manipu_reading.html#reading-multiple-files",
    "title": "5  Reading Data",
    "section": "5.2 Reading Multiple Files",
    "text": "5.2 Reading Multiple Files\nSometimes your data is split across multiple files instead of being contained in a single file. For example, you might have sales data for multiple months, with each month’s data in a separate file: 01-sales.csv for January, 02-sales.csv for February, and 03-sales.csv for March.\nWith read_csv()you can read these data in at once and stack them on top of each other in a single data frame.\nExample:\nsales_files &lt;- c(\"data/01-sales.csv\", \"data/02-sales.csv\", \"data/03-sales.csv\")\nread_csv(sales_files, id = \"file\")\nThe id argument adds a new column called file to the resulting data frame that identifies the file the data come from. This is especially helpful in circumstances where the files you’re reading in do not have an identifying column that can help you trace the observations back to their original sources.\nAn alternative to read multiple data made available online is this:\nExample:\nsales_files &lt;- c(\n  \"https://pos.it/r4ds-01-sales\",\n  \"https://pos.it/r4ds-02-sales\",\n  \"https://pos.it/r4ds-03-sales\"\n)\nread_csv(sales_files, id = \"file\")"
  },
  {
    "objectID": "manipu_reading.html#reading-line-by-line",
    "href": "manipu_reading.html#reading-line-by-line",
    "title": "5  Reading Data",
    "section": "5.3 Reading Line by Line",
    "text": "5.3 Reading Line by Line\nText files can be read line by line using the readLines() function:\nExample:\ncon &lt;- gzfile(\"words.gz\") \nx &lt;- readLines(con, 10)\nThis approach is useful because it allows you to read from a file without having to uncompress the file first, which would be a waste of space and time.\nTo read in lines of webpages it can be useful the readLines() function:\nExample:\ncon &lt;- url(\"http://www.jhsph.edu\", \"r\") ## Read the web page \nx &lt;- readLines(con) ## Print out the first few lines head(x)"
  },
  {
    "objectID": "manipu_reading.html#reading-excel-spreadsheets",
    "href": "manipu_reading.html#reading-excel-spreadsheets",
    "title": "5  Reading Data",
    "section": "5.4 Reading Excel Spreadsheets",
    "text": "5.4 Reading Excel Spreadsheets\nIf the dataset is stored in the .xls or .xlsx format you can use the readxl package. This package is non-core tidyverse, so you need to load it explicitly, but it is installed automatically when you install the tidyverse package.\nMost of readxl’s functions allow you to load Excel spreadsheets into R:\n\nread_xls() reads Excel files with xls format.\nread_xlsx() read Excel files with xlsx format.\nread_excel() can read files with both xls and xlsx format. It guesses the file type based on the input.\n\nExample:\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(writexl)\n(students &lt;- read_excel(\"data/students.xlsx\"))\n\n# A tibble: 6 × 5\n  `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4.0  \n2            2 Barclay Lynn     French fries       Lunch only          5.0  \n3            3 Jayendra Lyne    N/A                Breakfast and lunch 7.0  \n4            4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6            6 Güvenç Attila    Ice cream          Lunch only          6.0  \n\n\nAn important feature that distinguishes spreadsheets from flat files is the notion of multiple sheets, called worksheets. You can read a single worksheet from a spreadsheet with the sheet argument in read_excel(). The default, which we’ve been relying on up until now, is the first sheet.\nread_excel(\"data/penguins.xlsx\", sheet = \"Torgersen Island\")\nAlternatively, you can use excel_sheets() to get information on all worksheets in an Excel spreadsheet, and then read the one(s) you’re interested in.\nexcel_sheets(\"data/penguins.xlsx\")\n\n# Read a worksheet individually\npenguins_biscoe &lt;- read_excel(\"data/penguins.xlsx\", sheet = \"Biscoe Island\", na = \"NA\") \nTo know the number of columns and rows of each worksheet you can use the function dim().\ndim(penguins_torgersen)\nWhen working with spreadsheet data, it’s important to keep in mind that the underlying data can be very different than what you see in the cell. For example, Excel has no notion of an integer. All numbers are stored as floating points, but you can choose to display the data with a customizable number of decimal points. Similarly, dates are actually stored as numbers, specifically the number of seconds since January 1, 1970. You can customize how you display the date by applying formatting in Excel. Confusingly, it’s also possible to have something that looks like a number but is actually a string (e.g., type ’10 into a cell in Excel).\n\n5.4.1 Reading part of a sheet\nIt’s quite common to find cell entries in a spreadsheet that are not part of the data you want to read into R. You can use the readxl_example() function to locate the spreadsheet on your system in the directory where the package is installed. This function returns the path to the spreadsheet, which you can use in read_excel()as usual.\nstudents_path &lt;- readxl_example(\"students.xlsx\")\nread_excel(students_path, range = \"A1:C4\")"
  },
  {
    "objectID": "manipu_reading.html#reading-google-sheets",
    "href": "manipu_reading.html#reading-google-sheets",
    "title": "5  Reading Data",
    "section": "5.5 Reading Google Sheets",
    "text": "5.5 Reading Google Sheets\nFor loading data from a Google Sheet you will be using the googlesheets4 package. This package is non-core tidyverse, so you need to load it explicitly.\n\nlibrary(googlesheets4)\nlibrary(tidyverse)\n\nThe main function of the googlesheets4 package is read_sheet(), which reads a Google Sheet from a URL or a file id. This function also goes by the name range_read(). You can also create a brand new sheet with gs4_create() or write to an existing sheet with sheet_write() and friends.\nThe first argument to read_sheet()is the URL of the file to read, and it returns a tibble:\nExample:\n\ngs4_deauth()\nstudents &lt;- read_sheet('https://docs.google.com/spreadsheets/d/1V1nPp1tzOuutXFLb3G9Eyxi3qxeEhnOXUzL5_BcCQ0w')\n\n✔ Reading from \"students\".\n\n\n✔ Range 'Sheet1'.\n\nstudents\n\n# A tibble: 6 × 5\n  `Student ID` `Full Name`      favourite.food     mealPlan            AGE      \n         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;list&gt;   \n1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          &lt;dbl [1]&gt;\n2            2 Barclay Lynn     French fries       Lunch only          &lt;dbl [1]&gt;\n3            3 Jayendra Lyne    N/A                Breakfast and lunch &lt;dbl [1]&gt;\n4            4 Leon Rossini     Anchovies          Lunch only          &lt;NULL&gt;   \n5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch &lt;chr [1]&gt;\n6            6 Güvenç Attila    Ice cream          Lunch only          &lt;dbl [1]&gt;\n\n\nIt’s also possible to read individual sheets from Google Sheets as well. Let’s read the “Torgersen Island” sheet from the penguins Google Sheet:\n\npenguins_sheet_id &lt;- \"1aFu8lnD_g0yjF5O-K6SFgSEWiHPpgvFCF0NY9D6LXnY\"\nread_sheet(penguins_sheet_id, sheet = \"Torgersen Island\")\n\n✔ Reading from \"penguins\".\n\n\n✔ Range ''Torgersen Island''.\n\n\n# A tibble: 52 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;     &lt;list&gt;         &lt;list&gt;        &lt;list&gt;            &lt;list&gt;     \n 1 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n 2 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n 3 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n 4 Adelie  Torgersen &lt;chr [1]&gt;      &lt;chr [1]&gt;     &lt;chr [1]&gt;         &lt;chr [1]&gt;  \n 5 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n 6 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n 7 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n 8 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n 9 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n10 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n# ℹ 42 more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\nYou can obtain a list of all sheets within a Google Sheet with sheet_names():\n\nsheet_names(penguins_sheet_id)\n\n[1] \"Torgersen Island\" \"Biscoe Island\"    \"Dream Island\"    \n\n\nNote that we’re also using the gs4_example() function below to locate an example Google Sheet that comes with the googlesheets4 package.\n\ndeaths_url &lt;- gs4_example(\"deaths\")\ndeaths &lt;- read_sheet(deaths_url, range = \"A5:F15\")\n\n✔ Reading from \"deaths\".\n\n\n✔ Range 'A5:F15'.\n\ndeaths\n\n# A tibble: 10 × 6\n   Name      Profession   Age `Has kids` `Date of birth`     `Date of death`    \n   &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;lgl&gt;      &lt;dttm&gt;              &lt;dttm&gt;             \n 1 David Bo… musician      69 TRUE       1947-01-08 00:00:00 2016-01-10 00:00:00\n 2 Carrie F… actor         60 TRUE       1956-10-21 00:00:00 2016-12-27 00:00:00\n 3 Chuck Be… musician      90 TRUE       1926-10-18 00:00:00 2017-03-18 00:00:00\n 4 Bill Pax… actor         61 TRUE       1955-05-17 00:00:00 2017-02-25 00:00:00\n 5 Prince    musician      57 TRUE       1958-06-07 00:00:00 2016-04-21 00:00:00\n 6 Alan Ric… actor         69 FALSE      1946-02-21 00:00:00 2016-01-14 00:00:00\n 7 Florence… actor         82 TRUE       1934-02-14 00:00:00 2016-11-24 00:00:00\n 8 Harper L… author        89 FALSE      1926-04-28 00:00:00 2016-02-19 00:00:00\n 9 Zsa Zsa … actor         99 TRUE       1917-02-06 00:00:00 2016-12-18 00:00:00\n10 George M… musician      53 FALSE      1963-06-25 00:00:00 2016-12-25 00:00:00\n\n\nWhen you attempt to read in a sheet that requires authentication, googlesheets4 will direct you to a web browser with a prompt to sign in to your Google account and grant permission to operate on your behalf with Google Sheets. However, if you want to specify a specific Google account, authentication scope, etc. you can do so with gs4_auth(), e.g., gs4_auth(email = \"mine@example.com\"), which will force the use of a token associated with a specific email."
  },
  {
    "objectID": "manipu_reading.html#rdata-format",
    "href": "manipu_reading.html#rdata-format",
    "title": "5  Reading Data",
    "section": "5.6 RData Format",
    "text": "5.6 RData Format\nIf you need to store more than one dataset in a single file we can use the *.RData format:\nExample:\n## example to load multiple datasets, and a vector of R objects from a single *.RData\nload(\"robjects.RData\")\n## to check if objects have been loaded correctly\nobjects()"
  },
  {
    "objectID": "manipu_reading.html#import-stataspss-file",
    "href": "manipu_reading.html#import-stataspss-file",
    "title": "5  Reading Data",
    "section": "5.7 Import Stata/SPSS File",
    "text": "5.7 Import Stata/SPSS File\nTo import a Stata/SPSS file into R:\n\nFirst you need to call the foreign library:\ninstall.packages(\"foreign\")\nThen use read.data()/read.spss():\ndata &lt;- read.spss(file=\"data.spss\", to.data.frame=TRUE)\nThe output will always be a data frame:\nwrite.foreign(data, \"mydata.txt\", \"mydata.sps\", package=\"SPSS\")"
  },
  {
    "objectID": "manipu_reading.html#json-file",
    "href": "manipu_reading.html#json-file",
    "title": "5  Reading Data",
    "section": "5.8 JSON File",
    "text": "5.8 JSON File\nTo read a JSON file:\ninstall.packages(\"rjson\") \ndata &lt;- fromJSON(file=\"data.json\")\ndata2 &lt;- as.data.frame(data)"
  },
  {
    "objectID": "manipu_reading.html#view",
    "href": "manipu_reading.html#view",
    "title": "5  Reading Data",
    "section": "5.9 view()",
    "text": "5.9 view()\nTo view the data variable, you can use the view()."
  },
  {
    "objectID": "manipu_reading.html#manual-data-entry",
    "href": "manipu_reading.html#manual-data-entry",
    "title": "5  Reading Data",
    "section": "5.10 Manual Data Entry",
    "text": "5.10 Manual Data Entry\nSometimes you’ll need to assemble a tibble “by hand” doing a little data entry in your R script.\nThere are two useful functions to help you do this which differ in whether you layout the tibble by columns or by rows.\n\ntibble() works by column:\nExample:\n\ntidyr::tibble(\n  x = c(1, 2, 5), \n  y = c(\"h\", \"m\", \"g\"),\n  z = c(0.08, 0.83, 0.60)\n)   \n\n# A tibble: 3 × 3\n      x y         z\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 h      0.08\n2     2 m      0.83\n3     5 g      0.6 \n\n\ntribble() (transposed tibble) lets you lay out your data row by row: column headings start with ~ and entries are separated by commas. This makes it possible to lay out small amounts of data in an easy to read form:\nExample:\n\ntidyr::tribble(\n  ~x, ~y, ~z,\n  1, \"h\", 0.08,\n  2, \"m\", 0.83,\n  5, \"g\", 0.60\n)\n\n# A tibble: 3 × 3\n      x y         z\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 h      0.08\n2     2 m      0.83\n3     5 g      0.6"
  },
  {
    "objectID": "manipu_writing.html#writing-to-excel",
    "href": "manipu_writing.html#writing-to-excel",
    "title": "6  Writing Data",
    "section": "6.1 Writing to Excel",
    "text": "6.1 Writing to Excel\nYou can write data back to disk as an Excel file using the write_xlsx() from the writexl package:\nwrite_xlsx(bake_sale, path = \"data/bake-sale.xlsx\") \nIf you’re interested in additional features like writing to sheets within a spreadsheet and styling, you will want to use the openxlsx package."
  },
  {
    "objectID": "manipu_writing.html#writing-to-google-sheets",
    "href": "manipu_writing.html#writing-to-google-sheets",
    "title": "6  Writing Data",
    "section": "6.2 Writing to Google Sheets",
    "text": "6.2 Writing to Google Sheets\nYou can write from R to Google Sheets with write_sheet(). The first argument is the dataframe to write, and the second argument is the name (or other identifier) of the Google Sheet to write to:\nwrite_sheet(bake_sale, ss = \"bake-sale\")\nIf you’d like to write your data to a specific (work)sheet inside a Google Sheet, you can specify that with the sheet argument as well.\nwrite_sheet(bake_sale, ss = \"bake-sale\", sheet = \"Sales\")"
  },
  {
    "objectID": "manipu_writing.html#writing-a-dataframe",
    "href": "manipu_writing.html#writing-a-dataframe",
    "title": "6  Writing Data",
    "section": "6.3 Writing a Dataframe",
    "text": "6.3 Writing a Dataframe\nTo write a dataframe to a file we use this:\nwrite.table(x, file = \"\", append = FALSE, sep = \" \", row.names = TRUE, col.names = TRUE)\nHere x is the vector to be written. If x is a matrix or array then it is converted to a vector (column by column) before being written. The other parameters are optional.\nWe can identify the complete rows from a two-dimensional object such as a dataframe (that is, rows that have no missing values) via the complete.cases command. We can easily remove rows with missing values using the na.omit function."
  },
  {
    "objectID": "manipu_writing.html#writing-matrices",
    "href": "manipu_writing.html#writing-matrices",
    "title": "6  Writing Data",
    "section": "6.4 Writing Matrices",
    "text": "6.4 Writing Matrices\nBecause write() converts matrices to vectors before writing them, using it to write a matrix to a file can cause unexpected results. Since R stores its matrices by column, you should pass the transpose of the matrix to write if you want the output to reflect the matrix structure.\nx &lt;- matrix(1:24, nrow = 4, ncol = 6)\nwrite(t(x), file = \"../results/out.txt\", ncolumns = 6)"
  },
  {
    "objectID": "manipu_writing.html#cat",
    "href": "manipu_writing.html#cat",
    "title": "6  Writing Data",
    "section": "6.5 cat()",
    "text": "6.5 cat()\nA more flexible command for writing to a file is cat(), which has the form:\ncat(..., file = \"\", sep = \" \", append = FALSE)\nNote that cat does not automatically write a newline after the expressions …. If you want a newline you must explicitly include the string \\n."
  },
  {
    "objectID": "manipu_writing.html#tabular-data",
    "href": "manipu_writing.html#tabular-data",
    "title": "6  Writing Data",
    "section": "6.6 Tabular Data",
    "text": "6.6 Tabular Data\nFor writing tabular data to text files (i.e. CSV) or connections you may use write.table(), which has this format:\nExample:\nwrite.table()"
  },
  {
    "objectID": "manipu_writing.html#dump",
    "href": "manipu_writing.html#dump",
    "title": "6  Writing Data",
    "section": "6.7 dump()",
    "text": "6.7 dump()\nThere is also the very useful dump() function, which creates a text representation of almost any R object that can subsequently be read by source.\nExample:\nx &lt;- matrix(rep(1:5, 1:5), nrow = 3, ncol = 5)\ndump(\"x\", file = \"../results/x.txt\")\nrm(x)\nsource(\"../results/x.txt\")\nx"
  },
  {
    "objectID": "manipu_tidydata.html#installation",
    "href": "manipu_tidydata.html#installation",
    "title": "7  Tidy Data",
    "section": "7.1 Installation",
    "text": "7.1 Installation\n\nInstall all the packages in the tidyverse by running:\ninstall.packages(\"tidyverse\")\nRun library(tidyverse) to load the core tidyverse and make it available in your current R session.\nNote the conflicts message that’s printed when you load the tidyverse. It tells you that dplyr overwrites some functions in base R. If you want to use the base version of these functions after loading dplyr, you’ll need to use their full names: stats::filter() and stats::lag().\nLearn more about the tidyverse package https://tidyverse.tidyverse.org"
  },
  {
    "objectID": "manipu_tidydata.html#core-packages",
    "href": "manipu_tidydata.html#core-packages",
    "title": "7  Tidy Data",
    "section": "7.2 Core Packages",
    "text": "7.2 Core Packages\nlibrary(tidyverse) will load the core tidyverse packages:\n\nggplot2, for data visualisation, more info.\ndplyr, for data manipulation, more info.\ntidyr, for data tidying, more info\nreadr, for data import, more info\npurrr, for functional programming, more info.\ntibble, for tibbles, a modern re-imagining of data frames, more info.\nstringr, for strings, more info\nforcats, for factors, [more info] (https://forcats.tidyverse.org/)\nlubridate, for date/times."
  },
  {
    "objectID": "manipu_tidydata.html#functionalities",
    "href": "manipu_tidydata.html#functionalities",
    "title": "7  Tidy Data",
    "section": "7.3 Functionalities",
    "text": "7.3 Functionalities\n\n7.3.1 Import\nAs well as readr, for reading flat files, the tidyverse package installs a number of other packages for reading data:\n\nDBI for relational databases. You’ll need to pair DBI with a database specific backends like RSQLite, RMariaDB, RPostgres, or odbc. More info here.\nhaven for SPSS, Stata, and SAS data.\nhttr for web APIs.\nreadxl for .xls and .xlsx sheets.\ngooglesheets4 for Google Sheets via the Sheets API v4.\ngoogledrive for Google Drive files.\nrvest for web scraping.\njsonlite for JSON. (Maintained by Jeroen Ooms.)\nxml2 for XML.\n\n\n\n7.3.2 Wrangle\nIn addition to tidyr, and dplyr, there are five packages (including stringr and forcats) which are designed to work with specific types of data:\n\nlubridate for dates and date-times.\nhms for time-of-day values.\nblob for storing blob (binary) data.\n\nThere are also two packages that allow you to interface with different backends using the same dplyr syntax:\n\ndbplyr allows you to use remote database tables by converting dplyr code into SQL.\ndtplyr provides a data.table backend by automatically translating to the equivalent, but usually much faster, data.table code. Program\n\n\n\n7.3.3 Programming\nIn addition to purrr, which provides very consistent and natural methods for iterating on R objects, there are two additional tidyverse packages that help with general programming challenges:\n\nmagrittr provides the pipe, %&gt;% used throughout the tidyverse. It also provide a number of more specialised piping operators (like %$% and %&lt;&gt;%) that can be useful in other places.\nglue provides an alternative to paste() that makes it easier to combine data and strings.\n\n\n\n7.3.4 Modeling\nModeling with the tidyverse uses the collection of tidymodels packages, which largely replace the modelr package used in R4DS.\nVisit the Getting Started Guide for more detailed examples, or go straight to the Learn page."
  },
  {
    "objectID": "manipu_tidydata.html#tidying-data",
    "href": "manipu_tidydata.html#tidying-data",
    "title": "7  Tidy Data",
    "section": "7.4 Tidying Data",
    "text": "7.4 Tidying Data\nWe’ll focus on tidyr, a package that provides a bunch of tools to help tidy up messy datasets. tidyr is a member of the core tidyverse.\nlibrary(tidyverse)\nThere are two main advantages of tidy data:\n\nThere’s a general advantage to picking one consistent way of storing data. If you have a consistent data structure, it’s easier to learn the tools that work with it because they have an underlying uniformity.\nThere’s a specific advantage to placing variables in columns because it allows R’s vectorized nature to shine.\n\n\n7.4.1 Pivot Data\nYou’ll begin by figuring out what the underlying variables and observations are. Sometimes this is easy; other times you’ll need to consult with the people who originally generated the data. Next, you’ll pivot your data into a tidy form, with variables in the columns and observations in the rows.\ntidyr provides two functions for pivoting data: pivot_longer() and pivot_wider(). We’ll first start with pivot_longer() because it’s the most common case. Let’s dive into some examples.\n\n\n7.4.2 Lengthening Data\nLengthening data means increasing the number of rows and decreasing the number of columns. The inverse transformation is pivot_wider().\nIn this dataset, each observation is a song. The first three columns (artist, track and date.entered) are variables that describe the song. Then we have 76 columns (wk1-wk76) that describe the rank of the song in each week1. Here, the column names are one variable (the week) and the cell values are another (the rank).\n\ntidyr::billboard\n\n# A tibble: 317 × 79\n   artist     track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n   &lt;chr&gt;      &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac      Baby… 2000-02-26      87    82    72    77    87    94    99    NA\n 2 2Ge+her    The … 2000-09-02      91    87    92    NA    NA    NA    NA    NA\n 3 3 Doors D… Kryp… 2000-04-08      81    70    68    67    66    57    54    53\n 4 3 Doors D… Loser 2000-10-21      76    76    72    69    67    65    55    59\n 5 504 Boyz   Wobb… 2000-04-15      57    34    25    17    17    31    36    49\n 6 98^0       Give… 2000-08-19      51    39    34    26    26    19     2     2\n 7 A*Teens    Danc… 2000-07-08      97    97    96    95   100    NA    NA    NA\n 8 Aaliyah    I Do… 2000-01-29      84    62    51    41    38    35    35    38\n 9 Aaliyah    Try … 2000-03-18      59    53    38    28    21    18    16    14\n10 Adams, Yo… Open… 2000-08-26      76    76    74    69    68    67    61    58\n# ℹ 307 more rows\n# ℹ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;, …\n\n\nTo tidy this data, we’ll use pivot_longer():\n\ntidyr::billboard |&gt; \n  tidyr::pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE # to get rid of NAs\n  )\n\n# A tibble: 5,307 × 5\n   artist  track                   date.entered week   rank\n   &lt;chr&gt;   &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk1      87\n 2 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk2      82\n 3 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk3      72\n 4 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk4      77\n 5 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk5      87\n 6 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk6      94\n 7 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk7      99\n 8 2Ge+her The Hardest Part Of ... 2000-09-02   wk1      91\n 9 2Ge+her The Hardest Part Of ... 2000-09-02   wk2      87\n10 2Ge+her The Hardest Part Of ... 2000-09-02   wk3      92\n# ℹ 5,297 more rows\n\n\nThis data is now tidy, but we could make future computation a bit easier by converting values of week from character strings to numbers using mutate() and readr::parse_number(). parse_number() is a handy function that will extract the first number from a string, ignoring all other text.\n\nbillboard_longer &lt;- tidyr::billboard |&gt; \n  tidyr::pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE\n  ) |&gt; \n  dplyr::mutate(\n    week = readr::parse_number(week)\n  )\nbillboard_longer\n\n# A tibble: 5,307 × 5\n   artist  track                   date.entered  week  rank\n   &lt;chr&gt;   &lt;chr&gt;                   &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac   Baby Don't Cry (Keep... 2000-02-26       1    87\n 2 2 Pac   Baby Don't Cry (Keep... 2000-02-26       2    82\n 3 2 Pac   Baby Don't Cry (Keep... 2000-02-26       3    72\n 4 2 Pac   Baby Don't Cry (Keep... 2000-02-26       4    77\n 5 2 Pac   Baby Don't Cry (Keep... 2000-02-26       5    87\n 6 2 Pac   Baby Don't Cry (Keep... 2000-02-26       6    94\n 7 2 Pac   Baby Don't Cry (Keep... 2000-02-26       7    99\n 8 2Ge+her The Hardest Part Of ... 2000-09-02       1    91\n 9 2Ge+her The Hardest Part Of ... 2000-09-02       2    87\n10 2Ge+her The Hardest Part Of ... 2000-09-02       3    92\n# ℹ 5,297 more rows\n\n\nOther cases on how to deal with lenghtening data can be found here.\n\n\n7.4.3 Widening data\npivot_wider() makes datasets wider by increasing columns and reducing rows and helps when one observation is spread across multiple rows.\nExample:\n\ntidyr::cms_patient_experience\n\n# A tibble: 500 × 5\n   org_pac_id org_nm                           measure_cd measure_title prf_rate\n   &lt;chr&gt;      &lt;chr&gt;                            &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n 1 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       63\n 2 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       87\n 3 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       86\n 4 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       57\n 5 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       85\n 6 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       24\n 7 0446162697 ASSOCIATION OF UNIVERSITY PHYSI… CAHPS_GRP… CAHPS for MI…       59\n 8 0446162697 ASSOCIATION OF UNIVERSITY PHYSI… CAHPS_GRP… CAHPS for MI…       85\n 9 0446162697 ASSOCIATION OF UNIVERSITY PHYSI… CAHPS_GRP… CAHPS for MI…       83\n10 0446162697 ASSOCIATION OF UNIVERSITY PHYSI… CAHPS_GRP… CAHPS for MI…       63\n# ℹ 490 more rows\n\n\nThe core unit being studied is an organization, but each organization is spread across six rows, with one row for each measurement taken in the survey organization. We can see the complete set of values for measure_cd and measure_title by using distinct():\n\ntidyr::cms_patient_experience |&gt; \n  dplyr::distinct(measure_cd, measure_title)\n\n# A tibble: 6 × 2\n  measure_cd   measure_title                                                    \n  &lt;chr&gt;        &lt;chr&gt;                                                            \n1 CAHPS_GRP_1  CAHPS for MIPS SSM: Getting Timely Care, Appointments, and Infor…\n2 CAHPS_GRP_2  CAHPS for MIPS SSM: How Well Providers Communicate               \n3 CAHPS_GRP_3  CAHPS for MIPS SSM: Patient's Rating of Provider                 \n4 CAHPS_GRP_5  CAHPS for MIPS SSM: Health Promotion and Education               \n5 CAHPS_GRP_8  CAHPS for MIPS SSM: Courteous and Helpful Office Staff           \n6 CAHPS_GRP_12 CAHPS for MIPS SSM: Stewardship of Patient Resources             \n\n\nWe’ll use measure_cd as the source for our new column names for now. Instead of choosing new column names, we need to provide the existing columns that define the values (values_from) and the column name (names_from):\n\ntidyr::cms_patient_experience |&gt; \n  tidyr::pivot_wider(\n    names_from = measure_cd,\n    values_from = prf_rate\n  )\n\n# A tibble: 500 × 9\n   org_pac_id org_nm           measure_title CAHPS_GRP_1 CAHPS_GRP_2 CAHPS_GRP_3\n   &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 0446157747 USC CARE MEDICA… CAHPS for MI…          63          NA          NA\n 2 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          87          NA\n 3 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          86\n 4 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          NA\n 5 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          NA\n 6 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          NA\n 7 0446162697 ASSOCIATION OF … CAHPS for MI…          59          NA          NA\n 8 0446162697 ASSOCIATION OF … CAHPS for MI…          NA          85          NA\n 9 0446162697 ASSOCIATION OF … CAHPS for MI…          NA          NA          83\n10 0446162697 ASSOCIATION OF … CAHPS for MI…          NA          NA          NA\n# ℹ 490 more rows\n# ℹ 3 more variables: CAHPS_GRP_5 &lt;dbl&gt;, CAHPS_GRP_8 &lt;dbl&gt;, CAHPS_GRP_12 &lt;dbl&gt;\n\n\nThe output doesn’t look quite right; we still seem to have multiple rows for each organization. That’s because, we also need to tell pivot_wider() which column or columns have values that uniquely identify each row; in this case those are the variables starting with “org”:\n\ntidyr::cms_patient_experience |&gt; \n  tidyr::pivot_wider(\n    id_cols = starts_with(\"org\"),\n    names_from = measure_cd,\n    values_from = prf_rate\n  )\n\n# A tibble: 95 × 8\n   org_pac_id org_nm CAHPS_GRP_1 CAHPS_GRP_2 CAHPS_GRP_3 CAHPS_GRP_5 CAHPS_GRP_8\n   &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 0446157747 USC C…          63          87          86          57          85\n 2 0446162697 ASSOC…          59          85          83          63          88\n 3 0547164295 BEAVE…          49          NA          75          44          73\n 4 0749333730 CAPE …          67          84          85          65          82\n 5 0840104360 ALLIA…          66          87          87          64          87\n 6 0840109864 REX H…          73          87          84          67          91\n 7 0840513552 SCL H…          58          83          76          58          78\n 8 0941545784 GRITM…          46          86          81          54          NA\n 9 1052612785 COMMU…          65          84          80          58          87\n10 1254237779 OUR L…          61          NA          NA          65          NA\n# ℹ 85 more rows\n# ℹ 1 more variable: CAHPS_GRP_12 &lt;dbl&gt;"
  },
  {
    "objectID": "manipu_transforming.html#headand-tail",
    "href": "manipu_transforming.html#headand-tail",
    "title": "8  Transforming Data",
    "section": "8.1 head()and tail()",
    "text": "8.1 head()and tail()\nAfter reading the file, you can use the head() and tail() functions to examine the object:\nExample:\nhead(dataset)\ntail()"
  },
  {
    "objectID": "manipu_transforming.html#changing-variable-types",
    "href": "manipu_transforming.html#changing-variable-types",
    "title": "8  Transforming Data",
    "section": "8.2 Changing Variable Types",
    "text": "8.2 Changing Variable Types\nA common task after reading in data is to consider variable types. For example, if a variable in a dataset is a categorical variable with a known set of possible values but you see it is represented as a character, then you will want to make it a factor. Or a numeric variable is a character variable because one of the observations is typed out as a character instead of a numeric (e.g. “five” instead of 5).\nExample:\nstudents |&gt;\n  mutate(meal_plan = factor(meal_plan),\n  age = parse_number(if_else(age == \"five\", \"5\", age))\nAfter this, you will that the type of variable denoted underneath the variable name has changed from character () to factor ()."
  },
  {
    "objectID": "manipu_transforming.html#changing-columns-names",
    "href": "manipu_transforming.html#changing-columns-names",
    "title": "8  Transforming Data",
    "section": "8.3 Changing Columns Names",
    "text": "8.3 Changing Columns Names\nNote that you can assign your own column names after reading a dataframe using the names() function, or when you read it in, using the col.names argument, which should be assigned a character vector the same length as the number of columns.\nIf there is no header and no col.names argument, then R uses the names V1, V2, etc."
  },
  {
    "objectID": "manipu_transforming.html#columns-no-syntatic-names",
    "href": "manipu_transforming.html#columns-no-syntatic-names",
    "title": "8  Transforming Data",
    "section": "8.4 Columns No-Syntatic Names",
    "text": "8.4 Columns No-Syntatic Names\nSometimes, you might also notice that the columns names are surrounded by backticks. That’s because they contain spaces, breaking R’s usual rules for variable names; they’re non-syntactic names.\n*Example:*\n\n``` R\nstudents |&gt; \n  rename(\n    student_id = `Student ID`,\n    full_name = `Full Name`\n  )\n```\nAn alternative approach is to use janitor::clean_names() to use some heuristics to turn them all into snake case at once1.\nstudents |&gt; janitor::clean_names()"
  },
  {
    "objectID": "manipu_transforming.html#changing-numbers",
    "href": "manipu_transforming.html#changing-numbers",
    "title": "8  Transforming Data",
    "section": "8.5 Changing Numbers",
    "text": "8.5 Changing Numbers\nIn most cases, numbers will be in one of R’s numeric types: integer or double. In some cases, however, you’ll encounter them as strings, possibly because you’ve created them by pivoting from column headers or because something has gone wrong in your data import process.\nreadr provides two useful functions for parsing strings into numbers: parse_double() and parse_number().\nExample\nExample:\n\nx &lt;- c(\"1.2\", \"5.6\", \"1e3\")\nparse_double(x)\n\n[1]    1.2    5.6 1000.0\n\nx &lt;- c(\"$1,234\", \"USD 3,513\", \"59%\")\nparse_number(x)\n\n[1] 1234 3513   59"
  },
  {
    "objectID": "manipu_transforming.html#removing-duplicates",
    "href": "manipu_transforming.html#removing-duplicates",
    "title": "8  Transforming Data",
    "section": "8.6 Removing Duplicates",
    "text": "8.6 Removing Duplicates\nYou can remove duplicates based on the x variable using:\nx &lt;- c(1, 2, NA, 4, NA, 5) \ndata[!duplicated(data$x), ]"
  },
  {
    "objectID": "manipu_transforming.html#reshaping-datasets",
    "href": "manipu_transforming.html#reshaping-datasets",
    "title": "8  Transforming Data",
    "section": "8.7 Reshaping Datasets",
    "text": "8.7 Reshaping Datasets\nStatistical analysis sometimes requires wide data and sometimes long data. In such cases, we need to be able to fluently and fluidly reshape the data to meet the requirements of statistical analysis. The function reshape() reshapes a dataframe between ‘wide’ format (with repeated measurements in separate columns of the same row) and ‘long’ format (with the repeated measurements in separate rows).\nData reshaping is just a rearrangement of the form of the data—it does not change the content of the dataset.\nExample:\nstudents &lt;- data.frame(sid=c(1,1,2,2), exmterm=c(1,2,1,2), math=c(50,65,75,69), \nliterature=c(40,45,55,59), language=c(70,80,75,78))\n\n# Reshaping dataset using reshape function to wide format\nwide_students &lt;- reshape(students, direction=\"wide\", idvar=\"sid\", timevar=\"exmterm\")\n\n# Now again reshape to long format\nlong_students &lt;- reshape (wide_students, direction=\"long\", idvar=\"id\")\n\n8.7.1 The reshape package\n\nMelting data (molten data):\n\nThough melting can be applied to different R objects, the most common use is to melt a data frame.\nTo perform melting operations using the melt function, we need to know what the identification variables and measured variables in the original input dataset are.\nOne important thing to note is that, whenever we use the melt function, all the measured variables should be of the same type, that is, the measured variables should be either numeric, factor, character, or date.\nTo deal with the implicit missing value, it is good to use na.rm = TRUE with the melt function to remove the structural missing value (i.e., it will fill empty cells in the data table with NA).\n\nExample:\n## the format of the resulting table is id/variable/value\nmelt(students, id=c(\"sid\",\"exmterm\"), measured=c(\"math\", \"literature\", \"language\"))\nCasting molten data:\n\nOnce we have molten data, we can rearrange it in any layout using the cast function from the reshape package.\nThere are two main arguments required to cast molten data: data and formula.\nThe basic casting formula is col_var_1+col_var_2 ~ row_var_1+ row_var_2, which describes the variables to appear in columns and rows.\n\nExample:\n# to return to the original data structure  \ncast(molten_students, sid+exmterm ~ variable)\nFor faster and large data rearrangement, use the reshape2 package and the functions dcast (data frames) and acast (arrays):\nExample:\nacast(molten_students, sid+exmterm~variable)"
  },
  {
    "objectID": "manipu_transforming.html#missing-values",
    "href": "manipu_transforming.html#missing-values",
    "title": "8  Transforming Data",
    "section": "8.8 Missing Values",
    "text": "8.8 Missing Values\nR represents missing observations through the data value NA. It is easiest to think of NA values as place holders for data that should have been there, but, for some reason, are not.\n\n8.8.1 Detect NA\nWe can detect whether variables are missing value using:\n\nis.na() is used to test objects if they are NA.\nis.nan() is used to test for NaN.\n\nExample:\n\nx &lt;- c(1, 2, NaN, NA, 4) \nis.na(x) \n\n[1] FALSE FALSE  TRUE  TRUE FALSE\n\nis.nan(x)\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\n\nTo check if there is any NA in a dataframe:\nExample:\n\nany(is.na(x))\n\n[1] TRUE\n\n\n\n\n8.8.2 Remove NAs\nExample:\n\na &lt;- c(11, NA, 13)\nmean(a, na.rm = TRUE)\n\n[1] 12\n\n\n\n\n8.8.3 Extract NA from a vector\nExample:\n\nx &lt;- c(1, 2, NA, 3) \nz &lt;- is.na(x) \nx[!z]\n\n[1] 1 2 3\n\n\n\n\n8.8.4 NA vs NULL\nNote that NA and NULL are not equivalent:\n\nNA is a place holder for something that exists but is missing.\nNULL stands for something that never existed at all.\n\n\n\n8.8.5 Removing NA values\nYou can remove rows with NA values in any variables:\nna.omit(data)\n\n## Another example\nx &lt;- c(1, 2, NA, 4, NA, 5) \nbad &lt;- is.na(x) \nx[!bad]\n\n[1] 1 2 4 5\n\n\nHow can you take the subset with no missing values in any of those objects?:\nExample:\ngood &lt;- complete.cases(airquality) \nhead(airquality[good,])"
  },
  {
    "objectID": "manipu_subsetting.html#operator",
    "href": "manipu_subsetting.html#operator",
    "title": "9  Subsetting R Objects",
    "section": "9.1 [ operator",
    "text": "9.1 [ operator\nThe [ operator always returns an object of the same class as the original:\n\nx &lt;- c(\"a\", \"b\", \"c\", \"c\", \"d\", \"a\")\n# Extract the first element\nx[1]\n\n[1] \"a\"\n\n\n\n# other examples\nx[1:4]\n\n[1] \"a\" \"b\" \"c\" \"c\"\n\nx[c(1, 3, 49)]\n\n[1] \"a\" \"c\" NA \n\n\n\nx &lt;- 100:110\ni &lt;- c(1, 3 ,2)\nx[i]\n\n[1] 100 102 101\n\n\n\nu &lt;- x &gt; \"a\" \nx[u] ## or x[x &gt; \"a\"]\n\ninteger(0)\n\n\n\nSubsetting a matrix:\nx[1,2] ## Extract the first row, first column \nx[1, ] ## Extract the first row \nx[, 2] ## Extract the second column \nx[1, 2, drop = FALSE] ## keeps the matrix format\nIt can be used to select multiple elements of an object (also in lists or dataframes):\n\n(x &lt;- list(foo = 1:4, bar = 0.6, baz = \"hello\"))\n\n$foo\n[1] 1 2 3 4\n\n$bar\n[1] 0.6\n\n$baz\n[1] \"hello\"\n\nx[c(1, 3)]\n\n$foo\n[1] 1 2 3 4\n\n$baz\n[1] \"hello\"\n\nx[\"foo\"]\n\n$foo\n[1] 1 2 3 4\n\nclass(x[\"foo\"])\n\n[1] \"list\"\n\nx$foo[1]\n\n[1] 1\n\nx$baz[1]\n\n[1] \"hello\"\n\n\n## use of negative subscript removes first element \"3\" \nnum10[-1]"
  },
  {
    "objectID": "manipu_subsetting.html#operator-1",
    "href": "manipu_subsetting.html#operator-1",
    "title": "9  Subsetting R Objects",
    "section": "9.2 [[ operator",
    "text": "9.2 [[ operator\nThe [[ operator is used to extract elements of a list or a dataframe. It can only be used to extract a single element and the class of the returned object will not necessarily be a list or data frame.\n\nSubsetting a Dataframe:\n# Use the notation [[ ]] to extract columns\nx[[\"var1\"]] = x[, 1] = x$var1 # All are equivalent\nSubsetting a List:\nx[[1]] ## Extract single element from a list \nx[[\"bar\"]] ## Extract named index\nx$bar ## Extract named index \nx[[c(1, 3)]] ## Get the 3rd element of the 1st element of the list \nx[[1]][[3]] ## Get the 3rd element of the 1st element of the list\nNow if we want to get access to the individual elements of list_obj[[2]], we have to use the following command:\n\n(data_2variable &lt;- data.frame(x1=c(2,3,4,5,6), x2=c(5,6,7,8,1)))\n\n  x1 x2\n1  2  5\n2  3  6\n3  4  7\n4  5  8\n5  6  1\n\n(list_obj &lt;- list(dat=data_2variable, vec.obj=c(1,2,3))) \n\n$dat\n  x1 x2\n1  2  5\n2  3  6\n3  4  7\n4  5  8\n5  6  1\n\n$vec.obj\n[1] 1 2 3\n\nlist_obj[[2]][1]\n\n[1] 1"
  },
  {
    "objectID": "manipu_subsetting.html#operator-2",
    "href": "manipu_subsetting.html#operator-2",
    "title": "9  Subsetting R Objects",
    "section": "9.3 $ operator",
    "text": "9.3 $ operator\nThe $ operator is used to extract elements of a list or data frame by literal name. Its semantics are similar to that of [[.\nExample:\n\nx[[\"bar\"]] ## Extract named index \n\n[1] 0.6\n\nx$bar ## Extract named index\n\n[1] 0.6"
  },
  {
    "objectID": "manipu_plyrdplyr.html#plyr",
    "href": "manipu_plyrdplyr.html#plyr",
    "title": "10  Plyr & Dplyr",
    "section": "10.1 Plyr",
    "text": "10.1 Plyr\nThe most important utility of the plyr package is that a single line of code can perform all the split(), apply(), and combine() steps.\nThe steps for the split-apply-combine approach of data analysis are as follows:\n\nFirst, we split the dataset into some mutually exclusive groups.\nWe then apply a task on each group and combine all the results to get the desired output.\nThis group-wise task could be generating new variables, summarizing existing variables, or even performing regression analysis on each group.\nFinally, combining approaches helps us get a nice output to compare the results from different groups.\n\nExample:\n\nlibrary(plyr)\nddply(iris, .(Species), function(x) colMeans(x[-5]))\n\n     Species Sepal.Length Sepal.Width Petal.Length Petal.Width\n1     setosa        5.006       3.428        1.462       0.246\n2 versicolor        5.936       2.770        4.260       1.326\n3  virginica        6.588       2.974        5.552       2.026\n\n\n\nThe first argument is the name of the data frame. We put iris, since the iris dataset is in the data frame structure, and we want to work on it.\nThe second argument is for a variable or variables, according to which we want to split our data frame. In this case, we have Species.\nThe third argument is a function that defines what kind of task we want to perform on each subset.\n\nNote that the first letter of the function name specifies the input, and the second letter specifies the output type:\n\n\n\nTable 1: Types of funcctions in the plyr package\n\n\nNote: mapply() can take multiple inputs as separate arguments, whereas a*ply() takes only a single array argument."
  },
  {
    "objectID": "manipu_plyrdplyr.html#dplyr",
    "href": "manipu_plyrdplyr.html#dplyr",
    "title": "10  Plyr & Dplyr",
    "section": "10.2 Dplyr",
    "text": "10.2 Dplyr\nQuite often, in real-life situations, we start our analysis with a dataframe-type structure. What do we do after getting a dataset and what are the basic data-manipulation tasks we usually perform before starting modeling?:\n\nCheck the validity of a dataset based on conditions.\nSort the dataset based on some variables, in ascending or descending order.\nCreate new variables based on existing variables.\nFinally, summarize them.\n\ndplyr can work with other data frame “backends” such as SQL databases. In fact, there is an SQL interface for relational databases via the DBI package\ndplyr can also be integrated with the data.table package for large fast tables.\n\n10.2.1 dplyr Grammar\nAll dplyr functions have a few common characteristics:\n\nThe first argument is a dataframe.\nThe subsequent arguments typically describe which columns to operate on using the variable names (without quotes).\nThe output is a new datafram (dplyr doesn’t modify the existing original dataset because dplyr functions never modify their inputs).\n\ndplyr’s verbs are organized into four groups based on what they operate on: rows, columns, groups, or tables.\n\nTable 2. Dplyr functions\n\n\nColumns\nRows\n\n\n\n\nselect()\nfilter() (base: subset)\n\n\nrename()\narrange()\n\n\nmutate()\ndistinct()\n\n\nrelocate()\nslice()\n\n\n\nBecause each verb does one thing well, solving complex problems will usually require combining multiple verbs, and we’ll do so with the pipe, |&gt;.\nFor verbs to work at all, dataframes must be properly formatted and annotated. In particular, the data must be tidy, that is, the data comply with the following three interrelated rules:\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\nNote that there’s a specific advantage to placing variables in columns because it allows R’s vectorized nature to shine. dplyr, ggplot2, and all the other packages in the tidyverse are designed to work with tidy data.\n\n\n10.2.2 The Pipe\nThe real power of dplyr arises when you start to combine multiple verbs. For example, imagine that you wanted to find the fastest flights to Houston’s IAH airport: you need to combine filter(), mutate(), select(), and arrange():\nThe pipe takes the thing on its left and passes it along to the function on its right so that x |&gt; f(y) is equivalent to f(x, y), and x |&gt; f(y) |&gt; g(z) is equivalent to g(f(x, y), z). The easiest way to pronounce the pipe is “then”.\nExample:\nflights |&gt; \n  filter(dest == \"IAH\") |&gt; \n  mutate(speed = distance / air_time * 60) |&gt; \n  select(year:day, dep_time, carrier, flight, speed) |&gt; \n  arrange(desc(speed))\nEven though this pipeline has four steps, it’s easy to skim because the verbs come at the start of each line: start with the flights data, then filter, then mutate, then select, then arrange.\nIMPORTANT: Chaining (|&gt;) is a powerful feature of dplyr that allows the output from one verb to be piped into the input of another verb using a short, easy-to-read syntax. To add the pipe to your code, we recommend using the built-in keyboard shortcut Ctrl/Cmd + Shift + M. You’ll need to make one change to your RStudio options to use |&gt; instead of %&gt;%.\n\n\n10.2.3 filter()\nThe filter() function is used to extract subsets of rows from a dataframe. This function is similar to the existing subset().\nExample:\nchic.f &lt;- filter(chicago, pm25tmean2 &gt; 30 & tmpd &gt; 80) \nsummary(chic.f$pm25tmean2)\nThe tidyverse alternative writing:\n# Flights with a departure dely higher than 120 mins\nflights |&gt; \n  filter(dep_delay &gt; 120)\nSometimes, it is more important to subset the dataframe based on values of a variable or multiple variables.\nExample:\nfilter(iris, Species==\"virginica\")\nfilter(iris, Species==\"virginica\" & Sepal.Length &lt;6 & Sepal. Width &lt;=2.7)\nTidyverse:\n# Flights that departed on January 1\nflights |&gt; \n  filter(month == 1 & day == 1)\n# Flights that departed in January or February\nflights |&gt; \n  filter(month == 1 | month == 2)\nThere’s a useful shortcut when you’re combining | and ==: %in%. It keeps rows where the variable equals one of the values on the right:\nflights |&gt; \n  filter(month %in% c(1, 2))\n\n\n10.2.4 arrange()\nThe arrange() function is used to change the order of the rows of a dataframe according to the value of the variables/columns (while preserving corresponding order of other columns).\nExample 1:\nchicago &lt;- arrange(chicago, date) \n## Columns can be arranged in descending order\nchicago &lt;- arrange(chicago, desc(date))\narrange(iris, Sepal.Length, desc(Sepal.Width))\nIt takes a data frame and a set of column names (or more complicated expressions) to order by. If you provide more than one column name, each additional column will be used to break ties in the values of the preceding columns.\nTidyverse:\n# Sorts by the departure time, which is spread over four columns\n# We get the earliest years first, then within a year, the earliest months, etc.\nflights |&gt; \n  arrange(year, month, day, dep_time)\n# Use desc() to re-order the df based on that column in big-to-small order  \nflights |&gt; \n  arrange(desc(dep_delay))\nNote that the number of rows has not changed – we’re only arranging the data, we’re not filtering it.\n\n\n10.2.5 distinct()\nSometimes, we might encounter duplicate observations in a data frame. The distinct() function helps eliminates these observations (finds all the unique rows in a dataset).\nExample:\ndistinct(iris, Species, Petal.Width)\nTidyverse:\nflights |&gt; \n  distinct()\nMost of the time, however, you’ll want the distinct combination of some variables, so you can also optionally supply column names:\nflights |&gt; \n  distinct(origin, dest)\n  \n# Keep other columns when filtering for unique rows\nflights |&gt; \n  distinct(origin, dest, .keep_all = TRUE)\nIf you want to find the number of occurrences instead, you’re better off swapping distinct() for count(). With the sort = TRUE argument, you can arrange them in descending order of the number of occurrences.\nExample:\nflights |&gt;\n  count(origin, dest, sort = TRUE)\n\n\n10.2.6 slice()\nYou can extract the subset of a dataframe using the slice() function.\nExample:\nslice(iris, 95:105)\nThere are five handy functions that allow you to extract specific rows within each group:\n# Takes the first row from each group.\ndf |&gt; slice_head(n = 1)\n# Takes the last row in each group\ndf |&gt; slice_tail(n = 1) .\n# Takes the row with the smallest value of column x\ndf |&gt; slice_min(x, n = 1) \n# Takes the row with the largest value of column x\ndf |&gt; slice_max(x, n = 1) \n# Takes one random row\ndf |&gt; slice_sample(n = 1)\nYou can vary n to select more than one row, or instead of n =, you can use prop = 0.1 to select (e.g.) 10% of the rows in each group. For example, the following code finds the flights that are most delayed upon arrival at each destination:\nExample:\nflights |&gt; \n  group_by(dest) |&gt; \n  slice_max(arr_delay, n = 1) |&gt;\n  relocate(dest)\n\n\n10.2.7 select()\nMost of the time, you do not work on all the variables in a dataframe. Selecting a few columns could make the analysis process less complicated. The select() function can be used to select columns of a data frame that you want to focus on.\nExample:\nchicago &lt;- readRDS(\"chicago.rds\")\nnames(chicago)[1:3] \nselect(chicago, c(\"city\", \"tmpd\"))\nselect(chicago, c(1, 3))\nsubset &lt;- select(chicago, city:dptp)\nTidyverse:\n\nSelect columns by name:\nflights |&gt; \n  select(year, month, day)\nSelect all columns between year and day (inclusive):\nflights |&gt; \n  select(year:day)\nSelect all columns except those from year to day (inclusive):\nflights |&gt; \n  select(!year:day)\nSelect all columns that are characters:\nflights |&gt; \n  select(where(is.character))\n\nThere are a number of helper functions you can use within select():\n\nstarts_with(\"abc\"): matches names that begin with “abc”.\nends_with(\"xyz\"): matches names that end with “xyz”.\ncontains(\"ijk\"): matches names that contain “ijk”.\nnum_range(\"x\", 1:3): matches x1, x2 and x3.\n\nYou can rename variables as you select() them by using =. The new name appears on the left-hand side of the =, and the old variable appears on the right-hand side\nExample:\nflights |&gt; \n  select(tail_num = tailnum)\nYou can also omit variables using the select() function by using the negative sign:\nExample:\nselect(chicago, -(city:dptp))\n\n\n10.2.8 rename()\nRenaming a variable in a dataframe in R is surprisingly hard to do! The rename() function is designed to make this process easier.\nExample:\nchicago &lt;- rename(chicago, dewpoint = dptp, pm25 = pm25tmean2)\nThe syntax inside the rename() function is to have the new name on the left-hand side of the = sign and the old name on the right-hand side.\nTidyverse\nflights |&gt; \n  rename(tail_num = tailnum)\n\n\n10.2.9 mutate()\nThe mutate() function exists to compute transformations of variables in a dataframe. Very often, you want to create new variables that are derived from existing variables and mutate() provides a clean interface for doing that (it adds new columns that are calculated from the existing columns).\nExample:\n# Create a pm25detrend variable that subtracts the mean from the pm25 variable\nchicago &lt;- mutate(chicago, pm25detrend = pm25 - mean(pm25, na.rm = TRUE))\n# Other example\nmutate(iris, SLm=Sepal.Length/100, SWm= Sepal.Width/100, PLm=Petal. Length/100, PWm= Petal.Width/100 )\nTidyverse:\nflights |&gt; \n  mutate(\n    gain = dep_delay - arr_delay,\n    speed = distance / air_time * 60\n  )\nYou can use the .before argument to add the variables to the left-hand side.\nExample:\nflights |&gt; \n  mutate(\n    gain = dep_delay - arr_delay,\n    speed = distance / air_time * 60,\n    .before = 1\n  )\nYou can also use .after to add after a variable, and in both .before and .after you can use the variable name instead of a position.\nNote that since we haven’t assigned the result of the above computation back to flights, the new variables gain, and speed will only be printed but will not be stored in a dataframe.\nAlternatively, you can control which variables are kept with the .keep argument. A particularly useful argument is \"used\" which specifies that we only keep the columns that were used in the “create” step with mutate(). For example, the following output will contain only the variables dep_delay, arr_delay, air_time, gain, hours, and gain_per_hour.\nflights |&gt; \n  mutate(\n    gain = dep_delay - arr_delay,\n    hours = air_time / 60,\n    gain_per_hour = gain / hours,\n    .keep = \"used\"\n  )\nIf you want to keep only the new variables and drop the old ones, we could use the transmute() function:\nExample:\n## Here we detrend the PM10 and ozone (O3) variables\ntransmute(chicago, pm10detrend = pm10tmean2 - mean(pm10tmean2, na.rm = TRUE), \n    o3detrend = o3tmean2 - mean(o3tmean2, na.rm = TRUE)))\n## Note that there are only two columns in the transmuted data frame\n\n\n10.2.10 relocate()\nUse relocate() to move variables around. You might want to collect related variables together or move important variables to the front. By default relocate() moves variables to the front.\nTidyverse\nflights |&gt; \n  relocate(time_hour, air_time)\nYou can also specify where to put them using the .before and .after arguments, just like in mutate():\nflights |&gt; \n  relocate(year:dep_time, .after = time_hour)\nflights |&gt; \n  relocate(starts_with(\"arr\"), .before = dep_time)\n\n\n10.2.11 group_by()\nThe group_by() function is used to divide your dataset into groups meaningful for your analysis. You will usually use the group_by() function in conjunction with the summarize() function.\nExample 1:\nflights |&gt; \n  group_by(month)\nYou can create groups using more than one variable. For example, we could make a group for each date.\nExample:\ndaily &lt;- flights |&gt;  \n  group_by(year, month, day)\nNote that group_by() doesn’t change the data but, if you look closely at the output, you’ll notice that the output indicates that it is “grouped by” month. This means subsequent operations will now work “by month”. group_by() adds this grouped feature (referred to as class) to the dataframe, which changes the behavior of the subsequent verbs applied to the data.\nYou might also want to remove grouping from a dataframe without using summarize(). You can do this with ungroup().\nExample:\ndaily |&gt; \n  ungroup()\n\n\n10.2.12 summarize()\nThe summarize() function is used to calculate a single summary statistic, and reduces the data frame to have a single row for each group.\nExample:\nflights |&gt; \n  group_by(month) |&gt; \n  summarize(\n    avg_delay = mean(dep_delay)\n  )\nYou can create any number of summaries in a single call to summarize(). One very useful summary is n(), which returns the number of rows in each group.\nExample:\nflights |&gt; \n  group_by(month) |&gt; \n  summarize(\n    avg_delay = mean(dep_delay, na.rm = TRUE), \n    n = n()\n  )\n\n\n10.2.13 Chaining (%&gt;%)\nSometimes, it could be necessary to use multiple functions to perform a single task. The pipeline operator %&gt;% is very handy for stringing together multiple dplyr functions in a sequence of operations.\nThis nesting is not a natural way to think about a sequence of operations:\nthird(second(first(x)))\nThe %&gt;% operator allows you to string operations in a left-to-right fashion:\nfirst(x) %&gt;% second(x) %&gt;% third(x)\nThis way we don’t have to create a set of temporary variables along the way or create a massive nested sequence of function calls.\nOnce you travel down the pipeline with %&gt;%, the first argument is taken to be the output of the previous element in the pipeline."
  },
  {
    "objectID": "manipu_date.html#date-and-time-classes",
    "href": "manipu_date.html#date-and-time-classes",
    "title": "11  Date Manipulation",
    "section": "11.1 Date and Time Classes",
    "text": "11.1 Date and Time Classes\nYou can use the Sys.Date and Sys.time functions to pull date and time objects respectively:\n\n(date &lt;- Sys.Date())\n\n[1] \"2024-10-03\"\n\nclass(date)\n\n[1] \"Date\"\n\n(time_ct &lt;- Sys.time())\n\n[1] \"2024-10-03 11:52:43 CEST\"\n\nclass(time_ct)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nBy default, the Sys.time function returns an object of the POSIXct, POSIXt, or POSIXct class. We can use the as.POSIXlt function to convert the object to a POSIXlt object:\n\n(time_lt &lt;- as.POSIXlt(time_ct))\n\n[1] \"2024-10-03 11:52:43 CEST\"\n\nclass(time_lt)\n\n[1] \"POSIXlt\" \"POSIXt\" \n\n\nWhile both the POSIXct and POSIXlt objects have the same representation, the key difference between the two is in the method in which each object is stored internally in the time details. The POSIXct object stored the numeric distance of the time object from the origin point. On the other hand, POSIXlt returned a detailed list with the different time components:\n\nunclass(time_ct)\n\n[1] 1727949164\n\nunclass(time_lt)\n\n$sec\n[1] 43.57581\n\n$min\n[1] 52\n\n$hour\n[1] 11\n\n$mday\n[1] 3\n\n$mon\n[1] 9\n\n$year\n[1] 124\n\n$wday\n[1] 4\n\n$yday\n[1] 276\n\n$isdst\n[1] 1\n\n$zone\n[1] \"CEST\"\n\n$gmtoff\n[1] 7200\n\nattr(,\"tzone\")\n[1] \"\"     \"CET\"  \"CEST\"\nattr(,\"balanced\")\n[1] TRUE\n\n# You can pull the day of the year\nunclass(time_lt)$yday\n\n[1] 276"
  },
  {
    "objectID": "manipu_date.html#creating-date-and-time-objects",
    "href": "manipu_date.html#creating-date-and-time-objects",
    "title": "11  Date Manipulation",
    "section": "11.2 Creating Date and Time Objects",
    "text": "11.2 Creating Date and Time Objects\nCreating a new object or converting the existing object to Date, POSIXlt, or POSIXct can be done with as.Date, as.POSIXlt, or as.POSIXct respectively.\nThe built-in R function as.Date() can handle only dates but not time. For instance, we can convert the “2014-5-12” string into a Date object using the as.Date function:\nExample:\n\nclass(\"2014-5-12\")\n\n[1] \"character\"\n\nclass(as.Date(\"1970-01-01\"))\n\n[1] \"Date\"\n\n# You can convert the Date object into a number\nclass(as.numeric(as.Date(\"1970-01-01\")))\n\n[1] \"numeric\"\n\n\nNote that if the format of the input object is different from the ISO 8601 standard, using the as.Date function without declaring the object format structure would return incorrect results (and, in some instances, an error). For example, let’s try to convert the 31-01-2018 date to Date format without reformatting it:\nExample:\n\nas.Date(\"31-01-2018\")\n\n[1] \"0031-01-20\"\n\n\nOne way to solve this issue is by adding the format argument in the as.Date function in order to map the different components of the input object to the structure of the Date object. The mapping should be according to the date components order of the input object:\n\nas.Date(\"31-01-2018\", format = \"%d-%m-%Y\")\n\n[1] \"2018-01-31\"\n\n\nYou can find a summary of the main arguments for date objects using ?strptime.\nThe as.POSIXct or as.POSIXlt function works in a similar way:\n\n(time_ct &lt;- as.POSIXct(\"2014-5-12 20:05:35\", tz = \"EST\"))\n\n[1] \"2014-05-12 20:05:35 EST\"\n\nclass(time_ct)\n\n[1] \"POSIXct\" \"POSIXt\""
  },
  {
    "objectID": "manipu_date.html#reformatting-and-converting-date-objects",
    "href": "manipu_date.html#reformatting-and-converting-date-objects",
    "title": "11  Date Manipulation",
    "section": "11.3 Reformatting and Converting Date Objects",
    "text": "11.3 Reformatting and Converting Date Objects\nReformatting or converting date objects is the process of transforming a non-date (or POSIXct/lt) object such as character or numeric to a Date format (or POSIXct/lt).\nExample:\n\nurl &lt;- \"https://github.com/PacktPublishing/Hands-On-Time-Series-Analysis-with-R/raw/refs/heads/master/Chapter02/dates_formats.csv\"\ndates_df &lt;- read.csv(url, stringsAsFactors = FALSE)\nstr(dates_df)\n\n'data.frame':   22 obs. of  7 variables:\n $ Japanese_format     : chr  \"2017/1/20\" \"2017/1/21\" \"2017/1/22\" \"2017/1/23\" ...\n $ US_format           : chr  \"1/20/2017\" \"1/21/2017\" \"1/22/2017\" \"1/23/2017\" ...\n $ US_long_format      : chr  \"Friday, January 20, 2017\" \"Saturday, January 21, 2017\" \"Sunday, January 22, 2017\" \"Monday, January 23, 2017\" ...\n $ CA_mix_format       : chr  \"January 20, 2017\" \"January 21, 2017\" \"January 22, 2017\" \"January 23, 2017\" ...\n $ SA_mix_format       : chr  \"20 January 2017\" \"21 January 2017\" \"22 January 2017\" \"23 January 2017\" ...\n $ NZ_format           : chr  \"20/01/2017\" \"21/01/2017\" \"22/01/2017\" \"23/01/2017\" ...\n $ Excel_Numeric_Format: int  42755 42756 42757 42758 42759 42760 42761 42762 42763 42764 ...\n\n\nThe first six columns are character objects and the seventh object is numeric. Let’s convert each one of the columns to a date object by identifying the date structure and reformat it accordingly.\n\n# Convert 1st column to Date format (ISO 8601 format)\ndates_df$Japanese_format_new &lt;- as.Date(dates_df$Japanese_format)\nclass(dates_df$Japanese_format)\n\n[1] \"character\"\n\nclass(dates_df$Japanese_format_new)\n\n[1] \"Date\"\n\n\n\n# Convert the 2nd column (no ISO 8601 format)\ndates_df$US_format_new &lt;- as.Date(dates_df$US_format, format = \"%m/%d/%Y\")\n\nThe same logic applies to the remaining columns:\n# Convert the 3rd column\ndates_df$US_long_format_new &lt;- as.Date(dates_df$US_long_format, format = \"%A, %B %d, %Y\")\n# Convert the 4th column\ndates_df$CA_mix_format_new &lt;- as.Date(dates_df$CA_mix_format, format = \"%B %d, %Y\")\n# Convert the 5th column\ndates_df$SA_mix_format_new &lt;- as.Date(dates_df$SA_mix_format, format = \"%d %B %Y\")\n# Convert the 6th column\ndates_df$NZ_format_new &lt;- as.Date(dates_df$NZ_format, format = \"%d/%m/%Y\")\nIn Excel, the numeric value of the origin point (that is January 1st, 1900) set to 1, as opposed to other programming languages such as R which define the origin point as 0. Therefore, when importing a date or time objects from Excel, in order to align to R origin point definition, the origin point should be set as December 31st, 1899 (which equivalent to 0 numeric value). Therefore, since Excel is using a different origin point than R (December 30, 1899 versus January 1, 1970), we will have to add the origin argument and specify the original date that is used to generate the data:\nExample:\n\nhead(dates_df$Excel_Numeric_Format)\n\n[1] 42755 42756 42757 42758 42759 42760\n\ndates_df$Excel_Numeric_Format_new &lt;- as.Date(dates_df$Excel_Numeric_Format, origin = as.Date(\"1899-12-30\"))\nhead(dates_df$Excel_Numeric_Format_new)\n\n[1] \"2017-01-20\" \"2017-01-21\" \"2017-01-22\" \"2017-01-23\" \"2017-01-24\"\n[6] \"2017-01-25\""
  },
  {
    "objectID": "manipu_date.html#reformatting-and-converting-time-objects",
    "href": "manipu_date.html#reformatting-and-converting-time-objects",
    "title": "11  Date Manipulation",
    "section": "11.4 Reformatting and Converting Time Objects",
    "text": "11.4 Reformatting and Converting Time Objects\nSimilar to the as.Date function, the as.POSIXct or as.POSIXlt functions are the base package applications for reformatting and conversion of any time input to a POSIXct or POSIXlt objects, respectively.\nThe POSIX classes are an extension of the Date class, with the addition of four elements (in addition to the date elements): hours, minutes, seconds, and time zone. The mapping will now include seven elements instead of four elements, as was the case with Date class.\n\n# Input follows the ISO 8601 standard\ntime_str &lt;- \"2018-12-31 23:59:59\"\nclass(time_str)\n\n[1] \"character\"\n\ntime_posix_ct1 &lt;- as.POSIXct(time_str)\nclass(time_posix_ct1)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nLet’s convert now the number 1546318799 to a time object using the as.POSIXct function with the origin argument:\nExample:\n\ntime_numeric &lt;- 1546318799\nclass(time_numeric)\n\n[1] \"numeric\"\n\ntime_posix_ct2 &lt;- as.POSIXct(time_numeric, origin = \"1970-01-01\")\nprint(c(time_posix_ct1, time_posix_ct2))\n\n[1] \"2018-12-31 23:59:59 CET\" \"2019-01-01 05:59:59 CET\"\n\n\nHowever, whenever the format of the input object does not follow a YYYY-m-d H:M:S structure, you will have to use the format argument to map the object’s elements.\nExample:\n\n# Monday, December 31, 2018 11:59:59 PM.\ntime_US_str &lt;- \"Monday, December 31, 2018 11:59:59 PM\"\n(time_posix_ct3 &lt;- as.POSIXct(time_US_str, format = \"%A, %B %d, %Y %I:%M:%S %p\"))\n\n[1] NA\n\n\nThe full arguments list can also be found using ?strptime."
  },
  {
    "objectID": "manipu_date.html#time-zone-setting",
    "href": "manipu_date.html#time-zone-setting",
    "title": "11  Date Manipulation",
    "section": "11.5 Time Zone Setting",
    "text": "11.5 Time Zone Setting\nThe time zone is the seventh element of the POSIX classes and can be set by either the tz argument of the as.POSIXct/as.POSIXlt functions or by the format argument.\nExample:\n\nSys.timezone()\n\n[1] \"Europe/Madrid\"\n\ntime_str &lt;- \"2024-09-30 11:25:49\"\ntime_default_tz &lt;- as.POSIXct(time_str)\n\n# Load a date object from different time zone\ntime_assign_tz &lt;- as.POSIXct(time_str, tz = \"GMT\")\n\nprint(c(time_default_tz, time_assign_tz))\n\n[1] \"2024-09-30 11:25:49 CEST\" \"2024-09-30 13:25:49 CEST\"\n\n\nA full list of the 592 time zones available in R (both location and abbreviation formats) can be found in the OlsonNames function:\n\nhead(OlsonNames(), 20)\n\n [1] \"Africa/Abidjan\"       \"Africa/Accra\"         \"Africa/Addis_Ababa\"  \n [4] \"Africa/Algiers\"       \"Africa/Asmara\"        \"Africa/Asmera\"       \n [7] \"Africa/Bamako\"        \"Africa/Bangui\"        \"Africa/Banjul\"       \n[10] \"Africa/Bissau\"        \"Africa/Blantyre\"      \"Africa/Brazzaville\"  \n[13] \"Africa/Bujumbura\"     \"Africa/Cairo\"         \"Africa/Casablanca\"   \n[16] \"Africa/Ceuta\"         \"Africa/Conakry\"       \"Africa/Dakar\"        \n[19] \"Africa/Dar_es_Salaam\" \"Africa/Djibouti\""
  },
  {
    "objectID": "manipu_date.html#creating-a-date-or-time-index-vector",
    "href": "manipu_date.html#creating-a-date-or-time-index-vector",
    "title": "11  Date Manipulation",
    "section": "11.6 Creating a Date or Time Index Vector",
    "text": "11.6 Creating a Date or Time Index Vector\nThe base package provides two pairs of functions, seq.Date and seq.POSIXt, to create a time index vector with Date or POSIX objects respectively. The main difference between the two functions (besides the class of the output) is the units of the time interval.\nIt will make sense to use the seq.Date function to generate a time sequence with daily frequency or lower (for example, weekly, monthly, and so on) and as.POSIXt in other instances (for higher frequencies than daily, such as hourly, half-hourly, or by minutes).\n\ndaily_index &lt;- seq.Date(from = as.Date(\"2016-01-01\"),\n                        to = as.Date(\"2018-12-31\"), \n                        by = \"day\")\nhead(daily_index)\n\n[1] \"2016-01-01\" \"2016-01-02\" \"2016-01-03\" \"2016-01-04\" \"2016-01-05\"\n[6] \"2016-01-06\"\n\ndaily_3_index &lt;- seq.Date(from = as.Date(\"2016-01-01\"),\n                          to = as.Date(\"2018-12-31\"),\n                          by = \"3 days\")\nhead(daily_3_index)\n\n[1] \"2016-01-01\" \"2016-01-04\" \"2016-01-07\" \"2016-01-10\" \"2016-01-13\"\n[6] \"2016-01-16\"\n\n\nFor example, let’s create an hourly sequence with a length of 48 hours, using the seq.POSIXt function:\n\nhourly_index &lt;- seq.POSIXt(from = as.POSIXct(\"2018-06-01\"), \n                           by = \"hours\",\n                           length.out = 48)\nstr(hourly_index)\n\n POSIXct[1:48], format: \"2018-06-01 00:00:00\" \"2018-06-01 01:00:00\" \"2018-06-01 02:00:00\" ..."
  },
  {
    "objectID": "manipu_date.html#lubridate-package",
    "href": "manipu_date.html#lubridate-package",
    "title": "11  Date Manipulation",
    "section": "11.7 lubridate Package",
    "text": "11.7 lubridate Package\nThe lubridate package offers alternative tools and applications for reformatting, converting, and handling date and time objects.\n\n11.7.1 Reformatting date and time objects\nTo see how simple it is to reformat date and time objects with the lubridate package,let’s go back to the complex time object (Monday, December 31, 2018 11:59:59 PM) we converted earlier to a POSIXct class:\nExample:\n\ntime_US_str &lt;- \"Monday, December 31, 2018 11:59:59 PM\"\nclass(time_US_str)\n\n[1] \"character\"\n\n\nNow let’s use the ymd_hms (which stands for a year, month, day, hour, minute, and second) family of conversion functions from the lubridate package to convert the object to a POSIXct object:\nExample:\n\nlibrary(lubridate)\ntime_lubridate &lt;- mdy_hms(time_US_str, tz = \"EST\")\nclass(time_lubridate)\n\n[1] \"POSIXct\" \"POSIXt\" \n\ntime_lubridate\n\n[1] \"2018-12-31 23:59:59 EST\"\n\n\nThis is much simpler than the as.POSIXct conversion. The ymd_hms function is able to automatically map the different time components of the input object, even when some components are redundant (such as the full weekday name in the example previously).\nThe conversion with ymd functions of the previous date_df example (https://github.com/PacktPublishing/Hands-On-Time-Series-Analysis-with-R/raw/refs/heads/master/Chapter02/dates_formats.csv) is as follows:\nExample:\ndates_df$Japanese_format_new &lt;- ymd(dates_df$Japanese_format)\ndates_df$US_format_new &lt;- mdy(dates_df$US_format)\ndates_df$US_long_format_new &lt;- mdy(dates_df$US_long_format)\ndates_df$CA_mix_format_new &lt;- mdy(dates_df$CA_mix_format)\ndates_df$SA_mix_format_new &lt;- dmy(dates_df$SA_mix_format)\ndates_df$NZ_format_new &lt;- dmy(dates_df$NZ_format)\nThe ymd function easily handles the different types of date formats; however, it is not designed to convert numeric values of date objects. This type of conversion can be done with the as_date function for date objects (or as_datetime for time objects), which works in the same manner as as.Date (or the as.POSIXct/as.POSIXlt functions) works with date numeric values:\nExample:\ndates_df$Excel_Numeric_Format_new &lt;- as_date(dates_df$Excel_Numeric_Format,\n        origin = ymd(\"1899-12-30\"))\nNote that you will get the same results with the ymd and the as_date functions as the results with as.Date.\nHowever, those functions (both ymd and ymd_hms) are not able to handle rare or extreme cases where one of the separators is a double string (such as a double apostrophe, ““) or some of the formats are not written well in a specific format of year, month, and day, such as 201811-01, 201811-1, or 111-2018 (as opposed to 2018-11-01 or 01-11-2018).\n\n\n11.7.2 Extraction functionality\n\nThis extracts the day of the year:\nyday(time_obj)\nThis extracts the day of the quarter:\nqday(time_obj)\nThis extracts the day of the month:\nday(time_obj)\nThis extracts the day of the week:\nwday(time_obj, label = TRUE)\n\n\n\n11.7.3 Modification functionality\n\nThis modifies the hour of the time object:\nhour(time_obj) &lt;- 11\nThis rounds the time object by the minute, hour, and day:\n\n# Origin: 2018-12-31 23:59:59 CET\n\nround_date(as.POSIXct(\"2018-12-31 23:59:59 CET\"), unit = \"minute\")\n\n[1] \"2019-01-01 CET\"\n\nfloor_date(as.POSIXct(\"2018-12-31 23:59:59 CET\"), unit = \"hour\")\n\n[1] \"2018-12-31 23:00:00 CET\"\n\nceiling_date(as.POSIXct(\"2018-12-31 23:59:59 CET\"), unit = \"day\")\n\n[1] \"2019-01-01 CET\"\n\n\n\nThose sets of functions are very useful when extracting mainly time objects from a different type of format."
  },
  {
    "objectID": "manipu_text.html#sources-of-text-data",
    "href": "manipu_text.html#sources-of-text-data",
    "title": "12  Text Manipulation",
    "section": "12.1 Sources of Text Data",
    "text": "12.1 Sources of Text Data\nText data can be found on tweets from any individual, or from any company, Facebook status updates, RSS feeds from any news site, Blog articles, Journal articles, Newspapers, Verbatim transcripts of an in-depth interview.\nFor example, t extract Twitter data, we can use tweetR() and, to extract data from Facebook, we could use facebookR()."
  },
  {
    "objectID": "manipu_text.html#getting-text-data",
    "href": "manipu_text.html#getting-text-data",
    "title": "12  Text Manipulation",
    "section": "12.2 Getting Text Data",
    "text": "12.2 Getting Text Data\nThe easiest way to get text data is to import from a .csv file where some of the variables contain character data. We have to protect automatic factor conversion by specifying the stringsAsFactors = FALSE argument\nExample 1: The tweets.txt file is the plain text file. We will import this file using the generic readLines() function. It is a vector of characters (not a data.frame).\nExample 2: Html (this is also a character string):\nconURL &lt;- \"http://en.wikipedia.org/wiki/R_%28programming_language%29\"\n# Establish the connection with the URL \nlink2URL &lt;- url(conURL) \n# Reading html code\nhtmlCode &lt;- readLines(link2URL)\n# Closing the connection\nclose(link2URL)\n# Printing the result \nhtmlCode\nThe tm text mining library has some other functions to import text data from various files such as PDF files, plain text files, and even from doc files."
  },
  {
    "objectID": "manipu_databases.html#database-basics",
    "href": "manipu_databases.html#database-basics",
    "title": "13  R and Data Bases",
    "section": "13.1 Database Basics",
    "text": "13.1 Database Basics\nAt the simplest level, you can think about a database as a collection of data frames, called tables in database terminology. Like a data frame, a database table is a collection of named columns, where every value in the column is the same type. There are three high level differences between data frames and database tables:\n\nR stores everything in RAM, and a typical personal computer consists of limited RAM. R is RAM intensive, and for that reason, the size of a dataset should be much smaller than its RAM. Database tables are stored on disk and can be arbitrarily large.\nDatabase tables almost always have indexes. Much like the index of a book, a database index makes it possible to quickly find rows of interest without having to look at every single row. Data frames and tibbles don’t have indexes, but data.tables do, which is one of the reasons that they’re so fast.\nMost classical databases are optimized for rapidly collecting data, not analyzing existing data. These databases are called row-oriented because the data is stored row-by-row, rather than column-by-column like R. More recently, there’s been much development of column-oriented databases that make analyzing the existing data much faster.\n\nDatabases are run by database management systems (DBMS’s for short), which come in three basic forms:\n\nClient-server DBMS’s run on a powerful central server, which you connect from your computer (the client). They are great for sharing data with multiple people in an organization. Popular client-server DBMS’s include PostgreSQL, MariaDB, SQL Server, and Oracle.\nCloud DBMS’s, like Snowflake, Amazon’s RedShift, and Google’s BigQuery, are similar to client server DBMS’s, but they run in the cloud. This means that they can easily handle extremely large datasets and can automatically provide more compute resources as needed.\nIn-process DBMS’s, like SQLite or duckdb, run entirely on your computer. They’re great for working with large datasets where you’re the primary user."
  },
  {
    "objectID": "manipu_databases.html#connecting-to-a-database",
    "href": "manipu_databases.html#connecting-to-a-database",
    "title": "13  R and Data Bases",
    "section": "13.2 Connecting to a Database",
    "text": "13.2 Connecting to a Database\nTo connect to the database from R, you’ll use a pair of packages:\n\nYou’ll always use DBI (database interface) because it provides a set of generic functions that connect to the database, upload data, run SQL queries, etc.\nYou’ll also use a package tailored for the DBMS you’re connecting to. This package translates the generic DBI commands into the specifics needed for a given DBMS. There’s usually one package for each DBMS, e.g. RPostgres for PostgreSQL and RMariaDB for MySQL.\n\nIf you can’t find a specific package for your DBMS, you can usually use the odbc package instead. This uses the ODBC protocol supported by many DBMS. odbc requires a little more setup because you’ll also need to install an ODBC driver and tell the odbc package where to find it.\nYou can create a database connection using DBI::dbConnect(). The first argument selects the DBMS, then the second and subsequent arguments describe how to connect to it (i.e. where it lives and the credentials that you need to access it). The following code shows a couple of typical examples:\nExample:\ncon &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(), \n  username = \"foo\"\n)\ncon &lt;- DBI::dbConnect(\n  RPostgres::Postgres(), \n  hostname = \"databases.mycompany.com\", \n  port = 1234\n)\n\n13.2.1 In-process DBMS\nSetting up a client-server or cloud DBMS would be a pain for this book, so we’ll instead use an in-process DBMS that lives entirely in an R package: duckdb. The only difference between using duckdb and any other DBMS is how you’ll connect to the database.\nduckdb is a high-performance database that’s designed very much for the needs of a data scientist. We use it here because it’s very easy to get started with, but it’s also capable of handling gigabytes of data with great speed. If you want to use duckdb for a real data analysis project, you’ll also need to supply the dbdir argument to make a persistent database and tell duckdb where to save it.\nConnecting to duckdb is particularly simple because the defaults create a temporary database that is deleted when you quit R. That’s great for learning because it guarantees that you’ll start from a clean slate every time you restart R:\ncon &lt;- DBI::dbConnect(duckdb::duckdb(), dbdir = \"duckdb\")\nSince this is a new database, we need to start by adding some data. Here we’ll add mpg and diamonds datasets from ggplot2 using DBI::dbWriteTable(). The simplest usage of dbWriteTable() needs three arguments: a database connection, the name of the table to create in the database, and a data frame of data.\nExample:\ndbWriteTable(con, \"mpg\", ggplot2::mpg)\ndbWriteTable(con, \"diamonds\", ggplot2::diamonds)\nYou can check that the data is loaded correctly by using a couple of other DBI functions: dbListTables() lists all tables in the database and dbReadTable() retrieves the contents of a table.\nExample:\ndbListTables(con)\n\ncon |&gt; \n  dbReadTable(\"diamonds\") |&gt; \n  as_tibble()\ndbReadTable() returns a data.frame so we use as_tibble() to convert it into a tibble so that it prints nicely.\nNow you can use (SQL language) dbGetQuery() to get the results of running a query on the database:\nExample:\nsql &lt;- \"\n  SELECT carat, cut, clarity, color, price \n  FROM diamonds \n  WHERE price &gt; 15000\n\"\nas_tibble(dbGetQuery(con, sql))\n\n\n13.2.2 Excel/MSAccess\nAn Excel file can be imported into R using ODBC. Remember Excel cannot deal with relational databases.\nWe will now create an ODBC connection with an MS Excel file with the connection string xlopen:\n\nIn our computer: To use the ODBC approach on an Excel file, we firstly need to create the connection string using the system administrator. We need to open the control panel of the operating system and then open Administrative Tools and then choose ODBC. A dialog box will now appear. Click on the Add button and select an appropriate ODBC driver and then locate the desired file and give a data source name. In our case, the data source name is xlopen.\nIn R:\n# calling ODBC library into R \nlibrary(RODBC)\n# creating connection with the database using odbc package. \nxldb &lt;- odbcConnect(\"xlopen\") / odbcConnect(\"accessdata\")\n# Now that the connection is created, we will use this connection and import the data xldata&lt;- sqlFetch(xldb, \"CSVanscombe\")\n\n\n\n13.2.3 Relational databases in R\n\nThere are packages to interface between R and different database software packages that use relational database management systems, such as MySQL (RMySQL), PostgreSQL (RPgSQL), and Oracle (ROracle).\nOne of the most popular packages is RMySQL. This package allows us to make connections between R and the MySQL server. In order to install this package properly, we need to download both the MySQL server and RMySQL.\nThere are several R packages available that allow direct interactions with large datasets within R, such as filehash, ff, and bigmemory. The idea is to avoid loading the whole dataset into memory."
  },
  {
    "objectID": "manipu_databases.html#dbplyr",
    "href": "manipu_databases.html#dbplyr",
    "title": "13  R and Data Bases",
    "section": "13.3 dbplyr",
    "text": "13.3 dbplyr\ndbplyr is a dplyr backend, which means that you keep writing dplyr code but the backend executes it differently. In this, dbplyr translates to SQL; other backends include dtplyr which translates to data.table, and multidplyr which executes your code on multiple cores.\nTo use dbplyr, you must first use tbl() to create an object that represents a database table:\nExample:\ndiamonds_db &lt;- tbl(con, \"diamonds\")\ndiamonds_db\nbig_diamonds_db &lt;- diamonds_db |&gt; \n  filter(price &gt; 15000) |&gt; \n  select(carat:clarity, price)\nbig_diamonds_db"
  },
  {
    "objectID": "manipu_databases.html#packages",
    "href": "manipu_databases.html#packages",
    "title": "13  R and Data Bases",
    "section": "13.4 Packages",
    "text": "13.4 Packages\n\n13.4.1 filehash package\nIt is used for solving large-data problems. The idea behind the development of this package was to avoid loading the dataset into a computer’s virtual memory. Instead, we dump the large dataset into the hard drive and then assign an environment name for the dumped objects.\nlibrary(filehash) \ndbCreate(\"exampledb\")\nfilehash_db&lt;- dbInit(\"exampledb\")  ## db needs to be initialized before accessing\ndbInsert(filehash_db, \"xx\", rnorm(50)) \nvalue&lt;- dbFetch(filehash_db, \"xx\")  ## to retrieve db values\nsummary(value)\nThis file connection will remain open until the database is closed via dbDisconnect or the database object in R is removed.\n\n\n13.4.2 ff package\nThis package extends the R system and stores data in the form of native binary flat files in persistent storage such as hard disks, CDs, or DVDs rather than in the RAM.\nThis package enables users to work on several large datasets simultaneously. It also allows the allocation of vectors or arrays that are larger than the RAM.\n\n\n13.4.3 sqldf package\nThe sqldf package is an R package that allows users to run SQL statements within R.\nWe can perform any type of data manipulation to an R data frame either in memory or during import.\nIf the dataset is too large and cannot entirely be read into the R environment, we can import a portion of that dataset using sqldf."
  },
  {
    "objectID": "stats_descriptive.html#summary-and-str",
    "href": "stats_descriptive.html#summary-and-str",
    "title": "14  Descriptive Statistics",
    "section": "14.1 summary() and str()",
    "text": "14.1 summary() and str()\nThe summary() and str() functions are the fastest ways to get descriptive statistics of the data.\n\nThe summary() function gives the basic descriptive statistics of the data.\nThe str() function gives the structure of the variables."
  },
  {
    "objectID": "stats_descriptive.html#measures-of-centrality",
    "href": "stats_descriptive.html#measures-of-centrality",
    "title": "14  Descriptive Statistics",
    "section": "14.2 Measures of Centrality",
    "text": "14.2 Measures of Centrality\n\n14.2.1 Mode\nThe mode is a value in data that has the highest frequency:\nExample:\n\na &lt;- c(1, 2, 3, 4, 5, 5, 5, 6, 7, 8)\n# To get mode in a vector you create a frequency table\n(y &lt;- table(a)) \n\na\n1 2 3 4 5 6 7 8 \n1 1 1 1 3 1 1 1 \n\nnames(y)[which(y==max(y))]\n\n[1] \"5\"\n\n\n\n\n14.2.2 Median\nThe median is the middle or midpoint of the data and is also the 50 percentile of the data.\nThe median is affected by the outliers and skewness of the data.\n\nmedian(a)\n\n[1] 5\n\n\n\n\n14.2.3 Mean\nThe mean is the average of the data. The mean works best if the data is distributed in a normal distribution or distributed evenly.\n\nmean(a)\n\n[1] 4.6"
  },
  {
    "objectID": "stats_descriptive.html#measures-of-variability",
    "href": "stats_descriptive.html#measures-of-variability",
    "title": "14  Descriptive Statistics",
    "section": "14.3 Measures of Variability",
    "text": "14.3 Measures of Variability\nThe measures of variability are the measures of the spread of the data. These are encompasses:\n\nVariance.\nStandard deviation.\nRange.\nInterquartile range.\nand more.\n\n\n14.3.1 Variance\nThe variance is the average of squared differences from the mean, and it is used to measure the spreadness of the data:\n\nPopulation variance:\n\n\nA &lt;- c(1, 2, 3, 4, 5, 5, 5, 6, 7, 8)\nN &lt;- length(A)\nvar(A) * (N - 1) / N\n\n[1] 4.24\n\n\n\nSample variance:\n\n\nvar(A)\n\n[1] 4.711111\n\n\n\n\n14.3.2 Standard deviation\nThe standard deviation is the square root of a variance and it measures the spread of the data.\n\nPopulation standard deviation:\n\n\nA &lt;- c(1, 2, 3, 4, 5, 5, 5, 6, 7, 8)\nN &lt;- length(A)\nvariance &lt;- var(A) * (N - 1) / N\nsqrt(variance)\n\n[1] 2.059126\n\n\n\nSample standard deviation:\n\n\nsd(A)\n\n[1] 2.170509\n\n\n\n\n14.3.3 range()\nThe range is the difference between the largest and smallest points in the data:\n\nrange(A)    \n\n[1] 1 8\n\nres &lt;- range(A)\ndiff(res)\n\n[1] 7\n\nmin(A)\n\n[1] 1\n\nmax(A)\n\n[1] 8\n\n\n\n\n14.3.4 Interquartile Range\nThe interquartile range is the measure of the difference between the 75 percentile or third quartile and the 25 percentile or first quartile.\n\nIQR(A)\n\n[1] 2.5\n\n\nYou can get the quartiles by using the quantile() function:\n\nquantile(A)\n\n  0%  25%  50%  75% 100% \n1.00 3.25 5.00 5.75 8.00"
  },
  {
    "objectID": "stats_descriptive.html#distributions",
    "href": "stats_descriptive.html#distributions",
    "title": "14  Descriptive Statistics",
    "section": "14.4 Distributions",
    "text": "14.4 Distributions\n\n14.4.1 Normal Distribution\nIf the points do not deviate away from the line, the data is normally distributed.\n\n\n\nFigure 1: The normal distribution\n\n\nTo see whether data is normally distributed, you can use the qqnorm() and qqline() functions:\nqqnorm(data$x) #You must first draw the distribution to draw the line afterwards \nqqline(data$x)\nYou can also use a Shapiro Test to test whether the data is normally distributed. If the p-value is more than 0.05, you can conclude that the data does not deviate from normal distribution:\nshapiro.test(data$x)\n\n\n14.4.2 Modality\nThe modality of a distribution can be seen by the number of peaks when we plot the histogram.\n\n\n\nFigure 2: The modality of a distribution\n\n\n\n\n14.4.3 Skewness\nSkewness is a measure of how symmetric a distribution is and how much the distribution is different from the normal distribution.\nNegative skew is also known as left skewed, and positive skew is also known as right skewed: - A positive skewness indicates that the size of the right-handed tail is larger than the left-handed tail. - A negative skewness indicates that the left-hand tail will typically be longer than the right-hand tail.\n\n\n\nFigure 3: Skewness of a distribution\n\n\nThe Pearson’s Kurtosis measure is used to see whether a dataset is heavy tailed, or light tailed. High kurtosis means heavy tailed, so there are more outliers in the data.\n\nWhen kurtosis is close to 0, then a normal distribution is often assumed. These are called mesokurtic distributions.\n\nWhen kurtosis&gt;0, then the distribution has heavier tails and is called a leptokurtic distribution.\nWhen kurtosis&lt;0, then the distribution is light tails and is called a platykurtic distribution.\n\nTo find the kurtosis and skewness in R, you must install the moments package:\ninstall.packages(\"moments\")\nskewness(data$x)\nkurtosis(data$x)\n\n\n14.4.4 Binomial Distribution\nA binomial distribution has two outcomes, success or failure, and can be thought of as the probability of success or failure in a survey that is repeated various times.\n\ndbinom(32, 100, 0.5)\n\n[1] 0.000112817"
  },
  {
    "objectID": "stats_exploratory.html#formulate-questions",
    "href": "stats_exploratory.html#formulate-questions",
    "title": "15  Exploratory Analysis Pipeline",
    "section": "15.1 Formulate Questions",
    "text": "15.1 Formulate Questions\nA sharp question or hypothesis can serve as a dimension reduction tool that can eliminate variables that are not immediately relevant to the question.\nIt’s usually a good idea to spend a few minutes to figure out what is the question you’re really interested in and narrow it down to be as specific as possible (without becoming uninteresting). One of the most important questions you can answer with an exploratory data analysis is: “Do I have the right data to answer this question?”\nThere is no universal rule about which questions you should ask to guide your research. However, two types of questions will always be useful for making discoveries within your data:\n\nWhat type of variation occurs within my variables?\nWhat type of covariation occurs between my variables?"
  },
  {
    "objectID": "stats_exploratory.html#read-in-your-data",
    "href": "stats_exploratory.html#read-in-your-data",
    "title": "15  Exploratory Analysis Pipeline",
    "section": "15.2 Read In Your Data",
    "text": "15.2 Read In Your Data\nSometimes the data will come in a very messy format, and you’ll need to do some cleaning and transformations.\nThe readr package is a nice package for reading in flat files very fast, or at least much faster than R’s built-in functions"
  },
  {
    "objectID": "stats_exploratory.html#check-the-packaging",
    "href": "stats_exploratory.html#check-the-packaging",
    "title": "15  Exploratory Analysis Pipeline",
    "section": "15.3 Check the “Packaging”",
    "text": "15.3 Check the “Packaging”\nAssuming you don’t get any warnings or errors when reading in the dataset, you should now have an object in your workspace, e.g. named “ozone”. It’s usually a good idea to poke at that object a little bit before we break open the wrapping paper. For example, you can check the number of rows and columns."
  },
  {
    "objectID": "stats_exploratory.html#run-str",
    "href": "stats_exploratory.html#run-str",
    "title": "15  Exploratory Analysis Pipeline",
    "section": "15.4 Run str()",
    "text": "15.4 Run str()\nThis is usually a safe operation in the sense that even with a very large dataset, running str() shouldn’t take too long.\nYou can examine the classes of each of the columns to make sure they are correctly specified (i.e., numbers are numeric, and strings are character, etc.)."
  },
  {
    "objectID": "stats_exploratory.html#look-top-and-bottom-of-your-data",
    "href": "stats_exploratory.html#look-top-and-bottom-of-your-data",
    "title": "15  Exploratory Analysis Pipeline",
    "section": "15.5 Look Top and Bottom of Your Data",
    "text": "15.5 Look Top and Bottom of Your Data\nThis lets me know if the data were read in properly, things are properly formatted, and that everything is there. If your data are time series data, then make sure the dates at the beginning and end of the dataset match what you expect the beginning and ending time period are.\nYou can peek at the top and bottom of the data with the head() and tail() functions. Sometimes there’s weird formatting at the end or some extra comment lines that someone decided to stick at the end."
  },
  {
    "objectID": "stats_exploratory.html#check-ns-frequency",
    "href": "stats_exploratory.html#check-ns-frequency",
    "title": "15  Exploratory Analysis Pipeline",
    "section": "15.6 Check “n”s (frequency)",
    "text": "15.6 Check “n”s (frequency)\nTo do this properly, you need to identify some landmarks that can be used to check against your data. For example, if you are collecting data on people, such as in a survey or clinical trial, then you should know how many people there are in your study (i.e., in an ozone monitoring data system we can take a look at the Time.Local variable to see what time measurements are recorded as being taken.)\ntable(ozone$Time.Local)"
  },
  {
    "objectID": "stats_exploratory.html#validate-with-external-data-source",
    "href": "stats_exploratory.html#validate-with-external-data-source",
    "title": "15  Exploratory Analysis Pipeline",
    "section": "15.7 Validate With External Data Source",
    "text": "15.7 Validate With External Data Source\nExternal validation can often be as simple as checking your data against a single number.Is the data are at least of the right order of magnitude (i.e., the units are correct)? or, is the range of the distribution roughly what we’d expect, given the regulation around ambient pollution levels?"
  },
  {
    "objectID": "stats_exploratory.html#check-the-variation-of-data",
    "href": "stats_exploratory.html#check-the-variation-of-data",
    "title": "15  Exploratory Analysis Pipeline",
    "section": "15.8 Check the Variation of Data",
    "text": "15.8 Check the Variation of Data\nVariation is the tendency of the values of a variable to change from measurement to measurement.\nEvery variable has its own pattern of variation, which can reveal interesting information about how that it varies between measurements on the same observation as well as across observations. The best way to understand a pattern is to visualize the distribution of the variable’s values.\nExample:\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n# Visualizing the carat from the diamonds dataset\nggplot(diamonds, aes(x = carat)) +\n  geom_histogram(binwidth = 0.5)\n\n\n\n\nNow that you can visualize variation, what should you look for in your plot? And what type of follow-up questions should you ask?\nThis histogram suggests several interesting questions:\n\nWhy are there more diamonds at whole carats and common fractions of carats?\nWhy are there more diamonds slightly to the right of each peak than there are slightly to the left of each peak?\n\nVisualizations can also reveal clusters, which suggest that subgroups exist in your data. To understand the subgroups, ask:\n\nHow are the observations within each subgroup similar to each other?\nHow are the observations in separate clusters different from each other?\nHow can you explain or describe the clusters?\n\n\n15.8.1 Typical values\nIn both bar charts and histograms, tall bars show the common values of a variable, and shorter bars show less-common values. Places that do not have bars reveal values that were not seen in your data. Now you can turn this information into useful questions:\n\nWhich values are the most common? Why?\nWhich values are rare? Why? Does that match your expectations?\nCan you see any unusual patterns? What might explain them?\n\nLet’s take a look at the distribution of carat for smaller diamonds.\nExample:\n\nsmaller &lt;- diamonds |&gt; \n              filter(carat &lt; 3)\nggplot(smaller, aes(x = carat)) +\n  geom_histogram(binwidth = 0.01)\n\n\n\n\nThis histogram suggests several interesting questions:\n\nWhy are there more diamonds at whole carats and common fractions of carats?\nWhy are there more diamonds slightly to the right of each peak than there are slightly to the left of each peak?\n\nVisualizations can also reveal clusters, which suggest that subgroups exist in your data. To understand the subgroups, ask:\n\nHow can you explain or describe the clusters?\nHow are the observations within each subgroup similar to each other?\nHow are the observations in separate clusters different from each other?\n\n\n\n15.8.2 Unusual values\nOutliers are observations that are unusual; data points that don’t seem to fit the pattern. Sometimes outliers are data entry errors, sometimes they are simply values at the extremes that happened to be observed in this data collection, and other times they suggest important new discoveries.\nWhen you have a lot of data, outliers are sometimes difficult to see in a histogram. For example, take the distribution of the y variable from the diamonds dataset. The only evidence of outliers is the unusually wide limits on the x-axis.\nExample:\n\nggplot(diamonds, aes(x = y)) + \n  geom_histogram(binwidth = 0.5) \n\n\n\n\nTo make it easy to see unusual values:\ncoord_cartesian(ylim = c(0, 50))\nIf you’ve encountered unusual values in your dataset, and simply want to move on to the rest of your analysis, you have two options.\n\nDrop the entire row with the strange values:\nExample:\n\ndiamonds2 &lt;- diamonds |&gt; \n  filter(between(y, 3, 20))\n\nThis is not recommended because one invalid value doesn’t imply that all the other values for that observation are also invalid.\nInstead, we recommend replacing the unusual values with missing values. The easiest way to do this is to use mutate() to replace the variable with a modified copy.\nExample:\n\ndiamonds2 &lt;- diamonds |&gt; \n    mutate(y = if_else(y &lt; 3 | y &gt; 20, NA, y))\nggplot(diamonds2, aes(x = x, y = y)) + \n    geom_point()\n\nWarning: Removed 9 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\nTo suppress that warning, set na.rm = TRUE."
  },
  {
    "objectID": "stats_exploratory.html#check-the-covariation",
    "href": "stats_exploratory.html#check-the-covariation",
    "title": "15  Exploratory Analysis Pipeline",
    "section": "15.9 Check the Covariation",
    "text": "15.9 Check the Covariation\nIf variation describes the behavior within a variable, covariation describes the behavior between variables. Covariation is the tendency for the values of two or more variables to vary together in a related way.\nThe best way to spot covariation is to visualize the relationship between two or more variables.\n\n15.9.1 Categorical and numerical variable\nFor example, let’s explore how the price of a diamond varies with its quality (measured by cut) using geom_freqpoly():\nExample:\n\nggplot(diamonds, aes(x = price)) + \n  geom_freqpoly(aes(color = cut), binwidth = 500, linewidth = 0.75)\n\n\n\n\nNote that ggplot2 uses an ordered color scale for cut because it’s defined as an ordered factor variable in the data.\nTo make the comparison easier we need to swap what is displayed on the y-axis. Instead of displaying count, we’ll display the density, which is the count standardized so that the area under each frequency polygon is one.\nExample:\n\nggplot(diamonds, aes(x = price, y = after_stat(density))) + \n  geom_freqpoly(aes(color = cut), binwidth = 500, linewidth = 0.75)\n\n\n\n\nThere’s something rather surprising about this plot: it seems that fair diamonds (the lowest quality) have the highest average price!\n\n\n15.9.2 Two categorical variables\nTo visualize the covariation between categorical variables, you’ll need to count the number of observations for each combination of levels of these categorical variables. One way to do that is to rely on the built-in geom_count():\n\nggplot(diamonds, aes(x = cut, y = color)) +\n  geom_count()\n\n\n\n\nThe size of each circle in the plot displays how many observations occurred at each combination of values. Covariation will appear as a strong correlation between specific x values and specific y values.\nAnother approach for exploring the relationship between these variables is computing the counts with dplyr:\n\ndiamonds |&gt; \n  count(color, cut)\n\n# A tibble: 35 × 3\n   color cut           n\n   &lt;ord&gt; &lt;ord&gt;     &lt;int&gt;\n 1 D     Fair        163\n 2 D     Good        662\n 3 D     Very Good  1513\n 4 D     Premium    1603\n 5 D     Ideal      2834\n 6 E     Fair        224\n 7 E     Good        933\n 8 E     Very Good  2400\n 9 E     Premium    2337\n10 E     Ideal      3903\n# ℹ 25 more rows\n\n\nThen visualize with geom_tile() and the fill aesthetic:\n\ndiamonds |&gt; \n  count(color, cut) |&gt;  \n  ggplot(aes(x = color, y = cut)) +\n  geom_tile(aes(fill = n))\n\n\n\n\nFor larger plots, you might want to try the heatmaply package, which creates interactive plots.\n\n\n15.9.3 Two numerical variables\nDrawing a scatterplot with geom_point() is one great way to visualize the covariation between two numerical variables. For example, you can see a positive relationship between the carat size and price of a diamond (diamonds with more carats have a higher price):\nExample:\n\nggplot(smaller, aes(x = carat, y = price)) +\n  geom_point()\n\n\n\n\nScatterplots become less useful as the size of your dataset grows, because points begin to overplot, and pile up into areas of uniform black, making it hard to judge differences in the density of the data across the 2-dimensional space as well as making it hard to spot the trend.\nTwo possible solutions are:\n\nUsing transparency: R     geom_point(alpha = 1 / 100)\nAnother solution is to use bin:\n\nExample:\n\n# geom_bin2d() creates rectangular bins\nggplot(smaller, aes(x = carat, y = price)) +\n  geom_bin2d()\n\n\n\n\n# geom_hex() creates hexagonal bins\n# You will need to install.packages(\"hexbin\")\nggplot(smaller, aes(x = carat, y = price)) +\n  geom_hex()"
  },
  {
    "objectID": "stats_exploratory.html#challenge-your-solutionbootstrap-sample",
    "href": "stats_exploratory.html#challenge-your-solutionbootstrap-sample",
    "title": "15  Exploratory Analysis Pipeline",
    "section": "15.10 Challenge Your Solution/Bootstrap Sample",
    "text": "15.10 Challenge Your Solution/Bootstrap Sample\nThe easy solution is generally nice because it is easy, but you should never allow those results to hold the day. You should always be thinking of ways to challenge the results, especially if those results collide with your prior expectation.\nFor example, How stable are the ozone rankings from year to year? We can imagine that from year to year, the ozone data are somewhat different randomly, but generally follow similar patterns across the country. So, the shuffling process could approximate the data changing from one year to the next. This not an ideal solution, but it could give us a sense of how stable the rankings are:\n\nFirst, we set our random number generator and resample the indices of the rows of the data frame with replacement. The statistical jargon for this approach is a bootstrap sample:\nWe use the resampled indices to create a new dataset, ozone2, that shares many of the same qualities as the original but is randomly perturbed.\nset.seed(10234) \nN &lt;- nrow(ozone) \nidx &lt;- sample(N, N, replace = TRUE) \nozone2 &lt;- ozone[idx, ]\nWe reconstruct our rankings of the counties based on this resampled data:\nranking2 &lt;- group_by(ozone2, State.Name, County.Name) %&gt;% \n  summarize(ozone = mean(Sample.Measurement)) %&gt;%\n  as.data.frame %&gt;%\n  arrange(desc(ozone))\nWe can then compare the top 10 counties from our original ranking and the top 10 counties from our ranking based on the resampled data.\ncbind(head(ranking, 10)\nhead(ranking2, 10))\nWe can see that the rankings based on the resampled data are very close to the original, with the first 7 being identical. Numbers 8 and 9 get flipped in the resampled rankings but that’s about it. This might suggest that the original rankings are somewhat stable."
  },
  {
    "objectID": "stats_exploratory.html#follow-up",
    "href": "stats_exploratory.html#follow-up",
    "title": "15  Exploratory Analysis Pipeline",
    "section": "15.11 Follow Up",
    "text": "15.11 Follow Up\nAt this point it’s useful to consider a few follow up questions:\n\nDo you have the right data?\nDo you need other data?\nDo you have the right question?\n\nThe goal of exploratory data analysis is to get you thinking about your data and reasoning about your question. At this point, we can refine our question or collect new data, all in an iterative process to get at the truth."
  },
  {
    "objectID": "stats_inferential.html#correlation",
    "href": "stats_inferential.html#correlation",
    "title": "16  Inferential Statistics",
    "section": "16.1 Correlation",
    "text": "16.1 Correlation\nCorrelations are statistical associations to find how close two variables are and to derive the linear relationships between them.\nYou can use correlation to find which variables are more related to the target variable and use this to reduce the number of variables.\nCorrelation does not mean a causal relationship, it does not tell you the how and why of the relationship.\ncor(data$var1, data$var2)\nThe correlation has a range from -1.0 to 1.0."
  },
  {
    "objectID": "stats_inferential.html#covariance",
    "href": "stats_inferential.html#covariance",
    "title": "16  Inferential Statistics",
    "section": "16.2 Covariance",
    "text": "16.2 Covariance\nCovariance is a measure of variability between two variables.\nThe greater the value of one variable and the greater of other variable means it will result in a covariance that is positive.\ncov(data$var1, data$var2)\nCovariance does not have a range. When two variables are independent of each other, the covariance is zero."
  },
  {
    "objectID": "stats_inferential.html#hypothesis-testing-and-p-value",
    "href": "stats_inferential.html#hypothesis-testing-and-p-value",
    "title": "16  Inferential Statistics",
    "section": "16.3 Hypothesis Testing and P-Value",
    "text": "16.3 Hypothesis Testing and P-Value\nBased on the research question, the hypothesis can be a null hypothesis, H0 (μ1= μ2) and an alternate hypothesis, Ha (μ1 ≠ μ2).\nFor data normally distributed:\n\np-value:\n\nA small p-value &lt;= alpha, which is usually 0.05, indicates that the observed data is sufficiently inconsistent with the null hypothesis, so the null hypothesis may be rejected. The alternate hypothesis is true at the 95% confidence interval.\nA larger p-value means that you failed to reject null hypothesis.\n\nt-test continuous variables of data.\nchi-square test for categorical variables or data.\nANOVA\n\nFor data not normally distributed:\n\nnon-parametric tests."
  },
  {
    "objectID": "stats_inferential.html#t-test",
    "href": "stats_inferential.html#t-test",
    "title": "16  Inferential Statistics",
    "section": "16.4 T-Test",
    "text": "16.4 T-Test\nA t-test is used to determine whether the mean between two data points or samples are equal to each other.\n\n\\(H_0\\) (\\(μ_1\\) = \\(μ_2\\)): The null hypothesis means that the two means are equal.\n\\(H_a\\) (\\(μ_1\\) ≠ \\(μ_2\\)): The alternative means that the two means are different.\n\nIn t-test there are two assumptions:\n\nThe population is normally distributed.\nThe samples are randomly sampled from their population.\n\nType I and Type II Errors:\n\nA Type I error is a rejection of the null hypothesis when it is really true.\nA Type II error is a failure to reject a null hypothesis that is false.\n\n\n16.4.1 One-Sample T-Test\nA one-sample t-test is used to test whether the mean of a population is equal to a specified mean.\nYou can use the t statistics and the degree of freedom (\\(df = n -1\\)) to estimate the p-value using a t-table.\nt.test(data$var1, mu=0.6) \n\n\n16.4.2 Two-Sample Independent T-Test (unpaired, paired = FALSE)\nThe two-sample unpaired t-test is when you compare two means of two independent samples. The degrees of freedom formula is \\(df = nA – nB – 2\\)\nIn the two-sample unpaired t-test, when the variance is unequal, you use the Welch t-test.\nt.test(data$var1, data\\$var2, var.equal=FALSE, paired=FALSE) \n\n\n16.4.3 Two-Sample Dependent T-Test (paired = TRUE)\nA two-sample paired t-test is used to test the mean of two samples that depend on each other. The degree of freedom formula is \\(df = n-1\\)\nt.test(data$var1, data$var2, paired=TRUE)"
  },
  {
    "objectID": "stats_inferential.html#chi-square-test",
    "href": "stats_inferential.html#chi-square-test",
    "title": "16  Inferential Statistics",
    "section": "16.5 Chi-Square Test",
    "text": "16.5 Chi-Square Test\nThe chi-square test is used to compare the relationships between two categorical variables.\nThe null hypothesis means that there is no relationship between the categorical variables.\n\n16.5.1 Goodness of Fit Test\nWhen you have only one categorical variable from a population and you want to compare whether the sample is consistent with a hypothesized distribution, you can use the goodness of fit test.\n\n\\(H_0\\): No significant difference between the observed and expected values.\n\\(H_A\\): There is a significant difference between the observed and expected values.\n\nTo use the goodness of fit chi-square test in R, you can use the chisq.test() function:\ndata &lt;- c(B=200, c=300, D=400)\nchisq.test(data)\n\n\n16.5.2 Contingency Test\nIf you have two categorical variables and you want to compare whether there is a relationship between two variables, you can use the contingency test.\n\n\\(H_0\\): the two categorical variables have no relationship. The two variables are independent.\n\\(H_A\\): the two categorical variables have a relationship. The two variables are not independent.\n\nvar1 &lt;- c(\"Male\", \"Female\", \"Male\", \"Female\", \"Male\")\nvar2 &lt;- c(\"chocolate\", \"strawberry\", \"strawberry\", \"strawberry\", \"chocolate\")\ndata &lt;- data.frame(var1, var2)\ndata.table &lt;- table(data$var1, data$var2)\ndata.table &gt; chisq.test(data.table)"
  },
  {
    "objectID": "stats_inferential.html#anova",
    "href": "stats_inferential.html#anova",
    "title": "16  Inferential Statistics",
    "section": "16.6 ANOVA",
    "text": "16.6 ANOVA\nANOVA is the process of testing the means of two or more groups. ANOVA also checks the impact of factors by comparing the means of different samples.\nIn ANOVA, you use two kinds of means:\n\nSample means.\nGrand mean (the mean of all of the samples’ means).\n\nHypothesis: - \\(H_0\\): \\(μ_1\\)= \\(μ_2\\) = … = \\(μ_L\\) ; the sample means are equal or do not have significant differences. - \\(H_A\\): \\(μ_1\\) ≠ \\(μ_m\\); is when the sample means are not equal.\nYou assume that the variables are sampled, independent, and selected or sampled from a population that is normally distributed with unknown but equal variances.\n\n16.6.1 Between Group Variability\nThe distribution of two samples, when they overlap, their means are not significantly different. Hence, the difference between their individual mean and the grand mean is not significantly different.\n\n\n\nFigure 6: Between group variability\n\n\nThis variability is called the between-group variability, which refers to the variations between the distributions of the groups or levels.\n\n\n16.6.2 Within Group Variability\nFor the following distributions of samples, as their variance increases, they overlap each other and become part of a population.\n\n\n\nFigure 7: Within group variability\n\n\nThe F-statistics are the measures if the means of samples are significantly different. The lower the F-statistics, the more the means are equal, so you cannot reject the null hypothesis.\n\n\n16.6.3 One-Way ANOVA\nOne-way ANOVA is used when you have only one independent variable.\n\nlibrary(graphics)\nset.seed(123) \nvar1 &lt;- rnorm(12, 2, 1) \nvar2 &lt;- c(\"B\", \"B\", \"B\", \"B\", \"C\", \"C\", \"C\", \"C\", \"C\", \"D\", \"D\", \"B\")\ndata &lt;- data.frame(var1, var2) \nfit &lt;- aov(data$var1 ~ data$var2, data = data)\nfit \n\nCall:\n   aov(formula = data$var1 ~ data$var2, data = data)\n\nTerms:\n                data$var2 Residuals\nSum of Squares   0.162695  9.255706\nDeg. of Freedom         2         9\n\nResidual standard error: 1.014106\nEstimated effects may be unbalanced\n\nsummary(fit)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\ndata$var2    2  0.163  0.0813   0.079  0.925\nResiduals    9  9.256  1.0284               \n\n\n\n\n16.6.4 Two-Way ANOVA\nTwo-way ANOVA is used when you have two independent variables. (continuar en el ejemplo anterior):\n\nvar3 &lt;- c(\"D\", \"D\", \"D\", \"D\", \"E\", \"E\", \"E\", \"E\", \"E\", \"F\", \"F\", \"F\")\ndata &lt;- data.frame(var1, var2, var3) \nfit &lt;- aov(data$var1 ~ data$var2 + data$var3, data=data)\nfit\n\nCall:\n   aov(formula = data$var1 ~ data$var2 + data$var3, data = data)\n\nTerms:\n                data$var2 data$var3 Residuals\nSum of Squares   0.162695  0.018042  9.237664\nDeg. of Freedom         2         1         8\n\nResidual standard error: 1.074573\n1 out of 5 effects not estimable\nEstimated effects may be unbalanced\n\nsummary(fit)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\ndata$var2    2  0.163  0.0813   0.070  0.933\ndata$var3    1  0.018  0.0180   0.016  0.904\nResiduals    8  9.238  1.1547               \n\n## var1 does not depend on var2's mean and var3's mean\n\n\n\n16.6.5 MANOVA\nThe multivariate analysis of variance is when there are multiple response variables that you want to test.\nExample:\n\nres &lt;- manova(cbind(iris$Sepal.Length, iris$Petal.Length) ~ iris$Species, data=iris) \nsummary(res)\n\n              Df Pillai approx F num Df den Df    Pr(&gt;F)    \niris$Species   2 0.9885   71.829      4    294 &lt; 2.2e-16 ***\nResiduals    147                                            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.aov(res)\n\n Response 1 :\n              Df Sum Sq Mean Sq F value    Pr(&gt;F)    \niris$Species   2 63.212  31.606  119.26 &lt; 2.2e-16 ***\nResiduals    147 38.956   0.265                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response 2 :\n              Df Sum Sq Mean Sq F value    Pr(&gt;F)    \niris$Species   2 437.10 218.551  1180.2 &lt; 2.2e-16 ***\nResiduals    147  27.22   0.185                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## Hence, you have two response variables, Sepal.Length and Petal.Length\n\nThe p-value is 2.2e-16, which is less than 0.05. Hence, you reject the null hypothesis"
  },
  {
    "objectID": "stats_inferential.html#nonparametric-test",
    "href": "stats_inferential.html#nonparametric-test",
    "title": "16  Inferential Statistics",
    "section": "16.7 Nonparametric Test",
    "text": "16.7 Nonparametric Test\nThe nonparametric test is a test that does not require the variable and sample to be normally distributed.\nYou use nonparametric tests when you do not have normally distributed data and the sample data is big.\n\nTable 3: Types of nonparametric tests\n\n\n\n\n\n\n\nNonparametric Test\nFunction\nMethod replaced\n\n\n\n\nWilcoxon Signed Rank Test\nwilcox.test(data[,1], mu=0, alternatives=\"two.sided\")\none-sample\n\n\nt-testundefined\n\n\n\n\nWilcoxon-Mann-Whitney Test\nwilcox.test(data[,1], data[,2], correct=FALSE)\nsubstitute\n\n\nto the two-sample t-test\n\n\n\n\nKruskal-Wallis Test\nkruskal.test(airquality$Ozone ~ airquality$Month)\none-way\n\n\n\n\n16.7.1 Wilcoxon Signed Rank Test\nThe Wilcoxon signed rank test is used to replace the one-sample t-test.\nHypothesis:\n\n\n\\(H_0\\): \\(μ_1\\)= \\(μ_o\\); the null hypothesis is that the population median has the specified value of \\(μ_0\\)\n\\(H_a\\): \\(μ_1\\) ≠ \\(μ_o\\)\n\nTo use the Wilcoxon signed rank test in R, you can first generate the data set using random.org packages, so that the variables are not normally distributed.\nExample:\ninstall.packages(\"random\") \nlibrary(random) \nvar1 &lt;- randomNumbers(n=100, min=1, max=1000, col=1)\nvar2 &lt;- randomNumbers(n=100, min=1, max=1000, col=1) \nvar3 &lt;- randomNumbers(n=100, min=1, max=1000, col=1) \ndata &lt;- data.frame(var1[,1], var2[,1], var3[,1]) \nwilcox.test(data[,1], mu=0, alternatives=\"two.sided\")\n\n\n16.7.2 Wilcoxon-Mann-Whitney Test\nThe Wilcoxon-Mann-Whitney test is a nonparametric test to compare two samples. It is a powerful substitute to the two-sample t-test.\nTo use the Wilcoxon-Matt-Whitney test (or the Wilcoxon rank sum test or the Mann-Whitney test) in R, you can use the wilcox.test() function:\nwilcox.test(data[,1], data[,2], correct=FALSE)\nThere are not significant differences in the median for first variable median and second variable median.\n\n\n16.7.3 Kruskal-Wallis Test\nThe Kruskal-Wallis test is a nonparametric test that is an extension of the Mann-Whitney U test for three or more samples.\nThe test requires samples to be identically distributed.\nKruskal-Wallis is an alternative to one-way ANOVA.\nThe Kruskal-Wallis test tests the differences between scores of k independent samples of unequal sizes with the ith sample containing li rows:\n\n\\(H_0\\): \\(μ_o\\) = \\(μ_1\\)= \\(μ_2\\) = … = \\(μ_k\\); The null hypothesis is that all the medians are the same.\n\\(H_a\\): \\(μ_1\\) ≠ \\(μ_k\\); The alternate hypothesis is that at least one median is different.\n\n\nkruskal.test(airquality$Ozone ~ airquality$Month)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  airquality$Ozone by airquality$Month\nKruskal-Wallis chi-squared = 29.267, df = 4, p-value = 6.901e-06"
  },
  {
    "objectID": "stats_regression.html#linear-regressions",
    "href": "stats_regression.html#linear-regressions",
    "title": "17  Regression Analysis",
    "section": "17.1 Linear Regressions",
    "text": "17.1 Linear Regressions\nThe linear regression equation is \\(y = b_0 + b_1 x\\), where y is the dependent variable, x is the independent variable, \\(b_0\\) is the intercept, and \\(b_1\\) is the slope.\nTo use linear regression in R, you use the lm() function:\n\nset.seed(123)\nx &lt;- rnorm(100, mean=1, sd=1)\ny &lt;- rnorm(100, mean=2, sd=2)\ndata &lt;- data.frame(x, y);\nmod &lt;- lm(data$y ~ data$x, data=data)\nmod\n\n\nCall:\nlm(formula = data$y ~ data$x, data = data)\n\nCoefficients:\n(Intercept)       data$x  \n     1.8993      -0.1049  \n\nsummary(mod)\n\n\nCall:\nlm(formula = data$y ~ data$x, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.815 -1.367 -0.175  1.161  6.581 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.8993     0.3033   6.261 1.01e-08 ***\ndata$x       -0.1049     0.2138  -0.491    0.625    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.941 on 98 degrees of freedom\nMultiple R-squared:  0.002453,  Adjusted R-squared:  -0.007726 \nF-statistic: 0.241 on 1 and 98 DF,  p-value: 0.6246\n\n\nWhen the p-value is less than 0.05, the model is significant:\n\n\\(H_0\\): Coefficient associated with the variable is equal to zero\n\\(H_a\\): Coefficient is not equal to zero (there is a relationship)\n\nFurthermore:\n\nThe higher the R-squared and the adjusted R-squared, the better the linear model.\nThe lower the standard error, the better the model"
  },
  {
    "objectID": "stats_regression.html#multiple-linear-regressions",
    "href": "stats_regression.html#multiple-linear-regressions",
    "title": "17  Regression Analysis",
    "section": "17.2 Multiple Linear Regressions",
    "text": "17.2 Multiple Linear Regressions\nMultiple linear regression is used when you have more than one independent variable.\nThe equation of a multiple linear regression is:\n\\(y = b_0 + b_1 x_1 + b_2 x_2 + ... + b_ k x_k + ϵ\\)\nWhen you have n observations or rows in the data set, you have the following model:\n\n\n\nFigure 4: Model for multiple regressions\n\n\nUsing a matrix, you can represent the equations as: \\(y = Xb + ϵ\\)\n\n\n\nFigure 5: Representation of equations as a matrix\n\n\nTo calculate the coefficients: ^b = (X’ X)-1 X’ y\n\nset.seed(123)\nx &lt;- rnorm(100, mean=1, sd=1)\nx2 &lt;- rnorm(100, mean=2, sd=5)\ny &lt;- rnorm(100, mean=2, sd=2)\ndata &lt;- data.frame(x, x2, y)\nmod &lt;- lm(data$y ~ data$x + data$x2, data=data)\nmod\n\n\nCall:\nlm(formula = data$y ~ data$x + data$x2, data = data)\n\nCoefficients:\n(Intercept)       data$x      data$x2  \n   2.517425    -0.266343     0.009525  \n\nsummary(mod)\n\n\nCall:\nlm(formula = data$y ~ data$x + data$x2, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7460 -1.3215 -0.2489  1.2427  4.1597 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.517425   0.305233   8.248 7.97e-13 ***\ndata$x      -0.266343   0.209739  -1.270    0.207    \ndata$x2      0.009525   0.039598   0.241    0.810    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.903 on 97 degrees of freedom\nMultiple R-squared:  0.01727,   Adjusted R-squared:  -0.00299 \nF-statistic: 0.8524 on 2 and 97 DF,  p-value: 0.4295"
  },
  {
    "objectID": "program_habits.html#useful-guidelines",
    "href": "program_habits.html#useful-guidelines",
    "title": "18  Programming Habits",
    "section": "18.1 Useful Guidelines",
    "text": "18.1 Useful Guidelines\nWe find the following to be useful guidelines:\n\nStart each program with some comments giving the name of the program, the author, the date it was written, and what the program does. A description of what a program does should explain what all the inputs and outputs are.\nVariable names should be descriptive, that is, they should give a clue as to what the value of the variable represents. Avoid using reserved names or function names as variable names (in particular t, c, and q are all function names in R). You can find out whether or not your preferred name for an object is already in use by the exists() function.\nUse blank lines to separate sections of code into related parts, and use indenting to distinguish the inside part of an if statement or a for or while loop.\nDocument the programs that you use in detail, ideally with citations for specific algorithms. There is no worse feeling than returning to undocumented code that had been written several years earlier to try to find and then explain an anomaly."
  },
  {
    "objectID": "program_logic.html#logical-expressions",
    "href": "program_logic.html#logical-expressions",
    "title": "19  Logic",
    "section": "19.1 Logical expressions",
    "text": "19.1 Logical expressions\nA logical expression is formed using:\n\nthe comparison operators &lt; , &gt; , &lt;=, &gt;=, == (equal to), != (not equal to), &&, || (sequentially evaluated versions of & and |, respectively).\nthe logical operators & (and), | (or), ! (not), xor() is exclusive or (i.e xor(x, y) is true if x is true, or y is true, but not both).\n\nAs well as & and |, R also has && and ||. Don’t use them in dplyr functions! These are called short-circuiting operators and only ever return a single TRUE or FALSE. They’re important for programming, not data science.\nFigure 10 shows the complete set of Boolean operations and how they work:  Figure 10. The complete set of Boolean operations. x is the left-hand circle, y is the right-hand circle, and the shaded regions show which parts each operator selects\nExample:\n# Find all rows where x is not missing\ndf |&gt; filter(!is.na(x))\n\n# Find all rows where x is smaller than -10 or bigger than 0\ndf |&gt; filter(x &lt; -10 | x &gt; 0) \nThe order of operations can be controlled using parentheses ( ).\nThe value of a logical expression is either TRUE or FALSE (the integers 1 and 0 can also be used to represent TRUE and FALSE, respectively).\nExample:\n\n## Note that A|B is TRUE if A or B or both are TRUE\nc(0,0,1,1)|c(0,1,0,1)\n\n[1] FALSE  TRUE  TRUE  TRUE\n\n## If you want exclusive disjunction, that is either A or B is TRUE but not both, then use xor(A,B)\nxor(c(0,0,1,1), c(0,1,0,1))\n\n[1] FALSE  TRUE  TRUE FALSE\n\n\n\n19.1.1 Order of operations\nNote that the order of operations doesn’t work like English. Take the following code that finds all flights that departed in November or December:\nflights |&gt; \n   filter(month == 11 | month == 12)\nYou might be tempted to write it like you’d say in English: “Find all flights that departed in November or December.”:\nflights |&gt; \n   filter(month == 11 | 12)\nThis code doesn’t error but it also doesn’t seem to have worked. What’s going on? Here, R first evaluates month == 11 creating a logical vector, which we call nov. It computes nov | 12. When you use a number with a logical operator it converts everything apart from 0 to TRUE, so this is equivalent to nov | TRUE which will always be TRUE, so every row will be selected:\nflights |&gt; \n  mutate(\n    nov = month == 11,\n    final = nov | 12,\n    .keep = \"used\"\n  )\n\n\n19.1.2 %in%\nAn easy way to avoid the problem of getting your ==s and |s in the right order is to use %in%. x %in% y returns a logical vector the same length as x that is TRUE whenever a value in x is anywhere in y.\nExample:\n\n1:12 %in% c(1, 5, 11)\n\n [1]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n\nletters[1:10] %in% c(\"a\", \"e\", \"i\", \"o\", \"u\")\n\n [1]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE\n\n\nSo to find all flights in November and December we could write:\n\nlibrary(nycflights13)\nlibrary(dplyr)\ndata(flights)\nflights |&gt; \n  filter(month %in% c(11, 12))\n\n# A tibble: 55,403 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013    11     1        5           2359         6      352            345\n 2  2013    11     1       35           2250       105      123           2356\n 3  2013    11     1      455            500        -5      641            651\n 4  2013    11     1      539            545        -6      856            827\n 5  2013    11     1      542            545        -3      831            855\n 6  2013    11     1      549            600       -11      912            923\n 7  2013    11     1      550            600       -10      705            659\n 8  2013    11     1      554            600        -6      659            701\n 9  2013    11     1      554            600        -6      826            827\n10  2013    11     1      554            600        -6      749            751\n# ℹ 55,393 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n19.1.3 Sequential && and ||\nTo evaluate x & y, R first evaluates x and y, then returns TRUE if x and y are both TRUE, FALSE otherwise.\nTo evaluate x && y, R first evaluates x. If x is FALSE then R returns FALSE without evaluating y. If x is TRUE then R evaluates y and returns TRUE if y is TRUE, FALSE otherwise.\nSequential evaluation of x and y is useful when y is not always well defined, or when y takes a long time to compute.\nNote that && and || only work on scalars, whereas & and | work on vectors on an element-by-element basis.\n\n\n19.1.4 Index position\nIf you wish to know the index positions of TRUE elements of a logical vector x, then use which(x):\nExample:\n\nx &lt;- c(1, 1, 2, 3, 5, 8, 13)\nwhich(x %% 2 == 0)\n\n[1] 3 6"
  },
  {
    "objectID": "program_logic.html#if-else",
    "href": "program_logic.html#if-else",
    "title": "19  Logic",
    "section": "19.2 if-else",
    "text": "19.2 if-else\nA natural extension of the if command includes an else part. Braces { } are used to group together one or more expressions. If there is only one expression then the braces are optional.\nWhen an if expression is evaluated, if logical_expression is TRUE then the first group of expressions is executed and the second group of expressions is not executed. Conversely, if logical_expression is FALSE then only the second group of expressions is executed.:\nif (logical_expression) {\n  expression_1 # do something\n  ...\n} else {\n  expression_2 # do something different\n  ...\n}\nExample:\n\nlibrary(tidyverse)\nx &lt;- c(-3:3, NA)\nif_else(x &gt; 0, \"+ve\", \"-ve\")\n\n[1] \"-ve\" \"-ve\" \"-ve\" \"-ve\" \"+ve\" \"+ve\" \"+ve\" NA   \n\n\n\n19.2.1 elseif\nWhen the else expression contains an if, then it can be written equivalently (and more clearly) as follows:\nif (logical_expression_1) {\n  expression_1\n  ...\n} else if (logical_expression_2) {\n  expression_2\n  ...\n} else {\n  expression_3\n  ...\n}\n\n\n19.2.2 case_when()\ndplyr’s case_when() is inspired by SQL’s CASE statement and provides a flexible way of performing different computations for different conditions. It takes pairs that look like condition ~ output. condition must be a logical vector; when it’s TRUE, output will be used.\nThis means we could recreate our previous nested if_else() as follows:\n\nx &lt;- c(-3:3, NA)\ncase_when(\n  x == 0   ~ \"0\",\n  x &lt; 0    ~ \"-ve\", \n  x &gt; 0    ~ \"+ve\",\n  is.na(x) ~ \"???\"\n)\n\n[1] \"-ve\" \"-ve\" \"-ve\" \"0\"   \"+ve\" \"+ve\" \"+ve\" \"???\""
  },
  {
    "objectID": "program_logic.html#comparisions",
    "href": "program_logic.html#comparisions",
    "title": "19  Logic",
    "section": "19.3 Comparisions",
    "text": "19.3 Comparisions\nA very common way to create a logical vector is via a numeric comparison with &lt;, &lt;=, &gt;, &gt;=, !=, and ==. For example, the following filter finds all daytime departures that arrive roughly on time:\nExample:\n\nlibrary(nycflights13)\nflights |&gt; \n  filter(dep_time &gt; 600 & dep_time &lt; 2000 & abs(arr_delay) &lt; 20)\n\n# A tibble: 172,286 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      601            600         1      844            850\n 2  2013     1     1      602            610        -8      812            820\n 3  2013     1     1      602            605        -3      821            805\n 4  2013     1     1      606            610        -4      858            910\n 5  2013     1     1      606            610        -4      837            845\n 6  2013     1     1      607            607         0      858            915\n 7  2013     1     1      611            600        11      945            931\n 8  2013     1     1      613            610         3      925            921\n 9  2013     1     1      615            615         0      833            842\n10  2013     1     1      622            630        -8     1017           1014\n# ℹ 172,276 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nYou can also use mutate() to explicitly create the underlying logical variables and add more intermediate steps to make it easier to both read your code and check that each step has been computed correctly.\nExample:\n\nflights |&gt; \n  mutate(\n    daytime = dep_time &gt; 600 & dep_time &lt; 2000,\n    approx_ontime = abs(arr_delay) &lt; 20,\n  ) |&gt; \n  filter(daytime & approx_ontime)\n\n# A tibble: 172,286 × 21\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      601            600         1      844            850\n 2  2013     1     1      602            610        -8      812            820\n 3  2013     1     1      602            605        -3      821            805\n 4  2013     1     1      606            610        -4      858            910\n 5  2013     1     1      606            610        -4      837            845\n 6  2013     1     1      607            607         0      858            915\n 7  2013     1     1      611            600        11      945            931\n 8  2013     1     1      613            610         3      925            921\n 9  2013     1     1      615            615         0      833            842\n10  2013     1     1      622            630        -8     1017           1014\n# ℹ 172,276 more rows\n# ℹ 13 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, daytime &lt;lgl&gt;,\n#   approx_ontime &lt;lgl&gt;"
  },
  {
    "objectID": "program_logic.html#logical-summaries",
    "href": "program_logic.html#logical-summaries",
    "title": "19  Logic",
    "section": "19.4 Logical summaries",
    "text": "19.4 Logical summaries\nThe following functions describe some useful techniques for summarizing logical vectors. There are two main logical summaries: any() and all():\n\nany(x) is the equivalent of |; it’ll return TRUE if there are any TRUE’s in x.\nall(x) is equivalent of &; it’ll return TRUE only if all values of x are TRUE’s. Like all summary functions, they’ll return NA if there are any missing values present, and as usual you can make the missing values go away with na.rm = TRUE.\n\nExample:\nflights |&gt; \n  group_by(year, month, day) |&gt; \n  summarize(\n    all_delayed = all(dep_delay &lt;= 60, na.rm = TRUE),\n    any_long_delay = any(arr_delay &gt;= 300, na.rm = TRUE),\n    .groups = \"drop\"\n  )"
  },
  {
    "objectID": "program_functions.html#building-functions",
    "href": "program_functions.html#building-functions",
    "title": "20  Functions",
    "section": "20.1 Building Functions",
    "text": "20.1 Building Functions\nFunctions are often used to encapsulate a sequence of expressions that need to be executed numerous times, perhaps under slightly different conditions. Functions are also often written when code must be shared with others or the public.\nThe writing of a function allows a developer to create an interface to the code, that is explicitly specified with a set of parameters.\nA function has the general form:\nname &lt;- function(argument_1, argument_2, ...) {\n  expression_1\n  expression_2\n  &lt;some other expressions&gt;\n  return(output)\n}\nHere argument_1, argument_2, etc., are the names of variables and expression_1, expression_2, and output are all regular R expressions.\nname is the name of the function. Because all function arguments have names, they can be specified using their name. Specifying an argument by its name is sometimes useful if a function has many arguments and it may not always be clear which argument is being specified.\nExample:\nf(num = 2)\nSome functions may have no arguments, and that the braces are only necessary if the function comprises more than one expression.\nExample:\n\nf &lt;- function(num = 1) {   \n## if the function is called without the num argument being explicitly specified, then it will print “Hello, world!” to the console once.\n  hello &lt;- \"Hello, world!\\n\" \n  for(i in seq_len(num)) {\n    cat(hello)\n  }\n  chars &lt;- nchar(hello) * num \n  chars\n}\n\nIn R, the return value of a function is always the very last expression that is evaluated (in the example is chars).\nThe formals() function returns a list of all the formal arguments of a function.\nNote that functions have their own class.\n\n20.1.1 Vector functions\nVector functions take one or more vectors and return a vector result. For example, take a look at this code. What does it do?\n\ndf &lt;- tibble(\n  a = rnorm(5),\n  b = rnorm(5),\n  c = rnorm(5),\n  d = rnorm(5),\n)\n\ndf |&gt; mutate(\n  a = (a - min(a, na.rm = TRUE)) / \n    (max(a, na.rm = TRUE) - min(a, na.rm = TRUE)),\n  b = (b - min(b, na.rm = TRUE)) / \n    (max(b, na.rm = TRUE) - min(b, na.rm = TRUE)),\n  c = (c - min(c, na.rm = TRUE)) / \n    (max(c, na.rm = TRUE) - min(c, na.rm = TRUE)),\n  d = (d - min(d, na.rm = TRUE)) / \n    (max(d, na.rm = TRUE) - min(d, na.rm = TRUE)),\n)\n\n# A tibble: 5 × 4\n      a      b      c      d\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1     0.832  0.0424 0.139 \n2 0.328 1      0.126  0     \n3 0.562 0      1      1     \n4 0     0.0749 0.0177 0.591 \n5 0.350 0.0925 0      0.0566\n\n\nTo write a function you need to first analyse your repeated code to figure what parts are constant and what parts vary. If we take the code above and pull it outside of mutate(), it’s a little easier to see the pattern because each repetition is now one line:\n(█ - min(█, na.rm = TRUE)) / (max(█, na.rm = TRUE) - min(█, na.rm = TRUE))\nThen you create a function by following the template:\nname &lt;- function(arguments) {\n  body\n}\nExample:\n\nrescale01 &lt;- function(x) {\n  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n}\n\nYou might test with a few simple inputs to make sure you’ve captured the logic correctly:\n\nrescale01(c(-10, 0, 10))\n\n[1] 0.0 0.5 1.0\n\n\nThen you can rewrite the call to mutate() as:\n\ndf |&gt; mutate(\n  a = rescale01(a),\n  b = rescale01(b),\n  c = rescale01(c),\n  d = rescale01(d),\n)\n\n# A tibble: 5 × 4\n      a      b      c      d\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1     0.832  0.0424 0.139 \n2 0.328 1      0.126  0     \n3 0.562 0      1      1     \n4 0     0.0749 0.0177 0.591 \n5 0.350 0.0925 0      0.0566\n\n\n\n\n20.1.2 Data frames functions\nData frame functions work like dplyr verbs: they take a data frame as the first argument, some extra arguments that say what to do with it, and return a data frame or a vector.\nWhen you start writing functions that use dplyr verbs you rapidly hit the problem of indirection. Let’s illustrate the problem with a very simple function: grouped_mean(). The goal of this function is to compute the mean of mean_var grouped by group_var:\nExample:\ngrouped_mean &lt;- function(df, group_var, mean_var) {\n  df |&gt; \n    group_by(group_var) |&gt; \n    summarize(mean(mean_var))\n}\nIf we try and use it, we get an error:\ndiamonds |&gt; grouped_mean(cut, carat)\n\n#&gt; Error in `group_by()`:\n#&gt; ! Must group by variables found in `.data`.\n#&gt; ✖ Column `group_var` is not found.\nHere we need some way to tell group_by() and summarize() not to treat group_var and mean_var as the name of the variables, but instead look inside them for the variable we actually want to use.\nSo to make grouped_mean() work, we need to surround group_var and mean_var with { }. Embracing a variable tells dplyr to use the value stored inside the argument, not the argument as the literal variable name. One way to remember what’s happening is to think of { } as looking down a tunnel — { var } will make a dplyr function look inside of var rather than looking for a variable called var.\nExample:\n\ngrouped_mean &lt;- function(df, group_var, mean_var) {\n  df |&gt; \n    group_by({{ group_var }}) |&gt; \n    summarize(mean({{ mean_var }}))\n}\n\n\ndiamonds |&gt; grouped_mean(cut, carat)\n\n# A tibble: 5 × 2\n  cut       `mean(carat)`\n  &lt;ord&gt;             &lt;dbl&gt;\n1 Fair              1.05 \n2 Good              0.849\n3 Very Good         0.806\n4 Premium           0.892\n5 Ideal             0.703\n\n\nSo the key challenge in writing data frame functions is figuring out which arguments need to be embraced. Fortunately, this is easy because you can look it up from the documentation.\n\n\n20.1.3 Plot functions\nYou might want to return a plot. For example, imagine that you’re making a lot of histograms:\ndiamonds |&gt; \n  ggplot(aes(x = carat)) +\n  geom_histogram(binwidth = 0.1)\n\ndiamonds |&gt; \n  ggplot(aes(x = carat)) +\n  geom_histogram(binwidth = 0.05)\nWouldn’t it be nice if you could wrap this up into a histogram function? This is easy as pie once you know that aes() is a data-masking function and you need to embrace:\n\nhistogram &lt;- function(df, var, binwidth = NULL) {\n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(binwidth = binwidth)\n}\n\ndiamonds |&gt; histogram(carat, 0.1)\n\n\n\n\nNote that histogram() returns a ggplot2 plot, meaning you can still add on additional components if you want. Just remember to switch from |&gt; to +:\n\ndiamonds |&gt; \n  histogram(carat, 0.1) +\n  labs(x = \"Size (in carats)\", y = \"Number of diamonds\")"
  },
  {
    "objectID": "program_functions.html#call-or-run-a-function",
    "href": "program_functions.html#call-or-run-a-function",
    "title": "20  Functions",
    "section": "20.2 Call or Run a Function",
    "text": "20.2 Call or Run a Function\nTo call or run the function we type (for example) name(x1, x2). The value of this expression is the value of the expression output. To calculate the value of output the function first copies the value of x1 to argument_1, x2 to argument_2, and so on. The arguments then act as variables within the function. We say that the arguments have been passed to the function. Next the function evaluates the grouped expressions contained in the braces { }; the value of the expression output is returned as the value of the function.\nTo use the function we first load it (using source or by copying and pasting into R), then call it, supplying suitable arguments.\nExample:\nrm(list=ls())\nsource(\"../scripts/quad3.r\")\nquad3(1,0,-1)\nNote that the name of the function does not have to match the name of the program file, but when a program consists of a single function this is conventional."
  },
  {
    "objectID": "program_functions.html#a-function-return",
    "href": "program_functions.html#a-function-return",
    "title": "20  Functions",
    "section": "20.3 A Function Return",
    "text": "20.3 A Function Return\nA function always returns a value. For some functions the value returned is unimportant, for example if the function has written its output to a file then there may be no need to return a value as well. In such cases one usually omits the return statement, or returns NULL.\nA function may have more than one return statement, in which case it stops after executing the first one it reaches. If there is no return(output) statement then the value returned by the function is the value of the last expression in the braces.\nIf, when called, the value returned by a function (or any expression) is not assigned to a variable, then it is printed. The expression invisible(x) will return the same value as x, but its value is not printed. For example, some versions of the summary() function use invisible on their returned object."
  },
  {
    "objectID": "program_functions.html#arguments",
    "href": "program_functions.html#arguments",
    "title": "20  Functions",
    "section": "20.4 Arguments",
    "text": "20.4 Arguments\nWe think about arguments both when the functions are written and when they are called. The arguments of an existing function can be obtained by calling the formals function.\nIn order to simplify calling functions, some arguments may be assigned default values, which are used in case the argument is not provided in the call to the function:\nExample:\n\ntest3 &lt;- function(x = 1) {\n  return(x)\n  }\ntest3(2)\n\n[1] 2\n\ntest3()\n\n[1] 1\n\n\nSometimes you will want to define arguments so that they can take only a small number of different values, and the function will stop informatively if an inappropriate value is passed.\nExample:\n\nfunk &lt;- function(vibe = c(\"Do\",\"Be\",\"Dooby\",\"Dooo\")) {\n  vibe &lt;- match.arg(vibe)\n  return(vibe)\n  }\nfunk()\n\n[1] \"Do\"\n\n\nfunk(Peter)\nError in match.arg(vibe) (from #2) :\n'arg' should be one of \"Do\", \"Be\", \"Dooby\", \"Dooo\"\n\n20.4.1 Argument Matching\nR functions arguments can be matched (called) positionally or by name:\n\nPositional matching just means that R assigns the first value to the first argument, the second value to second argument, etc.:\nExample:\n# Positional match first by argument, default for 'na.rm'\nsd(mydata)\n# Specify 'x' argument by name, default for 'na.rm'\nsd(x = mydata)\n# Specify both arguments by name\nsd(x = mydata, na.rm = FALSE)\nWhen specifying the function arguments by name, it doesn’t matter in what order you specify them. Named arguments are useful on the command line when you have a long argument list and you want to use the defaults for everything except for an argument near the end of the list:\n## Specify both arguments by name\nsd(na.rm = FALSE, x = mydata)  \nR also provides partial matching of arguments, where doing so is not ambiguous. This means that argument names in the call to the function do not have to be complete. Reliance on partial matching makes code more fragile.\n\ntest6 &lt;- function(a = 1, b.c.d = 1) {\n  return(a + b.c.d)\n  }\ntest6()\n\n[1] 2\n\ntest6(b = 5)\n\n[1] 6\n\n\n\n\n\n20.4.2 The … Argument\n...indicates a variable number of arguments that are usually passed on to other functions, it is often used when extending another function and you don’t want to copy the entire argument list of the original function.\nThe ... argument is necessary when the number of arguments passed to the function cannot be known in advance, e.g. paste(), cat().\nAny arguments that appear after ... on the argument list must be named explicitly and cannot be partially matched or matched positionally.\nR provides a very useful means of passing arguments, unaltered, from the function that is being called to the functions that are called within it. These arguments do not need to be named explicitly in the outer function, hence providing great flexibility. To use this facility you need to include ... in your argument list. These three dots (an ellipsis) act as a placeholder for any extra arguments given to the function.\nExample:\n\ntest4 &lt;- function(x, ...) {\n  return(sd(x, ...))\n  }\ntest4(1:3)\n\n[1] 1\n\n# Arguments that do not match those in test4 are provided, in order, to any function within test4 that has the dots in the list of arguments to the function call.\ntest4(c(1:2,NA), na.rm = TRUE)\n\n[1] 0.7071068\n\n\nUsing the dots in this way means that the user has access to all the function arguments without our needing to list them when we define the function.\nIn general, naming the arguments in the function call is good practice, because it increases the readability and eliminates one potential source of errors."
  },
  {
    "objectID": "program_functions.html#scoping-rules",
    "href": "program_functions.html#scoping-rules",
    "title": "20  Functions",
    "section": "20.5 Scoping Rules",
    "text": "20.5 Scoping Rules\nArguments and variables that are defined within a function exist only within that function. That is, if you define and use a variable x inside a function, it does not exist outside the function. If variables with the same name exist inside and outside a function, then they are separate and do not interact at all. You can think of a function as a separate environment that communicates with the outside world only through the values of its arguments and its output expression. For example if you execute the command rm(list=ls()) inside a function (which is only rarely a good idea), you only delete those objects that are defined inside the function.\nExample:\n\ntest &lt;- function(x) {\n  y &lt;- x + 1\n  return(y)\n  }\ntest(1)\n\n[1] 2\n\n\nx\nError: Object \"x\" not found\nThat part of a program in which a variable is defined is called its scope. Restricting the scope of variables within a function provides an assurance that calling the function will not modify variables outside the function, except by assigning the returned value.\nBeware, however, the scope of a variable is not symmetric. That is, variables defined inside a function cannot be seen outside, but variables defined outside the function can be seen inside the function, provided there is not a variable with the same name defined inside.This arrangement makes it possible to write a function whose behavior depends on the context within which it is run.\nExample:\n\ntest2 &lt;- function(x) {\n  y &lt;- x + z\n  return(y)\n  }\nz &lt;- 1\ntest2(1)\n\n[1] 2\n\n\nThe moral of this example is that it is generally advisable to ensure that the variables you use in a function either are declared as arguments, or have been defined in the function."
  },
  {
    "objectID": "program_functions.html#style",
    "href": "program_functions.html#style",
    "title": "20  Functions",
    "section": "20.6 Style",
    "text": "20.6 Style\nIdeally, the name of your function will be short, but clearly evoke what the function does. Generally, function names should be verbs, and arguments should be nouns. There are some exceptions: nouns are ok if the function computes a very well known noun (i.e. mean() is better than compute_mean()), or accessing some property of an object (i.e. coef() is better than get_coefficients()). Use your best judgement and don’t be afraid to rename a function if you figure out a better name later.\nExample:\n# Too short\nf()\n\n# Not a verb, or descriptive\nmy_awesome_function()\n\n# Long, but clear\nimpute_missing()\nAdditionally, function() should always be followed by squiggly brackets ({}), and the contents should be indented by an additional two spaces. This makes it easier to see the hierarchy in your code by skimming the left-hand margin.\n# Missing extra two spaces\ndensity &lt;- function(color, facets, binwidth = 0.1) {\ndiamonds |&gt; \n  ggplot(aes(x = carat, y = after_stat(density), color = {{ color }})) +\n  geom_freqpoly(binwidth = binwidth) +\n  facet_wrap(vars({{ facets }}))\n}\ncollapse_years()\n\n# Pipe indented incorrectly\ndensity &lt;- function(color, facets, binwidth = 0.1) {\n  diamonds |&gt; \n  ggplot(aes(x = carat, y = after_stat(density), color = {{ color }})) +\n  geom_freqpoly(binwidth = binwidth) +\n  facet_wrap(vars({{ facets }}))\n}"
  },
  {
    "objectID": "program_iteration.html#loops",
    "href": "program_iteration.html#loops",
    "title": "21  Iteration",
    "section": "21.1 Loops",
    "text": "21.1 Loops\nR is set up so that such programming tasks can be accomplished using vector operations rather than looping. Using vector operations is more efficient computationally, as well as more concise literally.\nExample:\n\n# Find the sum of the first n squares using a loop\nn &lt;- 100\nS &lt;- 0\nfor (i in 1:n) {\n  S &lt;- S + i^2\n}\nS\n\n[1] 338350\n\n# Alternatively, use vector operations\nsum((1:n)^2)\n\n[1] 338350\n\n\n\n21.1.1 for Loops\nThe for command has the following form, where x is a simple variable and vector is a vector.\nfor (x in vector) {\n  expression_1\n  ...\n}\nExample:\nx &lt;- c(\"a\", \"b\", \"c\", \"d\")\nfor(i in 1:4) / for(i in seq_along(x)) / for(letter in x) { ## set an iterator variable and assign it successive values over the elements of an object (list, vector, etc.) \n  print(x[i])\n  print(letter)\n}\nWhen executed, the for command executes the group of expressions within the braces { } once for each element of vector. The grouped expressions can use x, which takes on each of the values of the elements of vector as the loop is repeated.\n\n\n21.1.2 nested for loops\nExample:\nx &lt;- matrix(1:6, 2, 3) \nfor(i in seq_len(nrow(x))) { \n  for(j in seq_len(ncol(x))) { \n    print(x[i,j]) \n  }\n}\n\n\n21.1.3 while Loops\nOften we do not know beforehand how many times we need to go around a loop. That is, each time we go around the loop, we check some condition to see if we are done yet. In this situation we use a while loop, which has the form:\nwhile (logical_expression) {\n  expression_1\n  ...\n}\nWhen a while command is executed, logical_expression is evaluated first. If it is TRUE then the group of expressions in braces { } is executed. Control is then passed back to the start of the command: if logical_expression is still TRUE then the grouped expressions are executed again, and so on. Clearly, for the loop to stop eventually, logical_expression must eventually be FALSE. To achieve this logical_expression usually depends on a variable that is altered within the grouped expressions.\nExample:\ntime &lt;- 0\ndebt &lt;- debt_initial\nwhile (debt &gt; 0) {\n  time &lt;- time + period\n  debt &lt;- debt*(1 + r*period) - repayments\n}\n\n\n21.1.4 repeat Loops\nrepeat initiates an infinite loop right from the start. The only way to exit a repeat loop is to call break:\nrepeat { \n  if() { \n    break\n  }\n}\n\n\n21.1.5 next, break\nnext is used to skip an iteration of a loop:\nfor(i in 1:100) { \n  if(i &lt;= 20) { \n    next ## Skip the first 20 iterations \n  } \n  ## Do something here\n}\nbreak is used to exit a loop immediately, regardless of what iteration the loop may be on.\nfor(i in 1:100) { \n  print(i) \n  if(i &gt; 20) { \n    break ## Stop loop after 20 iterations \n  }\n}"
  },
  {
    "objectID": "program_iteration.html#loop-functions",
    "href": "program_iteration.html#loop-functions",
    "title": "21  Iteration",
    "section": "21.2 Loop Functions",
    "text": "21.2 Loop Functions\nMany R functions are vectorised, meaning that given vector input the function acts on each element separately, and a vector output is returned. This is a very powerful aspect of R that allows for compact, efficient, and readable code. Moreover, for many R functions, applying the function to a vector is much faster than if we were to write a loop to apply it to each element one at a time.\nBesides, writing for and while loops is useful when programming but not particularly easy when working interactively on the command line.\nR has some functions which implement looping in a compact form to make your life easier:\nlapply() ## Apply the function FUN to every element of vector X\nsapply() ## Same as lapply but try to simplify the result \napply() ## Apply a function that takes a vector argument to each of the rows (or columns) of a matrix, \ntapply() ## Apply a function over subsets of a vector\nmapply() ## Multivariate version of lapply (to vectorise over more than one argument)\nX can be a list or an atomic vector, which is a vector that comprises atomic objects (logical, integer, numeric, complex, character and raw). That is, sapply(X, FUN) returns a vector whose i -th element is the value of the expression FUN(X[i]). Note that R performs a loop over the elements of X, so execution of this code is not faster than execution of an equivalent loop.\nIf FUN has arguments other than X[i], then they can be included using the dots protocol as shown above. That is, sapply(X, FUN, ...) returns FUN(X[i], ...) as the i -th element.\n\n21.2.1 lapply\nThe lapply() function does the following simple series of operations:\n\nIt loops over a list, iterating over each element in that list\nIt applies a function to each element of the list (a function that you specify)\nReturns a list (the “l” is for “list”).\n\nlapply() always returns a list, regardless of the class of the input.\n\nstr(lapply)\n\nfunction (X, FUN, ...)  \n\n\nNote that the function() definition is right in the call to lapply():\nlapply(x, function(elt) { elt[,1] }\nExample:\n\nx &lt;- list(a = 1:5, b = rnorm(10)) \nlapply(x, mean)\n\n$a\n[1] 3\n\n$b\n[1] -0.05888739\n\n\nNote that when you pass a function to another function, you do not need to include the open and closed parentheses () like you do when you are calling a function.\nOnce the call to lapply() is finished, the function disappears and does not appear in the workspace.\n\n\n21.2.2 sapply\nThe sapply() function behaves similarly to lapply(), the only real difference being that the return value is a vector or a matrix.\n\nstr(sapply)\n\nfunction (X, FUN, ..., simplify = TRUE, USE.NAMES = TRUE)  \n\n\nsapply() will try to simplify the result of lapply() if possible.\nExample:\n\ns &lt;- split(airquality, airquality$Month)\nmode(s)\n\n[1] \"list\"\n\n(q &lt;- sapply(s, function(x) { \n              colMeans(x[, c(\"Ozone\", \"Solar.R\", \"Wind\")])\n          }\n))\n\n               5         6          7        8        9\nOzone         NA        NA         NA       NA       NA\nSolar.R       NA 190.16667 216.483871       NA 167.4333\nWind    11.62258  10.26667   8.941935 8.793548  10.1800\n\nmode(q)\n\n[1] \"numeric\"\n\n\n\n\n21.2.3 apply\nThe apply() function can take a list, matrix, or array. It is most often used to apply a function to the rows or columns of a matrix (which is just a 2-dimensional array).\n\nstr(apply)\n\nfunction (X, MARGIN, FUN, ..., simplify = TRUE)  \n\n\nUsing apply() is not really faster than writing a loop, but it works in one line and is highly compact.\n\nx &lt;- matrix(rnorm(200), 20, 10)\n# Take the mean of each column\n(k &lt;- apply(x, 2, mean))\n\n [1] -0.00418959 -0.13961675 -0.01918786  0.03668556 -0.08870387  0.40403088\n [7]  0.48197381  0.05156418 -0.26594834 -0.28498230\n\nmode(k)\n\n[1] \"numeric\"\n\n## Take the mean of each row\napply(x, 1, sum) \n\n [1]  2.5371599 -0.8462921  1.5727993 -3.3165679 -5.7904995 -2.3419253\n [7]  1.8760633  1.5223091 -1.4355897  1.9218128  3.0055875  0.3887839\n[13] -5.4690347  1.6710274 -3.5237386  4.9377428  4.7897812 -1.2801738\n[19]  0.1314319  3.0818367\n\n\nFor the special case of column/row sums and column/row means of matrices, there are some useful shortcuts:\nrowSums = apply(x, 1, sum) \nrowMeans = apply(x, 1, mean)\ncolSums = apply(x, 2, sum) \ncolMeans = apply(x, 2, mean)\n\n\n21.2.4 tapply\ntapply() is used to apply a function over subsets of a vector. It can be thought of as a combination of split() and sapply() for vectors only.\n\nstr(tapply)\n\nfunction (X, INDEX, FUN = NULL, ..., default = NA, simplify = TRUE)  \n\n\nExample:\n\n(x &lt;- c(rnorm(10), runif(10), rnorm(10, 1)))\n\n [1] -0.86862467  2.39363437 -1.00336572  0.40194716  1.07817348 -0.06154057\n [7]  1.07583187  1.15157505  0.80382264 -0.19319911  0.42035566  0.06499881\n[13]  0.75077761  0.92538854  0.45578440  0.20958017  0.52350188  0.78482985\n[19]  0.81527099  0.14910490 -0.11642061  1.91654193  0.31402704  0.12220536\n[25]  1.10272584  1.30234200  0.80874663 -0.17285829 -0.55918634  1.91669068\n\n# Define some groups with a factor variable\n(f &lt;- gl(3, 10))\n\n [1] 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3\nLevels: 1 2 3\n\ntapply(x, f, mean)\n\n        1         2         3 \n0.4778254 0.5099593 0.6634814 \n\n# We can reduce the noise as follows:\nround(tapply(x, f, mean), digits=1)\n\n  1   2   3 \n0.5 0.5 0.7 \n\n\n\n\n21.2.5 mapply\nThe mapply() function is a multivariate apply of sorts which applies a function in parallel over a set of arguments. Recall that lapply() and friends only iterate over a single R object. What if you want to iterate over multiple R objects in parallel? This is what mapply() is for.\n\nstr(mapply) \n\nfunction (FUN, ..., MoreArgs = NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE)  \n\n\nThe mapply() function has a different argument order from lapply() because the function to apply comes first rather than the object to iterate over. The R objects over which we apply the function are given in the ... argument because we can apply over an arbitrary number of R objects.\nExample 1:\n\nmapply(rep, 1:4, 4:1)\n\n[[1]]\n[1] 1 1 1 1\n\n[[2]]\n[1] 2 2 2\n\n[[3]]\n[1] 3 3\n\n[[4]]\n[1] 4\n\n\nExample 2:\n\nnoise &lt;- function(n, mean, sd) { \n            rnorm(n, mean, sd)\n          }\nmapply(noise, 1:5, 1:5, 2)\n\n[[1]]\n[1] -1.343527\n\n[[2]]\n[1]  1.4465996 -0.7114069\n\n[[3]]\n[1] 3.647641 4.072234 2.254939\n\n[[4]]\n[1] 3.255495 5.015238 2.374105 3.773722\n\n[[5]]\n[1] 5.122095 8.956223 5.041587 2.881163 5.619348\n\n\nThe mapply() function can be used to automatically “vectorize” a function. What this means is that it can be used to take a function that typically only takes single arguments and create a new function that can take vector arguments. This is often needed when you want to plot functions.\nExample:\n## Generate some data\nx &lt;- rnorm(100)\n## This is not what we want\nsumsq(1:10, 1:10, x)  \n## Note that the call to sumsq() only produced one value instead of 10 values.\nmapply(sumsq, 1:10, 1:10, MoreArgs = list(x = x))\nThere’s even a function in R called vectorize() that automatically can create a vectorized version of your function. So, we could create a vsumsq() function that is fully vectorized as follows.\nExample:\nvsumsq &lt;- vectorize(sumsq, c(\"mu\", \"sigma\")) \nvsumsq(1:10, 1:10, x)\n\n\n21.2.6 split\nThe split() function takes a vector or other objects and splits it into groups determined by a factor or list of factors.\nThe form of the split() function is this:\n\nstr(split) \n\nfunction (x, f, drop = FALSE, ...)  \n\n\nThe combination of split() and a function like lapply() or sapply() is a common paradigm in R.\nExample 1:\n\nx &lt;- c(rnorm(10), runif(10), rnorm(10, 1))\nf &lt;- gl(3, 10) \n# split() returns a list\n(j &lt;- split(x, f))\n\n$`1`\n [1]  0.07999646 -1.26641186  0.54519471  1.32786493 -0.90254279 -1.04232791\n [7] -0.55083112 -0.96446703 -0.46074487  0.18779746\n\n$`2`\n [1] 0.1116886 0.1558875 0.8889255 0.7093023 0.6757748 0.9784200 0.2469096\n [8] 0.3910239 0.8217220 0.1720508\n\n$`3`\n [1] 2.0563532 0.9451362 2.6603745 2.0936473 1.6898743 1.8331063 1.1628994\n [8] 1.4552615 2.1993825 0.6422503\n\nmode(j)\n\n[1] \"list\"\n\n\nExample 2:\n# Splitting a Dataframe\ns &lt;- split(airquality, airquality$Month)"
  },
  {
    "objectID": "program_iteration.html#functional-programming-loops",
    "href": "program_iteration.html#functional-programming-loops",
    "title": "21  Iteration",
    "section": "21.3 Functional Programming Loops",
    "text": "21.3 Functional Programming Loops\nFunctional programming loops are built around functions that take other functions as inputs. In this chapter we’ll focus on three common tasks: modifying multiple columns, reading multiple files, and saving multiple objects. The tools that we need are provided by dplyr and purrr, both core members of the tidyverse.\n\nlibrary(tidyverse)\n\n\n21.3.1 Modifying multiple columns\nLet’s start creating a simple tibble and counting the number of observations and compute the median of every column.\nExample:\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\nYou could do it with copy-and-paste:\n\ndf |&gt; summarize(\n  n = n(),\n  a = median(a),\n  b = median(b),\n  c = median(c),\n  d = median(d),\n)\n\n# A tibble: 1 × 5\n      n     a      b      c      d\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1    10 0.287 0.0412 -0.469 -0.386\n\n\nBut you can imagine that this will get very tedious if you have tens or even hundreds of columns. Instead, you can use across():\nExample:\n\ndf |&gt; summarize(\n  n = n(),\n  across(a:d, median),\n)\n\n# A tibble: 1 × 5\n      n     a      b      c      d\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1    10 0.287 0.0412 -0.469 -0.386\n\n\nThere are two additional selection techniques that are particularly useful for across():\n\neverything(): selects every (non-grouping) column:\n\ndf |&gt; \n  group_by(grp) |&gt; \n  summarize(across(everything(), median))\n\nwhere(): allows you to select columns based on their type:\n-   `where(is.numeric)` selects all numeric columns.\n-   `where(is.character)` selects all string columns.\n-   `where(is.Date)` selects all date columns.\n-   `where(is.POSIXct)` selects all date-time columns.\n-   `where(is.logical)` selects all logical columns.\nJust like other selectors, you can combine these with Boolean algebra. For example, !where(is.numeric) selects all non-numeric columns."
  },
  {
    "objectID": "program_debugging.html#show-values",
    "href": "program_debugging.html#show-values",
    "title": "22  Debugging",
    "section": "22.1 Show values",
    "text": "22.1 Show values\nYou will spend a lot of time correcting errors in your programs. To find an error or bug, you need to be able to see how your variables change as you move through the branches and loops of your code.\nAn effective and simple way of doing this is to include statements like cat(\"var =\", var, \"\\n\") throughout the program, to display the values of variables such as var as the program executes. Once you have the program working you can delete these or just comment them so they are not executed.\nIt is also very helpful to make dry runs of your code, using simple starting conditions for which you know what the answer should be. These dry runs should ideally use short and simple versions of the final program, so that analysis of the output can be kept as simple as possible.\nGraphs and summary statistics of intermediate outcomes can be very revealing, and the code to create them is easily commented out for production runs."
  },
  {
    "objectID": "program_debugging.html#message",
    "href": "program_debugging.html#message",
    "title": "22  Debugging",
    "section": "22.2 Message",
    "text": "22.2 Message\nWe can fix this problem by anticipating the possibility of NA values and checking to see if the input is NA with the is.na() function:\nprintmessage3 &lt;- function(x) {\n    if(length(x) &gt; 1L)\n        stop(\"'x' has length &gt; 1\")\n        if(is.na(x))\n            print(\"x is a missing value!\")\n    else if(x &gt; 0) \n        print(\"x is greater than zero\")\n    else\n        print(\"x is less than or equal to zero\") \n    invisible(x)\n}"
  },
  {
    "objectID": "program_debugging.html#traceback",
    "href": "program_debugging.html#traceback",
    "title": "22  Debugging",
    "section": "22.3 traceback()",
    "text": "22.3 traceback()\nThe traceback() command prints out the function call stack after an error has occurred. The function call stack is the sequence of functions that was called before the error occurred.\nThe traceback() command must be called immediately after an error occurs. Once another function is called, you lose the traceback.\nlm(y ~ x) \nError in eval(expr, envir, enclos) : object 'y' not found\ntraceback() \n7: eval(expr, envir, enclos)\n6: eval(predvars, data, env)\n5: model.frame.default(formula = y \\~ x, drop.unused.levels = TRUE)\n4: model.frame(formula = y \\~ x, drop.unused.levels = TRUE)\n3: eval(expr, envir, enclos)\n2: eval(mf, parent.frame())\n1: lm(y \\~ x)\nLooking at the traceback is useful for figuring out roughly where an error occurred but it’s not useful for more detailed debugging. For that you might turn to the debug() function."
  },
  {
    "objectID": "program_debugging.html#debug",
    "href": "program_debugging.html#debug",
    "title": "22  Debugging",
    "section": "22.4 debug()",
    "text": "22.4 debug()\nThe debug() function takes a function as its first argument. Here is an example of debugging the lm() function.\ndebug(lm)   ## Flag the 'lm()' function for interactive debugging\nlm(y ~ x) debugging\nNow, every time you call the lm() function it will launch the interactive debugger. To turn this behavior off you need to call the undebug() function.\nThe debugger calls the browser at the very top level of the function body. From there you can step through each expression in the body. There are a few special commands you can call in the browser:\n\nn: executes the current expression and moves to the next expression.\nc: continues execution of the function and does not stop until either an error or the function exits.\nQ: quits the browser.\n\nYou can turn off interactive debugging with the undebug() function like this:\nundebug(lm)  ## Unflag the 'lm()' function for debugging"
  },
  {
    "objectID": "program_debugging.html#recover",
    "href": "program_debugging.html#recover",
    "title": "22  Debugging",
    "section": "22.5 recover()",
    "text": "22.5 recover()\nThe recover() function can be used to modify the error behavior of R when an error occurs. Normally, when an error occurs in a function, R will print out an error message, exit out of the function, and return you to your workspace to await further commands.\nWith recover() you can tell R that when an error occurs, it should halt execution at the exact point at which the error occurred.\noptions(error = recover)  ## Change default R error behavior\nread.csv(\"nosuchfile\")  ## This code doesn't work"
  },
  {
    "objectID": "program_profiling.html#system.time",
    "href": "program_profiling.html#system.time",
    "title": "23  Profiling R Code",
    "section": "23.1 system.time()",
    "text": "23.1 system.time()\nThe system.time() function computes the time (in seconds) needed to execute an expression and if there’s an error, gives the time until the error occurred. Using system.time() allows you to test certain functions or code blocks to see if they are taking excessive amounts of time.\nThe function returns an object of class proc_time which contains two useful bits of information:\n\nuser time: time charged to the CPU(s) for this expression.\nelapsed time: “wall clock” time, the amount of time that passes for you as you’re sitting there.\n\nThe elapsed time may be greater than the user time if the CPU spends a lot of time waiting around. The elapsed time may be smaller than the user time if your machine has multiple cores/processors (and is capable of using them).\nHere’s an example of where the elapsed time is greater than the user time:\n\n## Elapsed time &gt; user time \nsystem.time(readLines(\"http://www.amazon.com\")) \n\nWarning in readLines(\"http://www.amazon.com\"): line 1 appears to contain an\nembedded nul\n\n\nWarning in readLines(\"http://www.amazon.com\"): line 6 appears to contain an\nembedded nul\n\n\nWarning in readLines(\"http://www.amazon.com\"): line 8 appears to contain an\nembedded nul\n\n\nWarning in readLines(\"http://www.amazon.com\"): line 12 appears to contain an\nembedded nul\n\n\nWarning in readLines(\"http://www.amazon.com\"): line 17 appears to contain an\nembedded nul\n\n\nWarning in readLines(\"http://www.amazon.com\"): line 19 appears to contain an\nembedded nul\n\n\nWarning in readLines(\"http://www.amazon.com\"): incomplete final line found on\n'http://www.amazon.com'\n\n\n   user  system elapsed \n   0.00    0.00    0.36 \n\n\nYou can time longer expressions by wrapping them in curly braces within the call to system.time().\nsystem.time({ \n  ## expression or loop or function\n  })"
  },
  {
    "objectID": "program_profiling.html#the-r-profiler",
    "href": "program_profiling.html#the-r-profiler",
    "title": "23  Profiling R Code",
    "section": "23.2 The R Profiler",
    "text": "23.2 The R Profiler\nsystem.time() assumes that you already know where the problem is and can call system.time() on that piece of code. What if you don’t know where to start? This is where the profiler comes in handy.\nIn conjunction with Rprof(), we will use the summaryRprof() function which summarizes the output from Rprof().\nRprof() keeps track of the function call stack at regularly sampled intervals and tabulates how much time is spent inside each function.\nThe profiler is started by calling the Rprof() function:\nRprof()  ## Turn on the profiler\nOnce you call the Rprof() function, everything that you do from then on will be measured by the profiler. Therefore, you usually only want to run a single R function or expression once you turn on the profiler and then immediately turn it off.\nThe profiler can be turned off by passing NULL to Rprof():\nRprof(NULL)  ## Turn off the profiler\nsummaryRprof()\nThe summaryRprof() function tabulates the R profiler output and calculates how much time is spend in which function. There are two methods for normalizing the data:\n\n“by.total”: divides the time spend in each function by the total run time.\n“by.self”: does the same as “by.total” but first subtracts out time spent in functions above the current function in the call stack.\n\nThe final bit of output that summaryRprof() provides is the sampling interval and the total runtime.\n$sample.interval \n[1] 0.02\n$sampling.time \n[1] 7.41"
  },
  {
    "objectID": "graphs.html#principles-of-graphics",
    "href": "graphs.html#principles-of-graphics",
    "title": "24  Graphs",
    "section": "24.1 Principles of Graphics",
    "text": "24.1 Principles of Graphics\n1. Show comparisons\nYou should always be comparing at least two things.\n2. Show causality, mechanism, explanation, systematic structure\nGenerally, it’s difficult to prove that one thing causes another thing even with the most carefully collected data. But it’s still often useful for your data graphics to indicate what you are thinking about in terms of cause.\n3. Show multivariate data\nThe point is that data graphics should attempt to show this information as much as possible, rather than reduce things down to one or two features that we can plot on a page… From the plot it seems that there is a slight negative relationship between the two variables….This example illustrates just one of many reasons why it can be useful to plot multivariate data and to show as many features as intelligently possible. In some cases, you may uncover unexpected relationships depending on how they are plotted or visualized.\n4. Integrate evidence\nData graphics should make use of many modes of data presentation simultaneously, not just the ones that are familiar to you or that the software can handle. One should never let the tools available drive the analysis; one should integrate as much evidence as possible on to a graphic as possible.\n5. Describe and document the evidence\nData graphics should be appropriately documented with labels, scales, and sources. A general rule for me is that a data graphic should tell a complete story all by itself. Is there enough information on that graphic for the person to get the story? While it is certainly possible to be too detailed, I tend to err on the side of more information rather than less.\n6. Content, Content, Content\nAnalytical presentations ultimately stand or fall depending on the quality, relevance, and integrity of their content. This includes the question being asked and the evidence presented in favor of certain hypotheses. Starting with a good question, developing a sound approach, and only presenting information that is necessary for answering that question, is essential to every data graphic."
  },
  {
    "objectID": "graphs.html#types-of-graphs",
    "href": "graphs.html#types-of-graphs",
    "title": "24  Graphs",
    "section": "24.2 Types of Graphs",
    "text": "24.2 Types of Graphs\nVisualizing the data via graphics can be important at the beginning stages of data analysis to understand basic properties of the data, to find simple patterns in data, and to suggest possible modeling strategies. In later stages of an analysis, graphics can be used to “debug” an analysis, if an unexpected (but not necessarily wrong) result occurs, or ultimately, to communicate your findings to others.\nWe will make a distinction between exploratory graphs and final graphs:\nExploratory graphs are usually made very quickly and a lot of them are made in the process of checking out the data, developing a personal understanding of the data and to prioritize tasks for follow up. Details like axis orientation or legends, while present, are generally cleaned up and prettified if the graph is going to be used for communication later.\n\n24.2.1 One-dimension graphs\n\n\n\n\n\n\n\n\nGraph\nCode\nUse\n\n\n\n\nFive-number summary\nfivenum(), summary()\n\n\n\nBoxplots\nboxplot(pollution$pm25, col = \"blue\")\nCommonly plot outliers that go beyond the bulk of the data\n\n\nBarplot\ntable(pollution$region) %&gt;% barplot(col = \"wheat\")\nFor visualizing categorical data, with the number of entries for each category being proportional to the height of the bar\n\n\nHistogram\nhist(pollution$pm25, col = \"green\", breaks = 100)\nrug(pollution$pm25)\nabline(v = 12, lwd = 2)\nabline(v = median(pollution$pm25), col = \"magenta\", lwd = 4)\nCheck skewness of the data, symmetry, multi-modality, and other features\n\n\nDensity plot\na &lt;- density(airquality$Ozone, na.rm = TRUE)\nplot(a)\nComputes a non-parametric estimate of the distribution of a variables\n\n\n\n\n\n24.2.2 Multi-dimension graphs\n\n\n\n\n\n\n\n\nGraph\nCode\nUse\n\n\n\n\nMultiple 1-D plots\nboxplot(pm25 ~ region, data = pollution, col = \"red\")\npar(mfrow = c(2, 1), mar = c(4, 4, 2, 1))\nhist(subset(pollution, region == \"east\")$pm25, col = \"green\")\nwith(subset(airquality, Month == 5), points(Wind, Ozone, col = \"blue\"))\nhist(subset(pollution, region == \"west\")$pm25, col = \"green\")&gt; boxplot(pm25 ~ region, data = pollution, col = \"red\")\npar(mfrow = c(2, 1), mar = c(4, 4, 2, 1))\nhist(subset(pollution, region == \"east\")$pm25, col = \"green\")\nwith(subset(airquality, Month == 5), points(Wind, Ozone, col = \"blue\"))\nhist(subset(pollution, region == \"west\")$pm25, col = \"green\")\nFor seeing the relationship between two variables, especially when one is naturally categorical\n\n\nScatterplots\nwith(pollution, plot(latitude, pm25, col = region)) &gt; abline(h = 12, lwd = 2, lty = 2)\nlevels(pollution$region)\npar(mfrow = c(2, 1), mar = c(4, 4, 2, 1))\nhist(subset(pollution, region == \"east\")$pm25, col = \"green\")\nhist(subset(pollution, region == \"west\")$pm25, col = \"green\")\nVisualizing two continuous variables\n\n\nScatter plot matrix\npairs(~var1+var2+var3+var4+var5, data=data, main=\"scatterplot matrix\")\nIt is used to find the correlation between a variable and other variables, and to select the important variables, which is also known as variable selection\n\n\nSmooth scatterplots\nwith(airquality, {plot(Temp, Ozone), lines(loess.smooth(Temp, Ozone))\nSimilar to scatterplots but rather plots a 2-D histogram. Can be useful for scatterplots with many many data points"
  },
  {
    "objectID": "graphs.html#the-base-plotting-system",
    "href": "graphs.html#the-base-plotting-system",
    "title": "24  Graphs",
    "section": "24.3 The Base Plotting System",
    "text": "24.3 The Base Plotting System\nThe base plotting system is the original plotting system for R. The basic model is sometimes referred to as the “artist’s palette” model. The idea is you start with blank canvas and build up from there.\nYou will typically start with a plot function (or similar plot creating function) to initiate a plot and then annotate the plot. If you don’t have a completely well-formed idea of how you want your data to look, you’ll often start by “throwing some data on the page” and then slowly add more information to it as our thought process evolves.\nThere are a few downsides though:\n\nYou can’t go backwards once the plot has started.\nWhile the base plotting system is nice in that it gives you the flexibility to specify these kinds of details to painstaking accuracy, sometimes it would be nice if the system could just figure it out for you.\nIt’s difficult to describe or translate a plot to others because there’s no clear graphical language or grammar that can be used to communicate what you’ve done.\n\n\n24.3.1 How to Create a Base Plot?\n\nFirst, you must read the data into R with read.csv(). For example, the avgpm25.csv dataset contains the annual mean PM2.5 averaged over the period 2008 through 2010\nclass &lt;- c(\"numeric\", \"character\", \"factor\", \"numeric\", \"numeric\")\npollution &lt;- read.csv(\"data/avgpm25.csv\", colClasses = class)  \nExplicitly launch a graphics device.\nCall a plotting function to make a plot (Note: if you are using a file device, no plot will appear on the screen).\nAnnotate the plot if necessary.\nExplicitly close graphics device with dev.off() (this is very important!).\n\nExample:\n# Open PDF device; create 'myplot.pdf' in my working directory\npdf(file = \"myplot.pdf\", width = 4, height = 3) # The height and width arguments are in units of inches.\n# Create plot and send to a file (no plot appears on screen) \nwith(faithful, plot(eruptions, waiting))\n# Annotate plot; still nothing on screen \ntitle(main = \"Old Faithful Geyser data\") \n# Close the PDF file device \ndev.off() \n# Now you can view the file 'myplot.pdf' on your computer\nLet’s further detail each of the steps involved in creating a base plot.\n\n\n24.3.2 Graphics Devices\nWe can think of a graphics device as being a platform upon which the plot is created. If we create a plot, then a default graphics device is automatically opened for the plot to appear upon. In other words, when you make a plot in R, it has to be “sent” to a specific graphics device.\nThe most common place for a plot to be “sent” is the screen device. Functions like plot() in base, xyplot() in lattice, or qplot in ggplot2 will default to sending a plot to the screen device. Therefore, when making a plot, you need to consider how the plot will be used to determine what device the plot should be sent to.\nThe list of devices supported by your installation of R is found in ?Devices.\n\nUsing the commands pdf, postscript, jpeg, png, or bmp, we can also produce graphics in the formats that correspond to these names.\nThe jpeg, png, and bmp graphics are all raster-style graphics, which may translate poorly when included in a document.\nIn contrast, the pdf, postscript, and windows metafile (win.metafile, available on Windows) formats allow for vector-style graphics, which are scaleable, and better suited to integration in documents.\n\n\n\n\n\n\n\n\nVector\nBitmap\n\n\n\n\nGood for line drawings and plots with solid colors using a modest number of points\ngood for plots with a large number of points, natural scenes or web-based plots\n\n\npdf (line-type graphics, resizes well, usually portable)\npng (good for line drawings or images with solid colors)\n\n\nsvg (XML-based scalable vector graphics; supports animation and interactivity)\njpeg (good for plotting many many many points, does not resize well)\n\n\nwin.metafile\ntiff\n\n\npostscript\nbmp\n\n\n\nIt is possible to open multiple graphics devices (screen, file, or both), for example when viewing multiple plots at once. Plotting can only occur on one graphics device at a time, though.\nOne way of having more than one plot visible is to open additional graphics devices. In a Windows environment this is done by using the command windows() before each additional plot.\nTo create a graphics device without a plot, we call the function that is specific to our operating system (that is, windows for Windows, quartz for Mac, and X11 for Unix).\nThe currently active graphics device can be found by calling dev.cur()\nEvery open graphics device is assigned an integer starting with 2 (there is no graphics device 1). You can change the active graphics device with dev.set(&lt;integer&gt;).\n\n\n24.3.3 Graphics Parameters\nGraphics parameters control how output appears on graphics devices. To get a complete list with their current values, type par(). Some of the parameters mentioned above, namely pch, lwd and col, are examples of graphics parameters. To get the value of a specific parameter, for example pch, type par(\"pch\").\nSome graphics parameters can apply to one or more plots, and others only make sense when applied to graphics devices. For example, to change the symbol for a single plot, we could include the argument pch = 2 in the call to the plot function. However, we could also make this change for all graphics that are produced on the device.\nTo change a graphics parameter for the graphics device, we use the par command. These are some examples:\n\npar(mfrow = c(a,b)) where a and b are integers, will create a matrix of plots on one page, with a rows and b columns. These will be filled by rows; use mfcol if you wish to fill them by columns.\nExample:\n\npar(mfrow = c(2, 2), mar=c(5, 4, 2, 1))\ncurve(x*sin(x), from = 0, to = 100, n = 1001)\ncurve(x*sin(x), from = 0, to = 10, n = 1001)\ncurve(x*sin(x), from = 0, to = 1, n = 1001)\ncurve(x*sin(x), from = 0, to = 0.1, n = 1001)\n\n\n\n\npar(mar = c(bottom, left, top, right)) will create space around each plot, in which to write axis labels and titles. Measurements are in units of character widths.\npar(oma = c(bottom, left, top, right)) will create space around the matrix of plots (an outer margin). Measurements are in units of character widths.\npar(las = 1) rotates labels on the y-axis to be horizontal rather than vertical.\npar(pty = \"s\") forces the plot shape to be square. The alternative is that the plot shape is mutable, which is the default, and corresponds to pty =\"m\".\n\n\n\n24.3.4 Make a Plot with plot(x,y)\nplot(x, y) is used to plot one vector against another, with the x values on the x-axis and the y values on the y-axis.\nThe plot command offers a wide variety of options for customizing the graphic. Each of the following arguments can be used within the plot statement, singly or together, separated by commas\n\ntype = \"p\": Determines the type of plot, with options:\n\n“p” for points (the default);\n“l” for lines;\n“b” for both, with gaps in the lines for the points; …\n\nExample:\n\nwith(airquality, plot(Wind, Ozone, \n    main = \"Ozone and Wind in New York City\",\n    type = \"p\")\n)\n\n\n\n\npoints(x, y): To add points (x[1], y[1]), (x[2], y[2]), … to the current plot.\nlines(x, y): To add lines.\nabline(v = xpos) and abline(h = ypos): to draw vertical or horizontal lines. - col: Both points and lines take the optional input from col (e.g. “red”, “blue”, etc.). The complete list of available colours can be obtained by the colours function (or colors).\nExample: # Plot with a Regression Line\n\nFirst make the plot (as above).\nFit a simple linear regression model using the lm() function.\nTake the output of lm() and pass it to the abline() function which automatically takes the information from the model object and calculates the corresponding regression line\n\ntext(x, y, labels): To add the text labels [i] at the point (x[i], y[i]). The optional input pos is used to indicate where to position the labels in relation to the points.\ntitle(text): To add a title, where text is a character string.\nmain = \"Plot title goes in here\": provides the plot title.\nxlab = \" \" / ylab = \" \": To add axis labels.\nNote: In a call to plot, the arguments for main, sub, and xlab and ylab can be character strings or expressions that contain a mathematical expression. For mathematical typesetting you can use the functions expression(require graphics) and bquote(base):\nExample:\n...\nxlab = expression(alpha),\nylab = expression(100 %*% (alpha^3 - alpha^2) + 15),\n# Mix of mathematical expressions and character strings into a single expression by using the paste function\nmain = expression(paste(\"Function: \",\n   f(alpha) == 100 %*% (alpha^3 - alpha^2) + 15)),\n...\nxlab = expression(alpha) tells R to interpret alpha in the context of the MML (mathematical markup language), producing an α as the label for the x-axis.\npch = k: Determines the shape of points, with k taking a value from 1 to 25.\nlwd = 1: line width, default 1.\nxlim = c(a,b)/ `ylim = c(a,b): Will set the lower and upper limits of the x-axis/y-axis to be a and b, respectively.\n\nAs an example we plot part of the parabola y2 = 4x, as well as its focus and directrix. We make use of the surprisingly useful input type = \"n\", which results in the graph dimensions being established, and the axes being drawn, but nothing else.\nExample:\n\nx &lt;- seq(0, 5, by = 0.01)\ny.upper &lt;- 2*sqrt(x)\ny.lower &lt;- -2*sqrt(x)\ny.max &lt;- max(y.upper)\ny.min &lt;- min(y.lower)\nplot(c(-2, 5), c(y.min, y.max), type = \"n\", xlab = \"x\", ylab = \"y\")\nlines(x, y.upper)\nlines(x, y.lower)\nabline(v=-1)\npoints(1, 0)\ntext(1, 0, \"focus (1, 0)\", pos=4)\ntext(-1, y.min, \"directrix x = -1\", pos = 4)\ntitle(\"The parabola y^2 = 4*x\")\n\n\n\n\n\n\n24.3.5 Augmenting a Plot\nA traditional plot can be augmented using any of a number of different tools after its creation. A number of these different steps are detailed below:\n\nStart by creating the plot object, which sets up the dimensions of the space, but omit any plot objects for the moment:\nExample:\nopar1 &lt;- par(las = 1, mar=c(4,4,3,2))   \nplot(ufc$dbh.cm, ufc$height.m, axes=FALSE, \n    xlab=\"\",\n    ylab=\"\",\n    type=\"n\"\n)\nNext, we add the points. Here we use different colours and symbols for different heights of trees: those that are realistic, and those that are not, which may reflect measurement errors. We use the vectorised ifelse() function:\nExample:\npoints(ufc$dbh.cm, ufc$height.m,     \n  col = ifelse(ufc$height.m &gt; 4.9, \"darkseagreen4\", \"red\"),\n  pch = ifelse(ufc$height.m &gt; 4.9, 1, 3)\n)\nThen we add axes. The following are the simplest possible calls. We can also control the locations of the tickmarks, and their labels; we can overlay different axes, change colour, and so on. ?axis provides the details:\nExample:\naxis(1)\naxis(2)\nWe can next add axis labels using margin text (switching back to vertical direction for the y-axis text):\nExample:\nopar2 &lt;- par(las=0)\nmtext(\"Diameter (cm)\", side=1, line=3)\nmtext(\"Height (m)\", side=2, line=3)\nWrap the plot in the traditional frame. As before, we can opt to use different line types and different colours:\nExample:\nbox()\nFinally, we add a legend:\nExample:\nlegend(x = 60, y = 15, c(\"Normal trees\", \"A weird tree\"),\n    col=c(\"darkseagreen3\", \"red\"),\n    pch=c(1, 3),\n    bty=\"n\"\n)\nNote the first two arguments: the location of the legend can also be expressed relative to the graph components, for example, by “bottomright”.\nIf we wish, we can return the graphics environment to a previous state:\npar(opar1)\n\nCheck out the playwith package, which provides interaction with graphical objects at a level unattainable in base R.\n\n\n24.3.6 Color in Plots\nTypically we add color to a plot, not to improve its artistic value, but to add another dimension to the visualization. It makes sense that the range and palette of colors you use will depend on the kind of data you are plotting. Careful choices of plotting color can have an impact on how people interpret your data and draw conclusions from them.\nThe function colors() lists the names of (657) colors you can use in any plotting function. Typically, you would specify the color in a (base) plotting function via the col argument.\nThe grDevices package has two functions, they differ only in the type of object that they return:\n\ncolorRamp: Take a palette of colors and return a function that takes values between 0 and 1.\npal &lt;- colorRamp(c(\"red\", \"blue\")) \npal(0)\nThe numbers in the matrix will range from 0 to 255 and indicate the quantities of red, green, and blue (RGB) in columns 1, 2, and 3 respectively. there are over 16 million colors that can be expressed in this way.\nThe idea here is that you do not have to provide just two colors in your initial color palette; you can start with multiple colors and colorRamp() will interpolate between all of them.\ncolorRampPalette: Takes a palette of colors and returns a function that takes integer arguments and returns a vector of colors interpolating the palette (like heat.colors() or topo.colors()).\npal &lt;- colorRampPalette(c(\"red\", \"yellow\"))\npal(3)\nReturns 3 colors in between red and yellow. Note that the colors are represented as hexadecimal strings.\nNote that the rgb() function can be used to produce any color via red, green, blue proportions and return a hexadecimal representation:\nrgb(0, 0, 234, maxColorValue = 255)\nPart of the art of creating good color schemes in data graphics is to start with an appropriate color palette that you can then interpolate with a function like colorRamp() or colorRampPalette().\n\nFor improved color palettes you can use the RColorBrewer package. Here is a display of all the color palettes available from this package:\n\nlibrary(RColorBrewer) \ndisplay.brewer.all()\n\n\n\n\n\nThe brewer.pal() function creates nice looking color palettes especially for thematic maps:\n\nlibrary(RColorBrewer)\n(cols &lt;- brewer.pal(3, \"BuGn\"))\n\n[1] \"#E5F5F9\" \"#99D8C9\" \"#2CA25F\"\n\n\nThese three colors make up your initial palette. Then you can pass them to colorRampPalette() to create my interpolating function.\n\npal &lt;- colorRampPalette(cols)\n\nNow you can plot your data using this newly created color (ramp) palette:\nimage(volcano, col = pal(20))\nThe smoothScatter() function is very useful for making scatterplots of very large datasets:\n\nset.seed(1)\nx &lt;- rnorm(10000)\ny &lt;- rnorm(10000) \nsmoothScatter(x, y)\n\n\n\n\nAdding transparency: Color transparency can be added via the alpha parameter to rgb() to produce color specifications with varying levels of transparency.\n\nrgb(1, 0, 0, 0.1)\n\n[1] \"#FF00001A\"\n\n\nTransparency can be useful when you have plots with a high density of points or lines. If you add some transparency to the black circles, you can get a better sense of the varying density of the points in the plot.\n\n# x, y from the previous example\nplot(x, y, pch = 19, col = rgb(0, 0, 0, 0.15)) \n\n\n\n\n\n\n\n24.3.7 Copying Plots\nNote that copying a plot is not an exact operation, so the result may not be identical to the original:\n## Copy my plot to a PNG file  (follows the previous code from pdf creation)\ndev.copy(png, file = \"geyserplot.png\") \n\n## Don't forget to close the PNG device! \ndev.off()\n\n\n24.3.8 Complementary Packages\n\ngraphics: contains plotting functions for the “base” graphing systems, including plot, hist, boxplot and many others\ngrdevices: contains all the code implementing the various graphics devices, including X11, PDF, PostScript, PNG, etc."
  },
  {
    "objectID": "graphs.html#the-lattice-system",
    "href": "graphs.html#the-lattice-system",
    "title": "24  Graphs",
    "section": "24.4 The Lattice System",
    "text": "24.4 The Lattice System\nTrellis graphics are a data visualisation framework developed at the Bell Labs, which have been implemented in R as the lattice package.\nTrellis graphics are a set of techniques for displaying multidimensional data. They allow great flexibility for producing conditioning plots; that is, plots obtained by conditioning on the value of one of the variables.\nTo use Trellis graphics you must first you must load the lattice package with the library function: library(lattice).\n\n24.4.1 When to Use Trellis Plots\n\nLattice plots tend to be most useful for conditioning types of plots, i.e. looking at how y changes with x across levels of z. These types of plots are useful for looking at multidimensional data and often allow you to squeeze a lot of information into a single window or page.\nWith the lattice system, plots are created with a single function call, such as xyplot() or bwplot().\nNote that there is no real distinction between the functions that create or initiate plots and the functions that annotate plots because it all happens at once.\nAnother difference from base plotting is that things like margins and spacing are set automatically. This is possible because entire plot is specified at once via a single function call.\nThe notion of panels comes up a lot with lattice plots because you typically have many panels in a lattice plot (each panel typically represents a condition, like “region”).\nOnce a plot is created, you cannot “add” to the plot (but of course you can just make it again with modifications).\n\n\n\n24.4.2 How to Use Lattice\nTo illustrate the use of the lattice package we use the ufc dataset, where dbh (diameter at breast height) and height vary by species. That is, the plots are conditioned on the value of the variable species.\nExample:\n# These plots display a distribution of values taken on by the variable dbh, divided up according to the value of the variable species\ndensityplot(~ dbh.cm | species, data = ufc) # A density plot\nbwplot(~ dbh.cm | species, data = ufc) # # A box and whiskers plot\nhistogram(~ dbh.cm | species, data = ufc) # A histogram\n# We plot height as a function of dbh.\nxyplot(height.m ~ dbh.cm | species, data = ufc) # A scatterplot\nAll four commands require a formula object, which is described using ~and |.\n\nIf a dataframe is passed to the function, using the argument data, then the column names of the dataframe can be used for describing the model.\nWe interpret y ~ x | a as saying we want y as a function of x, divided up by the different levels of a.\nIf `a is not a factor then a factor will be created by coercion.\nIf we are just interested in x we still include the ~ symbol, so that R knows that we are specifying a model.\nIf we wish to provide within-panel conditioning on a second variable, then we use the group argument.\n\nIn order to display numerous lattice objects on a graphics device, we call the print function with the split and more arguments.\nExample:\n# Place a lattice object (called my.lat) in the top-right corner of a 3-row, 2-column graphics device, and allow for more objects\nprint(my.lat, split = c(2,1,2,3), more = TRUE)\nSee ?print.trellis for further details.\nGraphics produced by lattice are highly customisable.\nExample:\nxyplot(height.m ~ dbh.cm | species,\n    data = ufc,\n    subset = species %in% list(\"WC\", \"GF\"),\n    panel = function(x, y, ...) {\n        panel.xyplot(x, y, ...),       \n        panel.abline(lm(y~x), ...)\n    },\n    xlab = \"Diameter (cm)\",\n    ylab = \"Height (m)\"\n    )\nNote that the subset argument is a logical expression or a vector of integers, and is used to restrict the data that are plotted. We can also change the order of the panels using the index.cond argument (see ?xyplot for more details). The panels are plotted bottom left to top right by default, or top left to bottom right if the argument as.table = TRUE is supplied.\nThe panel argument accepts a function, the purpose of which is to control the appearance of the plot in each panel. The panel function should have one input argument for each variable in the model, not including the conditioning variable."
  },
  {
    "objectID": "graphs.html#d-plots",
    "href": "graphs.html#d-plots",
    "title": "24  Graphs",
    "section": "24.5 3D Plots",
    "text": "24.5 3D Plots\nR provides considerable functionality for constructing 3D graphics, using either the base graphics engine or the lattice package. However, its is recommended to use the lattice package, because the data can be supplied to the lattice functions in a familiar structure: observations in rows and variables in columns, unlike that required by the base 3D graphics engine (observations in a grid).\nExample:\nufc.plots &lt;- read.csv(\"../data/ufc-plots.csv\")\nstr(ufc.plots)\nlibrary(lattice)\nwireframe(vol.m3.ha ~ east * north,\n    main = expression(paste(\"Volume (\", m^3, ha^{-1}, \")\", sep =        \"\")),\n    xlab = \"East (m)\", ylab = \"North (m)\",\n    data = ufc.plots\n    )\nTo learn more about base-graphics 3D plots, run the demonstrations demo(persp) and demo(image) and look at the examples presented."
  },
  {
    "objectID": "graphs.html#ggplot2",
    "href": "graphs.html#ggplot2",
    "title": "24  Graphs",
    "section": "24.6 ggplot2",
    "text": "24.6 ggplot2\nggplot2 is one of the most elegant and most versatile systems for making graphs. ggplot2 implements the grammar of graphics, a coherent system for describing and building graphs.\nggplot2 splits the difference between base and lattice in a number of ways. Taking cues from lattice, the ggplot2 system automatically deals with spacings, text, titles but also allows you to annotate by “adding” to a plot.\nYou can find more information in the Graphs section here.\n\n24.6.1 Installation\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\nA typical plot with the ggplot package looks as follows:\ndata(mpg)\nqplot(displ, hwy, data = mpg)\nIn ggplot2, aesthetics are the things we can see (e.g. position, color, fill, shape, line type, size). You can use aesthetics in ggplot2 via the aes() function:\nggplot(data, aes(x=var1, y=var2))\n\n\n24.6.2 Geometric objects\nGeometric objects are the plots or graphs you want to put in the chart.\nYou can use geom_point() to create a scatterplot, geom_line() to create a line plot, and geom_boxplot() to create a boxplot in the chart.\nhelp.search(\"geom_\", package=\"ggplot2\")\nIn ggplot2, geom is also the layers of the chart. You can add in one geom object after another, just like adding one layer after another layer.\nggplot(data, aes(x=var1, y=var2)) + \n  geom_point(aes(color=\"red\"))\n\n\n24.6.3 Plotly\nPlotly JS allows you to create interactive, publication-quality charts. You can create a Plotly chart using ggplot:\ninstall.packages(\"plotly\")\nset.seed(12)\nvar1 &lt;- rnorm(100, mean=1, sd=1)\nvar2 &lt;- rnorm(100, mean=2, sd=1)\ndata &lt;- data.frame(var1, var2)\ngg &lt;- ggplot(data) + geom_line(aes(x=var1, y=var2))\ng &lt;- ggplotly(gg)"
  },
  {
    "objectID": "graph_ggplot.html#the-layered-grammar-of-graphics",
    "href": "graph_ggplot.html#the-layered-grammar-of-graphics",
    "title": "25  Creating a ggplot",
    "section": "25.1 The Layered Grammar of Graphics",
    "text": "25.1 The Layered Grammar of Graphics\nThe grammar of graphics is a template made of seven parameters (the bracketed words that appear in the template) for building plots. The grammar of graphics is based on the insight that you can uniquely describe any plot as a combination of a dataset, a geom, a set of mappings, a stat, a position adjustment, a coordinate system, a faceting scheme, and a theme.\nggplot(data = &lt;DATA&gt;) + \n  &lt;GEOM_FUNCTION&gt;(\n     mapping = aes(&lt;MAPPINGS&gt;),\n     stat = &lt;STAT&gt;, \n     position = &lt;POSITION&gt;\n  ) +\n  &lt;COORDINATE_FUNCTION&gt; +\n  &lt;FACET_FUNCTION&gt;\nTo build a basic plot from scratch:\n\nYou could start with a dataset and then transform it into the information that you want to display (with a stat).\nNext, you could choose a geometric object to represent each observation in the transformed data.\nYou could then use the aesthetic properties of the geoms to represent variables in the data. You would map the values of each variable to the levels of an aesthetic.\nYou’d then select a coordinate system to place the geoms into, using the location of the objects (which is itself an aesthetic property) to display the values of the x and y variables.\nYou could further adjust the positions of the geoms within the coordinate system (a position adjustment) or split the graph into subplots (faceting).\nYou could also extend the plot by adding one or more additional layers, where each additional layer uses a dataset, a geom, a set of mappings, a stat, and a position adjustment.\n\nTo learn more about the theoretical underpinnings that describes the theory of ggplot2 in detail, click here: The Layered Grammar of Graphics."
  },
  {
    "objectID": "graph_ggplot.html#define-a-plot-object",
    "href": "graph_ggplot.html#define-a-plot-object",
    "title": "25  Creating a ggplot",
    "section": "25.2 Define a Plot Object",
    "text": "25.2 Define a Plot Object\nWith ggplot2, you begin a plot with the function ggplot(), defining a plot object that you then add layers to.\nThe first argument of ggplot() is the dataset to use in the graph and so ggplot(data = penguins) creates an empty graph that is primed to display the penguins data, but since we haven’t told it how to visualize it yet, for now it’s empty (it’s like an empty canvas you’ll paint the remaining layers of your plot onto).\nExample:\n\nggplot(data = penguins)"
  },
  {
    "objectID": "graph_ggplot.html#map-the-data",
    "href": "graph_ggplot.html#map-the-data",
    "title": "25  Creating a ggplot",
    "section": "25.3 Map the Data",
    "text": "25.3 Map the Data\nNext, we need to tell ggplot() how the information from our data will be visually represented. The mapping argument of the ggplot() function defines how variables in your dataset are mapped to visual properties (aesthetics) of your plot.\nThe mapping argument is always defined in the aes() function, and the x and yarguments of aes() specify which variables to map to the x and y axes.\nExample:\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n)\n\n\n\n\nOur empty canvas now has more structure – it’s clear where flipper lengths will be displayed (on the x-axis) and where body masses will be displayed (on the y-axis). But the penguins themselves are not yet on the plot. This is because we have not yet determine how to represent the observations from our dataframe on our plot."
  },
  {
    "objectID": "graph_ggplot.html#represent-the-data-geoms",
    "href": "graph_ggplot.html#represent-the-data-geoms",
    "title": "25  Creating a ggplot",
    "section": "25.4 Represent the Data (geoms)",
    "text": "25.4 Represent the Data (geoms)\nTo do so, we need to define a geom: the geometrical object that a plot uses to represent data. These geometric objects are made available in ggplot2 with functions that start with geom_.\nFor example, bar charts use geom_bar(), line charts use geom_line(), boxplots use geom_boxplot(), scatterplots use geom_point(), and so on.\nExample:\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nNote: One common problem when creating ggplot2 graphics is to put the + in the wrong place: it has to come at the end of the line, not the start.\nYou can rewrite the previous plot more concisely:\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nOr using the pipe, |&gt;:\n\npenguins |&gt; \n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nNow we have something that looks like what we might think of as a “scatterplot”.\nYou can also set the visual properties of your geom manually as an argument of your geom function (outside of aes()) instead of relying on a variable mapping to determine the appearance. For example, we can make all of the points in our plot blue:\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point(color = \"blue\")\n\n\n\n\n\n25.4.1 Help with geoms\nThe best place to get a comprehensive overview of all of the geoms ggplot2 offers, as well as all functions in the package, is this page: https://ggplot2.tidyverse.org/reference.\nTo learn more about any single geom, use the help (e.g., ?geom_smooth).\nSee also https://exts.ggplot2.tidyverse.org/gallery/ for a sampling of community made geoms."
  },
  {
    "objectID": "graph_ggplot.html#add-aesthetics-and-layers",
    "href": "graph_ggplot.html#add-aesthetics-and-layers",
    "title": "25  Creating a ggplot",
    "section": "25.5 Add Aesthetics and Layers",
    "text": "25.5 Add Aesthetics and Layers\nYou will we need to modify the aesthetic mapping, inside of aes().\nExample:\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nOnce you map an aesthetic, ggplot2 takes care of the rest. It selects a reasonable scale to use with the aesthetic, and it constructs a legend that explains the mapping between levels and values. For x and y aesthetics, ggplot2 does not create a legend, but it creates an axis line with tick marks and a label. The axis line provides the same information as a legend; it explains the mapping between locations and values.\nWhen a categorical variable is mapped to an aesthetic, ggplot2 will automatically assign a unique value of the aesthetic (here a unique color) to each unique level of the variable (each of the three species), a process known as scaling. ggplot2 will also add a legend that explains which values correspond to which levels.\nNow let’s add one more layer: a smooth curve displaying the relationship between body mass and flipper length. Since this is a new geometric object representing our data, we will add a new geom as a layer on top of our point geom: geom_smooth(). And we will specify that we want to draw the line of best fit based on a linear model with method = \"lm\".\nExample:\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nWhen aesthetic mappings are defined in ggplot(), at the global level, they’re passed down to each of the subsequent geom layers of the plot. However, each geom function in ggplot2 can also take a mapping argument, which allows for aesthetic mappings at the local level that are added to those inherited from the global level. In other words, if you place mappings in a geom function, ggplot2 will treat them as local mappings for the layer. It will use these mappings to extend or overwrite the global mappings for that layer only. This makes it possible to display different aesthetics in different layers.\nExample:\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species)) +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nNote that since we want points to be colored based on species but don’t want the lines to be separated out for them, we should specify color = species for geom_point() only.\nYou can specify different data for each layer. Here, we use red points as well as open circles to highlight two-seater cars. The local data argument in geom_point() overrides the global data argument in ggplot() for that layer only.\nExample:\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  geom_point(\n    data = mpg |&gt; filter(class == \"2seater\"), \n    color = \"red\"\n  ) +\n  geom_point(\n    data = mpg |&gt; filter(class == \"2seater\"), \n    shape = \"circle open\", size = 3, color = \"red\"\n  )\n\n\n\n\nIt’s generally not a good idea to represent information using only colors on a plot, as people perceive colors differently due to color blindness or other color vision differences. Therefore, in addition to color, we can also map species to the shape aesthetic.\nExample:\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n25.5.1 Help with aes\nYou can learn more about all possible aesthetic mappings in the aesthetic specifications vignette at https://ggplot2.tidyverse.org/articles/ggplot2-specs.html. Remember that the specific aesthetics you can use for a plot depend on the geom you use to represent the data."
  },
  {
    "objectID": "graph_ggplot.html#statistical-transformation",
    "href": "graph_ggplot.html#statistical-transformation",
    "title": "25  Creating a ggplot",
    "section": "25.6 Statistical Transformation",
    "text": "25.6 Statistical Transformation\nMany graphs, like scatterplots, plot the raw values of a dataset. Other graphs, like bar charts, calculate new values to plot. The algorithm used to calculate new values for a graph is called a stat, short for statistical transformation.\nYou can figure out which stat a geom uses by inspecting the default value for the stat argument. For example, ?geom_bar shows that the default value for stat is “count”, which means that geom_bar() uses stat_count(). To find the possible variables that can be computed by the stat, look for the section titled “computed variables” in the help for geom_bar().\nEvery geom has a default stat; and every stat has a default geom. This means that you can typically use geoms without worrying about the underlying statistical transformation. However, there are three reasons why you might need to use a stat explicitly:\n\nYou might want to override the default stat. In the following example we change the stat of geom_bar() from count (the default) to identity. This lets us map the height of the bars to the raw values of a y variable.\nExample:\n\ndiamonds |&gt;\n  count(cut) |&gt;\n  ggplot(aes(x = cut, y = n)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\nYou might want to override the default mapping from transformed variables to aesthetics.\nExample:\n\n# Display a bar chart of proportions, rather than counts\nggplot(diamonds, aes(x = cut, y = after_stat(prop), group = 1)) + \n  geom_bar()\n\n\n\n\nYou might want to focus on the statistical transformation in your code. For example, you might use stat_summary(), which summarizes the y values for each unique x value, to draw attention to the summary that you’re computing:\nExample:\n\nggplot(diamonds) + \n  stat_summary(\n    aes(x = cut, y = depth),\n    fun.min = min,\n    fun.max = max,\n    fun = median\n  )"
  },
  {
    "objectID": "graph_ggplot.html#position-adjustments",
    "href": "graph_ggplot.html#position-adjustments",
    "title": "25  Creating a ggplot",
    "section": "25.7 Position Adjustments",
    "text": "25.7 Position Adjustments\nYou can color a bar chart using either the color aesthetic, or, more usefully, the fill aesthetic:\nExample:\n\nggplot(mpg, aes(x = drv, color = drv)) + \n  geom_bar()\n\n\n\n\nExample:\n\nggplot(mpg, aes(x = drv, fill = drv)) + \n  geom_bar()\n\n\n\n\nNote that if you map the fill aesthetic to another variable, like class, the bars are automatically stacked. In our example, each colored rectangle represents a combination of drv and class.\nExample:\n\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar()\n\n\n\n\nThe stacking is performed automatically using the position adjustment specified by the position argument. If you don’t want a stacked bar chart, you can use one of four other options:\n\nposition = \"identity\": will place each object exactly where it falls in the context of the graph. See how bars overlap and why the identity position adjustment is more useful for 2d geoms, like points, where it is the default.\n\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar(alpha = 1/5, position = \"identity\")\n\n\n\n\nposition = \"fill\": works like stacking, but makes each set of stacked bars the same height. This makes it easier to compare proportions across groups.\nExample:\n\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar(position = \"fill\")\n\n\n\n\nposition = \"dodge\": places overlapping objects directly beside one another. This makes it easier to compare individual values.\nExample:\n\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\nposition = \"jitter\": adds a small amount of random noise to each point in a scatterplot. This spreads the points out to solve the problem of overplotting, which makes it difficult to see the distribution of the data.\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point(position = \"jitter\")"
  },
  {
    "objectID": "graph_ggplot.html#coordinate-systems",
    "href": "graph_ggplot.html#coordinate-systems",
    "title": "25  Creating a ggplot",
    "section": "25.8 Coordinate Systems",
    "text": "25.8 Coordinate Systems\nThe default coordinate system is the Cartesian coordinate system where the x and y positions act independently to determine the location of each point.\nThere are two other coordinate systems that are occasionally helpful.\n\ncoord_quickmap(): sets the aspect ratio correctly for plotting spatial data with ggplot2. See more in the Maps chapter of the ggplot2 book.\nExample:\n\nlibrary(maps)\n\nnz &lt;- map_data(\"nz\")\n\nggplot(nz, aes(x = long, y = lat, group = group)) +\n  geom_polygon(fill = \"white\", color = \"black\")\n\n\n\nggplot(nz, aes(x = long, y = lat, group = group)) +\n  geom_polygon(fill = \"white\", color = \"black\") +\n  coord_quickmap()\n\n\n\n\ncoord_polar(): uses polar coordinates. Polar coordinates can reveal interesting connections between a bar chart and a Coxcomb chart.\nExample:\n\nbar &lt;- ggplot(data = diamonds) + \n  geom_bar(\n    mapping = aes(x = clarity, fill = clarity), \n    show.legend = FALSE,\n    width = 1\n  ) + \n  theme(aspect.ratio = 1)\n\nbar + coord_flip()\n\n\n\nbar + coord_polar()"
  },
  {
    "objectID": "graph_ggplot.html#facets",
    "href": "graph_ggplot.html#facets",
    "title": "25  Creating a ggplot",
    "section": "25.9 Facets",
    "text": "25.9 Facets\nFacets splits a plot into subplots that each display one subset of the data based on a categorical variable.\nExample:\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_wrap(~cyl)\n\n\n\n\nTo facet your plot with the combination of two variables, switch from facet_wrap() to facet_grid(). The first argument of facet_grid() is also a formula, but now it’s a double sided formula: rows ~ cols.\nExample:\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_grid(drv ~ cyl)\n\n\n\n\nBy default each of the facets share the same scale and range for x and y axes. This is useful when you want to compare data across facets but it can be limiting when you want to visualize the relationship within each facet better. Setting the scales argument in a faceting function to free_x will allow for different scales of x-axis across columns, free_y will allow for different scales on y-axis across rows, and free will allow both.\nExample:\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_grid(drv ~ cyl, scales = \"free\")"
  },
  {
    "objectID": "graph_ggplot.html#improve-labels",
    "href": "graph_ggplot.html#improve-labels",
    "title": "25  Creating a ggplot",
    "section": "25.10 Improve Labels",
    "text": "25.10 Improve Labels\nAnd finally, we can improve the labels of our plot using the labs() function in a new layer:\n\ntitle, adds a title.\nsubtitle, adds a subtitle to the plot.\nx, is the x-axis label.\ny, is the y-axis label.\ncolor and shape define the label for the legend.\n\nIn addition, we can improve the color palette to be colorblind safe with the scale_color_colorblind() function from the ggthemes package.\nExample:\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Body mass and flipper length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n    color = \"Species\", shape = \"Species\"\n  ) +\n  scale_color_colorblind()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nThe purpose of a plot title is to summarize the main finding. Avoid titles that just describe what the plot is (e.g. “Fuel efficcieny generally decreases with engine size”).\n\n25.10.1 Mathematical labels\nIt’s possible to use mathematical equations instead of text strings. Just switch ” ” out for quote() and read about the available options in ?plotmath:\nExample:\nggplot(df, aes(x, y)) +\n  geom_point() +\n  labs(\n    x = quote(x[i]),\n    y = quote(sum(x[i] ^ 2, i == 1, n))\n  )"
  },
  {
    "objectID": "graph_ggplot.html#themes",
    "href": "graph_ggplot.html#themes",
    "title": "25  Creating a ggplot",
    "section": "25.11 Themes",
    "text": "25.11 Themes\ngplot2 includes the eight themes, with theme_gray() as the default. Many more are included in add-on packages like ggthemes (https://jrnold.github.io/ggthemes). You can also create your own themes, if you are trying to match a particular corporate or journal style.\nExample:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(se = FALSE) +\n  theme_bw()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nIt’s also possible to control individual components of each theme, like the size and color of the font used for the y axis.\nExample:\n  ...\n  theme(\n    legend.position = c(0.6, 0.7),\n    legend.direction = \"horizontal\",\n    legend.box.background = element_rect(color = \"black\"),\n    plot.title = element_text(face = \"bold\"),\n    plot.title.position = \"plot\",\n    plot.caption.position = \"plot\",\n    plot.caption = element_text(hjust = 0)\n  )\nFor an overview of all theme() components, see help with ?theme."
  },
  {
    "objectID": "graph_ggplot.html#layout",
    "href": "graph_ggplot.html#layout",
    "title": "25  Creating a ggplot",
    "section": "25.12 Layout",
    "text": "25.12 Layout\nWhat if you have multiple plots you want to lay out in a certain way? The patchwork package allows you to combine separate plots into the same graphic.\nTo place two plots next to each other, you can simply add them to each other. Note that you first need to create the plots and save them as objects (in the following example they’re called p1 and p2). Then, you place them next to each other with +.\n\nlibrary(patchwork)\np1 &lt;- ggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  labs(title = \"Plot 1\")\np2 &lt;- ggplot(mpg, aes(x = drv, y = hwy)) + \n  geom_boxplot() + \n  labs(title = \"Plot 2\")\np1 + p2\n\n\n\n\nYou can also create complex plot layouts with patchwork. In the following, | places the p1 and p3 next to each other and / moves p2 to the next line.\n\np3 &lt;- ggplot(mpg, aes(x = cty, y = hwy)) + \n  geom_point() + \n  labs(title = \"Plot 3\")\n(p1 | p3) / p2"
  },
  {
    "objectID": "graph_ggplot.html#save-your-plot",
    "href": "graph_ggplot.html#save-your-plot",
    "title": "25  Creating a ggplot",
    "section": "25.13 Save Your Plot",
    "text": "25.13 Save Your Plot\nOnce you’ve made a plot, you might want to get it out of R by saving it as an image that you can use elsewhere. That’s the job of ggsave(), which will save the plot most recently created to disk:\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\nggsave(filename = \"penguin-plot.png\")\n\nSaving 7 x 5 in image\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nThis will save your plot to your working directory."
  },
  {
    "objectID": "graph_types.html#visualizing-categorial-variables",
    "href": "graph_types.html#visualizing-categorial-variables",
    "title": "26  Types of Graphs",
    "section": "26.1 Visualizing Categorial Variables",
    "text": "26.1 Visualizing Categorial Variables\nA variable is categorical if it can only take one of a small set of values.\n\n26.1.1 Bar Chart\nTo examine the distribution of a categorical variable, you can use a bar chart. The height of the bars displays how many observations occurred with each x value.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(ggthemes)\n\nExample:\n\nggplot(penguins, aes(x = species)) +\n  geom_bar()\n\n\n\n\n\n\n26.1.2 Bar Plots\nIn bar plots of categorical variables with non-ordered levels, like the penguin species above, it’s often preferable to reorder the bars based on their frequencies. Doing so requires transforming the variable to a factor (how R handles categorical data) and then reordering the levels of that factor.\nExample:\n\nggplot(penguins, aes(x = fct_infreq(species))) +\n  geom_bar()"
  },
  {
    "objectID": "graph_types.html#visualizing-numerical-variables",
    "href": "graph_types.html#visualizing-numerical-variables",
    "title": "26  Types of Graphs",
    "section": "26.2 Visualizing Numerical Variables",
    "text": "26.2 Visualizing Numerical Variables\nA variable is numerical (or quantitative) if it can take on a wide range of numerical values, and it is sensible to add, subtract, or take averages with those values. Numerical variables can be continuous or discrete.\n\n26.2.1 Histogram\nOne commonly used visualization for distributions of continuous variables is a histogram.\nExample:\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 200)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nA histogram divides the x-axis into equally spaced bins and then uses the height of a bar to display the number of observations that fall in each bin. In the graph above, the tallest bar shows that 39 observations have a body_mass_g value between 3,500 and 3,700 grams, which are the left and right edges of the bar.\nYou can set the width of the intervals in a histogram with the binwidth argument, which is measured in the units of the x variable.\nExample:\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 20)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 2000)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nAn alternative visualization for distributions of numerical variables is a density plot. A density plot is a smoothed-out version of a histogram and a practical alternative, particularly for continuous data that comes from an underlying smooth distribution.\nExample:\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_density()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`)."
  },
  {
    "objectID": "graph_types.html#visualizing-relationships",
    "href": "graph_types.html#visualizing-relationships",
    "title": "26  Types of Graphs",
    "section": "26.3 Visualizing Relationships",
    "text": "26.3 Visualizing Relationships\nTo visualize a relationship we need to have at least two variables mapped to aesthetics of a plot.\n\n26.3.1 Numerical and Categorical Variable\nYou can use side-by-side box plots. A boxplot is a type of visual shorthand for measures of position (percentiles) that describe a distribution. It is also useful for identifying potential outliers.\nExample:\n\nggplot(penguins, aes(x = species, y = body_mass_g)) +\n  geom_boxplot()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nAlternatively, we can make density plots with geom_density().\nExample:\n\nggplot(penguins, aes(x = body_mass_g, color = species)) +\n  geom_density(linewidth = 0.75)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\nAdditionally, we can map species to both color and fill aesthetics and use the alpha aesthetic to add transparency to the filled density curves. This aesthetic takes values between 0 (completely transparent) and 1 (completely opaque). In the following plot it’s set to 0.5.\n\nggplot(penguins, aes(x = body_mass_g, color = species, fill = species)) +\n  geom_density(alpha = 0.5)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n26.3.2 Two Categorical Variables\nWe can use stacked bar plots to visualize the relationship between two categorical variables. For example, the following two stacked bar plots both display the relationship between island and species, or specifically, visualizing the distribution of species within each island.\nExample:\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar()\n\n\n\n\nThe following plot, a relative frequency plot created by setting position = \"fill\" in the geom, is more useful for comparing species distributions across islands since it’s not affected by the unequal numbers of penguins across the islands.\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar(position = \"fill\")\n\n\n\n\nIn creating these bar charts, we map the variable that will be separated into bars to the x aesthetic, and the variable that will change the colors inside the bars to the fill aesthetic.\n\n\n26.3.3 Two Numerical Variables\nA scatterplot is probably the most commonly used plot for visualizing the relationship between two numerical variables.\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n26.3.4 Three or More Variables\nWe can incorporate more variables into a plot by mapping them to additional aesthetics. For example, in the following scatterplot the colors of points represent species and the shapes of points represent islands.\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = island))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n26.3.5 Facets\nAdding too many aesthetic mappings to a plot makes it cluttered and difficult to make sense of. Another way, which is particularly useful for categorical variables, is to split your plot into facets, subplots that each display one subset of the data.\nTo facet your plot by a single variable, use facet_wrap(). The first argument of facet_wrap() is a formula3, which you create with ~ followed by a variable name. The variable that you pass to facet_wrap() should be categorical.\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = species)) +\n  facet_wrap(~island)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nIf you don’t specify the width and height they will be taken from the dimensions of the current plotting device. For reproducible code, you’ll want to specify them."
  },
  {
    "objectID": "graph_types.html#visualizing-maps",
    "href": "graph_types.html#visualizing-maps",
    "title": "26  Types of Graphs",
    "section": "26.4 Visualizing Maps",
    "text": "26.4 Visualizing Maps\n\n26.4.1 Coordinates on a Map\nExample:\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(mapview)\nlibrary(modeldata)\n\ndata(ames)\n\names |&gt;\n  select(Longitude, Latitude, Neighborhood) |&gt;\n  # glimpse() |&gt; \n  mapview(map.types = \"OpenStreetMap\",\n          xcol = \"Longitude\", ycol = \"Latitude\", \n          zcol = \"Neighborhood\", \n          crs = 4269, \n          grid = FALSE)\n\n\n\n\n\n\nYou can find more information here\n\n\n26.4.2 Maps silhouettes\nExample\n\nlibrary(maps)\n\nnz &lt;- map_data(\"nz\")\n\nggplot(nz, aes(x = long, y = lat, group = group)) +\n  geom_polygon(fill = \"white\", color = \"black\")\n\n\n\nggplot(nz, aes(x = long, y = lat, group = group)) +\n      geom_polygon(fill = \"white\", color = \"black\") +\n      coord_quickmap()"
  },
  {
    "objectID": "model_flow.html",
    "href": "model_flow.html",
    "title": "27  Modeling Workflow",
    "section": "",
    "text": "A model workflow is important in two ways. First, using a workflow encourages good methodology since it is a single point of entry to the estimation components of a data analysis. Second, it enables the user to better organize projects\nThe typical path to determining an appropriate model is an iterative process. The general phases are:\n\nExploratory data analysis (EDA): Initially there is a back and forth between numerical analysis and data visualization where different discoveries lead to more questions and data analysis side-quests to gain more understanding.\nFeature engineering: The understanding gained from EDA results in the creation of specific model terms that make it easier to accurately model the observed data. This can include complex methodologies (e.g., PCA) or simpler features (using the ratio of two predictors).\nModel tuning and selection: A variety of models are generated and their performance is compared. Some models require parameter tuning in which some structural parameters must be specified or optimized. This is the stage where repeated data splitting is used for resampling.\nModel evaluation: During this phase of model development, we assess the model’s performance metrics, examine residual plots, and conduct other EDA-like analyses to understand how well the models work. In some cases, formal between-model comparisons help you understand whether any differences in models are within the experimental noise.\n\nAfter an initial sequence of these tasks, more understanding is gained regarding which models are superior as well as which data subpopulations are not being effectively estimated. This leads to additional EDA and feature engineering, another round of modeling, and so on. Once the data analysis goals are achieved, typically the last steps are to finalize, document, and communicate the model. For predictive models, it is common at the end to validate the model on an additional set of data reserved for this specific purpose."
  },
  {
    "objectID": "model_spend.html#splitting-data",
    "href": "model_spend.html#splitting-data",
    "title": "28  Spending Your Data",
    "section": "28.1 Splitting Data",
    "text": "28.1 Splitting Data\nThe primary approach for empirical model validation is to split the existing pool of data into two distinct sets:\n\nThe training set. It is usually the majority of the data. These data are a sandbox for model building where different models can be fit, feature engineering strategies are investigated, and so on. As modeling practitioners, we spend the vast majority of the modeling process using the training set as the substrate to develop the model.\nThe test set. This is held in reserve until one or two models are chosen as the methods most likely to succeed. The test set is then used as the final arbiter to determine the efficacy of the model. More specifically, it is critical to:\n\nLook at the test set only once; otherwise, it becomes part of the modeling process.\nQuarantine the test set from any model building activities.\nMirror what the model would encounter in the wild. In other words, the test set should always resemble new data that will be given to the model.\n\n\nNote that the proportion of data that should be allocated for splitting is highly dependent on the context of the problem at hand. Too little data in the training set hampers the model’s ability to find appropriate parameter estimates. Conversely, too little data in the test set lowers the quality of the performance estimates. Bear in mind that keeping the training data in a separate data frame from the test set is one small check to make sure that information leakage does not occur by accident.\nSuppose we allocate 80% of the data to the training set and the remaining 20% for testing. The most common method is to use simple random sampling. The rsample package has tools for making data splits such as this; the function initial_split() was created for this purpose. It takes the data frame as an argument as well as the proportion to be placed into training.\nExample:\n\nlibrary(tidymodels)\ntidymodels_prefer()\n\n# Set the random number stream so that the results can be \n# reproduced later\nset.seed(501)\n\n# Save the split information for an 80/20 split of the data\names_split &lt;- initial_split(ames, prop = 0.80)\names_split\n\n&lt;Training/Testing/Total&gt;\n&lt;2344/586/2930&gt;\n\n\nThe printed information denotes the amount of data in the training set (n=2,344), the amount in the test set (n=586), and the size of the original pool of samples (n=2,930).\nThe object ames_split is an rsplit object and contains only the partitioning information; to get the resulting data sets, we apply two more functions:\n\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\ndim(ames_train)\n\n[1] 2344   74\n\n\nThese objects are data frames with the same columns as the original data but only the appropriate rows for each set.\n\n28.1.1 Stratified sampling\nSimple random sampling is appropriate in many cases but there are exceptions. When there is a dramatic class imbalance in classification problems, one class occurs much less frequently than another. Using a simple random sample may haphazardly allocate these infrequent samples disproportionately into the training or test set. To avoid this, stratified sampling can be used. The training/test split is conducted separately within each class and then these subsamples are combined into the overall training and test set.\nAs an example, a stratified random sample would conduct the 80/20 split within each of these data subsets and then pool the results. In rsample, this is achieved using the strata argument:\nExample:\n\nset.seed(502)\n\n# Only a single column can be used for stratification (e.g. Sale_Price)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\ndim(ames_train)\n\n[1] 2342   74\n\n\n\n\n28.1.2 Time series\nThere is one situation when random sampling is not the best choice: when the data have a significant time component, such as time series data. Here, it is more common to use the most recent data as the test set.\nThe rsample package contains a function called initial_time_split() that is very similar to initial_split(). Instead of using random sampling, the prop argument denotes what proportion of the first part of the data should be used as the training set; the function assumes that the data have been pre-sorted in an appropriate order.\n\n\n28.1.3 Validation set\nIn the neural network and deep learning literature it is common to hear about validation sets as an answer to the question about how we can tell what is the best test set that should be used to properly evaluate a model performance on the final model(s), if we don’t measure performance until the test set?\nDuring the early days of neural networks, researchers realized that measuring performance by re-predicting the training set samples led to results that were overly optimistic (significantly, unrealistically so). This led to models that overfit, meaning that they performed very well on the training set but poorly on the test set.12 To combat this issue, a small validation set of data were held back and used to measure performance as the network was trained. Once the validation set error rate began to rise, the training would be halted. In other words, the validation set was a means to get a rough sense of how well the model performed prior to the test set.\nValidation sets are a special case of resampling methods that are used on the training set.\nExample:\n\nset.seed(52)\n# To put 60% into training, 20% in validation, and 20% in testing:\names_val_split &lt;- initial_validation_split(ames, prop = c(0.6, 0.2))\names_val_split\n\n&lt;Training/Validation/Testing/Total&gt;\n&lt;1758/586/586/2930&gt;\n\n\nTo get the training, validation, and testing data, the same syntax is used:\n\names_train &lt;- training(ames_val_split)\names_test &lt;- testing(ames_val_split)\names_val &lt;- validation(ames_val_split)"
  },
  {
    "objectID": "model_fit.html#linear-regression-model",
    "href": "model_fit.html#linear-regression-model",
    "title": "29  Fitting Models",
    "section": "29.1 Linear Regression Model",
    "text": "29.1 Linear Regression Model\nSuppose that a linear regression model was our initial choice. This is equivalent to specifying that the outcome data is numeric and that the predictors are related to the outcome in terms of simple slopes and intercepts:\n\\[\n\\begin{aligned}\ny_i = β_0 + β_1 {x_1}_i +...+ β_p {x_p}_i\n\\end{aligned}\n\\]\nA variety of methods can be used to estimate the model parameters:\n\nOrdinary linear regression uses the traditional method of least squares to solve for the model parameters.\nIn R, the stats package can be used for the first case. The syntax for linear regression using the function lm() is:\nmodel &lt;- lm(formula, data, ...)\nRegularized linear regression adds a penalty to the least squares method to encourage simplicity by removing predictors and/or shrinking their coefficients towards zero. This can be executed using Bayesian or non-Bayesian techniques.\nA Bayesian model can be fit using the rstanarm package:\nmodel &lt;- stan_glm(formula, data, family = \"gaussian\", ...)\nA popular non-Bayesian approach to regularized regression is the glmnet model. Its syntax is:\nmodel &lt;- glmnet(x = matrix, y = vector, family = \"gaussian\", ...)\n\nNote that to fit models across different packages, the data must be formatted in different ways. lm() and stan_glm() only have formula interfaces while glmnet() does not. For a person trying to do data analysis, these differences require the memorization of each package’s syntax and can be very frustrating.\nFor tidymodels, the approach to specifying a model is intended to be more unified:\n\nSpecify the type of model based on its mathematical structure (e.g., linear regression, random forest, KNN, etc).\nSpecify the engine for fitting the model. Most often this reflects the software package that should be used, like Stan or glmnet. These are models in their own right, and parsnip provides consistent interfaces by using these as engines for modeling.\nWhen required, declare the mode of the model. The mode reflects the type of prediction outcome. For numeric outcomes, the mode is regression; for qualitative outcomes, it is classification. If a model algorithm can only address one type of prediction outcome, such as linear regression, the mode is already set. For example, for the three cases we outlined:\n\n\nlibrary(tidymodels)\ntidymodels_prefer()\n\nlinear_reg() |&gt; set_engine(\"lm\") |&gt; translate()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nModel fit template:\nstats::lm(formula = missing_arg(), data = missing_arg(), weights = missing_arg())\n\nlinear_reg(penalty = 1) |&gt; set_engine(\"glmnet\") |&gt; translate()\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1\n\nComputational engine: glmnet \n\nModel fit template:\nglmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    family = \"gaussian\")\n\nlinear_reg() |&gt; set_engine(\"stan\") |&gt; translate()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: stan \n\nModel fit template:\nrstanarm::stan_glm(formula = missing_arg(), data = missing_arg(), \n    weights = missing_arg(), family = stats::gaussian, refresh = 0)\n\n\nNote that missing_arg() is just a placeholder for the data that has yet to be provided.\n\n29.1.1 Example\nLet’s walk through how to predict the sale price of houses in the Ames data as a function of only longitude and latitude:\n\nlibrary(tidymodels)\nlibrary(modeldata)\ndata(ames)\ntidymodels_prefer()\n\n\n# Set the random number stream so that the results can be \n# reproduced later\nset.seed(501)\n\names_log10 &lt;- \n  ames |&gt;\n    mutate(Sale_Price = log10(Sale_Price))\n\n# Save the split information for an 80/20 split of the data\names_split &lt;- initial_split(ames_log10, prop = 0.80)\names_train &lt;- training(ames_split)\names_test  &lt;- testing(ames_split)\n\n\nlm_model &lt;- \n  linear_reg() %&gt;% \n  set_engine(\"lm\")\n\nlm_form_fit &lt;- \n  lm_model %&gt;% \n  # Recall that Sale_Price has been pre-logged\n  fit(Sale_Price ~ Longitude + Latitude, data = ames_train)\n\nlm_xy_fit &lt;- \n  lm_model %&gt;% \n  fit_xy(\n    x = ames_train %&gt;% select(Longitude, Latitude),\n    y = ames_train %&gt;% pull(Sale_Price)\n  )\n\nlm_form_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Sale_Price ~ Longitude + Latitude, data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n   -310.757       -2.055        2.938  \n\nlm_xy_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n   -310.757       -2.055        2.938"
  },
  {
    "objectID": "model_fit.html#using-the-model-results",
    "href": "model_fit.html#using-the-model-results",
    "title": "29  Fitting Models",
    "section": "29.2 Using the Model Results",
    "text": "29.2 Using the Model Results\nOnce the model is created and fit, we can use the results in a variety of ways; we might want to plot, print, or otherwise examine the model output. Several quantities are stored in a parsnip model object, including the fitted model. This can be found in an element called fit, which can be returned using the extract_fit_engine() function:\nExample:\n\nlm_form_fit %&gt;% extract_fit_engine()\n\n\nCall:\nstats::lm(formula = Sale_Price ~ Longitude + Latitude, data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n   -310.757       -2.055        2.938  \n\n\nNormal methods can be applied to this object, such as printing and plotting:\nExample:\n\nlm_form_fit %&gt;% extract_fit_engine() %&gt;% vcov()\n\n            (Intercept)     Longitude      Latitude\n(Intercept)  219.777464  1.6820856038 -1.4812181412\nLongitude      1.682086  0.0176858915 -0.0006168762\nLatitude      -1.481218 -0.0006168762  0.0338638634\n\n\nThe summary() method for lm objects can be used to print the results of the model fit, including a table with parameter values, their uncertainty estimates, and p-values.\n\nmodel_res &lt;- \n  lm_form_fit %&gt;% \n  extract_fit_engine() %&gt;% \n  summary()\n\nThe model coefficient table is accessible via the coef method.\nExample:\n\nparam_est &lt;- coef(model_res)\nparam_est\n\n               Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) -310.756920 14.8248934 -20.96183 1.492498e-89\nLongitude     -2.055314  0.1329883 -15.45485 2.211671e-51\nLatitude       2.938288  0.1840214  15.96710 1.479971e-54\n\nclass(param_est)\n\n[1] \"matrix\" \"array\" \n\n\nA next step might be to create a visualization of the parameter values. To do this, it would be sensible to convert the parameter matrix to a data frame. We could add the row names as a column so that they can be used in a plot. However, notice that several of the existing matrix column names would not be valid R column names for ordinary data frames (e.g., “Pr(&gt;|t|)”).\nAs a solution, the broom package can convert many types of model objects to a tidy structure. For example, using the tidy() method on the linear model produces:\n\ntidy(lm_form_fit)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  -311.      14.8       -21.0 1.49e-89\n2 Longitude      -2.06     0.133     -15.5 2.21e-51\n3 Latitude        2.94     0.184      16.0 1.48e-54\n\n\nThe column names are standardized across models and do not contain any additional data (such as the type of statistical test)."
  },
  {
    "objectID": "model_fit.html#make-predictions",
    "href": "model_fit.html#make-predictions",
    "title": "29  Fitting Models",
    "section": "29.3 Make Predictions",
    "text": "29.3 Make Predictions\nFor predictions, parsnip always conforms to the following rules:\n\nThe results are always a tibble.\nThe column names of the tibble are always predictable.\nThere are always as many rows in the tibble as there are in the input data set.\n\nFor example, when numeric data are predicted:\nExample:\n\names_test_small &lt;- ames_test %&gt;% slice(1:5)\npredict(lm_form_fit, new_data = ames_test_small)\n\n# A tibble: 5 × 1\n  .pred\n  &lt;dbl&gt;\n1  5.28\n2  5.28\n3  5.28\n4  5.24\n5  5.24\n\n\nNote that some tidyverse and tidymodels arguments and return values contain periods. This is to protect against merging data with duplicate names. There are some data sets that contain predictors named “pred!”.\nThese three rules make it easier to merge predictions with the original data:\n\names_test_small %&gt;% \n  select(Sale_Price) %&gt;% \n  bind_cols(predict(lm_form_fit, ames_test_small)) %&gt;% \n  # Add 95% prediction intervals to the results:\n  bind_cols(predict(lm_form_fit, ames_test_small, type = \"pred_int\")) \n\n# A tibble: 5 × 4\n  Sale_Price .pred .pred_lower .pred_upper\n       &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1       5.33  5.28        4.96        5.60\n2       5.37  5.28        4.96        5.60\n3       5.25  5.28        4.96        5.60\n4       5.18  5.24        4.92        5.56\n5       5.10  5.24        4.92        5.56"
  },
  {
    "objectID": "model_random.html#always-remember-to-set-your-seed",
    "href": "model_random.html#always-remember-to-set-your-seed",
    "title": "30  Random Numbers",
    "section": "30.1 Always Remember to Set Your Seed!",
    "text": "30.1 Always Remember to Set Your Seed!\nWhen simulating any random numbers it is essential to set the random number seed. Setting the random number seed with set.seed() ensures reproducibility of the sequence of random numbers.\nExample:\n\nset.seed(1)\nrnorm(5)\n\n[1] -0.6264538  0.1836433 -0.8356286  1.5952808  0.3295078\n\n\nExample: Simulating a Linear Model\nSuppose we want to simulate from the following linear model \\(y=β_0+β_1x+ε\\) where \\(ε ∼ N(0, 22)\\)\nAssume \\(x ∼ N(0, 12)\\), \\(β_0 = 0.5\\) and \\(β_1 = 2\\). The variable x might represent an important predictor of the outcome y.\nHere’s how you could do that in R:\n\n## Always set your seed! \nset.seed(20) \n## Simulate predictor variable \nx &lt;- rnorm(100) \n## Simulate the error term \ne &lt;- rnorm(100, 0, 2) \n## Compute the outcome via the model \ny &lt;- 0.5 + 2 * x + e \nsummary(y)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-6.4084 -1.5402  0.6789  0.6893  2.9303  6.5052 \n\nplot(x, y)"
  },
  {
    "objectID": "model_random.html#random-sampling",
    "href": "model_random.html#random-sampling",
    "title": "30  Random Numbers",
    "section": "30.2 Random Sampling",
    "text": "30.2 Random Sampling\nThe sample() function draws randomly from a specified set of (scalar) objects allowing you to sample from arbitrary distributions of numbers.\nExample:\n\nset.seed(1)\nsample(1:10,4)\n\n[1] 9 4 7 1\n\n# Doesn't have to be numbers\nsample(letters, 5)\n\n[1] \"b\" \"w\" \"k\" \"n\" \"r\"\n\n\nTo sample more complicated things, such as rows from a dataframe or a list, you can sample the indices into an object rather than the elements of the object itself:\n\nset.seed(20)\nidx &lt;- seq_len(nrow(airquality))## Create index vector \nsamp &lt;- sample(idx, 6) ##Sample from the index vector \nairquality[samp, ]\n\n    Ozone Solar.R Wind Temp Month Day\n107    NA      64 11.5   79     8  15\n120    76     203  9.7   97     8  28\n130    20     252 10.9   80     9   7\n98     66      NA  4.6   87     8   6\n29     45     252 14.9   81     5  29\n45     NA     332 13.8   80     6  14"
  },
  {
    "objectID": "time_flow.html",
    "href": "time_flow.html",
    "title": "31  TS Analysis Workflow",
    "section": "",
    "text": "Time series analysis is the process of extracting meaningful insights from time series data with the use of data visualization tools, statistical applications, and mathematical models. Those insights can be used to learn and explore past events and to forecast future events. The analysis process can be divided into the following steps:\n\nData collection: This step includes extracting data from different data sources, such as flat files (such as CSV, TXT, and XLMS), databases (for example, SQL Server, and Teradata), or other internet sources (such as academic resources and the Bureau of Statistics datasets).\nData preparation: In most cases, raw data is unstructured and may require cleaning, transformation, aggregation, and reformatting.\nDescriptive analysis: This is used in summary statistics and data visualization tools to extract insights from the data, such as patterns, distributions, cycles, and relationships with other drivers to learn more about past events.\nPredictive analysis: We use this to apply statistical methods in order to forecast future events, including forecasting strategies, forecasting with linear regression, forecasting with exponential smoothing models, forecasting with ARIMA models, and forecasting with machine learning models."
  },
  {
    "objectID": "time_objects.html#the-ts-class",
    "href": "time_objects.html#the-ts-class",
    "title": "32  TS Objects",
    "section": "32.1 The ts class",
    "text": "32.1 The ts class\nThe ts class is R’s built-in format for a regular univariate time series object.\nA regular time series is defined as an ordered sequence of observations over time, which is captured at equally spaced time intervals (that is, every day, week, month, and so on). Whenever this condition ceases to exist, the series becomes an irregular time series. We are specifically going to focus on the analysis and the forecasting of regular time series data.\nThe main characteristics of regular time series data is as follows:\n\nCycle/period: A regular and repeating unit of time that split the series into consecutive and equally long subsets (for example, for monthly series, a full cycle would be a year).\nFrequency: Defines the length or the number of units of the cycle (for example, for quarterly series, the frequency is four).\nTimestamp: Provides the time each observation in the series was captured, and can be used as the series index. By definition, in a regular time series the time difference (or delta) between two consecutive observations must be equal.\n\nA ts object is composed of two elements:\n\nthe series values and,\nits corresponding timestamp.\n\nIn addition, it also has several attributes, such as the series, cycle, frequency, and the time interval between each consecutive observation.\nAfter loading new data into the environment, it is always recommended that you check and verify whether the data structure is aligned with your expectations. A fast and recommended check to start with would be to verify the data class and length, which can be done with the is.ts and length functions:\nExample:\n# Test if the object is a \"ts\" class\nis.ts(NGC) \n\n# Get the number of observations\nlength(NGC) \nThe structure of the ts object is a bit different from most data structures in R (matrix, data.frame, data.table, tibble, and so on). While the ts object is a two-dimensional dataset (time/index and the series observations), it doesn’t share the common attributes of most of the regular tables in R (such as matrix or data.frame) since the series index is embedded within the object itself. Therefore, some of the common functions for R’s tables won’t work with a ts object (such as the dim function).\nA practical and concise method to get the characteristics of the series is with the frequency and deltat functions from the stats package, which provide the series frequency and the time interval between the observations:\nExample:\nfrequency(NGC)\ndeltat(NGC)\nOther useful utility functions are the start and end functions, which, as their names imply, return the series timestamp’s starting and ending point, respectively:\nExample:\nstart(NGC)\nend(NGC)\nThe ts_info function from the TSstudio package provides a concise summary of most of the preceding functions, including the object class, the number of observations, the frequency, and the starting and ending of the series.\nExample:\nlibrary(TSstudio)\nts_info(NGC)"
  },
  {
    "objectID": "time_objects.html#creating-a-ts-object",
    "href": "time_objects.html#creating-a-ts-object",
    "title": "32  TS Objects",
    "section": "32.2 Creating a ts Object",
    "text": "32.2 Creating a ts Object\nThe ts function from the stats package allows you to create a ts object by assigning sequential observations and mapping their attributes.\nExample:\n\nlibrary(TSstudio)\nmy_ts1 &lt;- ts(data = 1:60, # The series values\n            start = c(2010, 1), # The time of the first observation\n            end = c(2014, 12), # The time of the last observation\n            frequency = 12) # The Series frequency\n\n# Review the attributes of the new object\nts_info(my_ts1)\n\n The my_ts1 series is a ts object with 1 variable and 60 observations\n Frequency: 12 \n Start time: 2010 1 \n End time: 2014 12 \n\nmy_ts1\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n2010   1   2   3   4   5   6   7   8   9  10  11  12\n2011  13  14  15  16  17  18  19  20  21  22  23  24\n2012  25  26  27  28  29  30  31  32  33  34  35  36\n2013  37  38  39  40  41  42  43  44  45  46  47  48\n2014  49  50  51  52  53  54  55  56  57  58  59  60\n\n\nNote that when creating a new ts objecct it is sufficient to define either the start or end arguments, as the end or start value of the series can be derived from the length of the series and its frequency.\n\n32.2.1 Preprocessing steps (ts)\nIt is most likely that your raw data won’t come in ts format and some preprocessing steps may be required before transforming the input to a ts object. Those preprocessing steps may include the following:\n\nExporting the data from an external source, for example, an Excel or CSV file, SQL database, GitHub, and so on.\nReformatting the timestamp of the series or defining the series index.\nReordering the series according to its chronological order.\n\nExample:\n\ndata(US_indicators)\nstr(US_indicators)\n\n'data.frame':   528 obs. of  3 variables:\n $ Date             : Date, format: \"1976-01-31\" \"1976-02-29\" ...\n $ Vehicle Sales    : num  885 995 1244 1191 1203 ...\n $ Unemployment Rate: num  8.8 8.7 8.1 7.4 6.8 8 7.8 7.6 7.4 7.2 ...\n\n\n\nExtract the data. We are going to convert the vehicle sales series into a ts object, so we will extract the first two columns of the series and assign them to a new data frame named tvs, which denotes the total vehicle sales:\nExample:\n\ntvs &lt;- US_indicators[, 1:2]\nstr(tvs)\n\n'data.frame':   528 obs. of  2 variables:\n $ Date         : Date, format: \"1976-01-31\" \"1976-02-29\" ...\n $ Vehicle Sales: num  885 995 1244 1191 1203 ...\n\n\nArrange the data. Before converting the data frame, or generally any other R table format (such as tibble and data.table) to a ts object, you will need to arrange the data in chronological order.\nExample:\n\nlibrary(dplyr)\ntvs &lt;- tvs |&gt;\n        arrange(Date)\nhead(tvs)\n\n        Date Vehicle Sales\n1 1976-01-31         885.2\n2 1976-02-29         994.7\n3 1976-03-31        1243.6\n4 1976-04-30        1191.2\n5 1976-05-31        1203.2\n6 1976-06-30        1254.7\n\n\nSet the start (or end) of the series. Since the input series is monthly, we need to define the cycle (year) and frequency (month) units of the first observation of the series. This can be done manually, by observing the first date of the series and set the start point accordingly. In this case, the series started in January 1976, and therefore we can define it as start = c(1976, 1), or we can write a code to capture the starting point of the series.\nExample:\n\nlibrary(lubridate)\nstart_point &lt;- c(year(min(tvs$Date)),\n                  month(min(tvs$Date)))\nstart_point\n\n[1] 1976    1\n\n\nDefine the series with the ts function. Utilize the start_point variable to define the beginning of the series:\nExample:\n\ntvs_ts &lt;- ts(data = tvs$`Vehicle Sales`,\n              start = start_point,\n              frequency = 12)\nhead(tvs_ts)\n\n[1]  885.2  994.7 1243.6 1191.2 1203.2 1254.7"
  },
  {
    "objectID": "time_objects.html#multivariate-time-series-objects",
    "href": "time_objects.html#multivariate-time-series-objects",
    "title": "32  TS Objects",
    "section": "32.3 Multivariate time series objects",
    "text": "32.3 Multivariate time series objects\nIn some instances, such as correlation analysis, you may need to work with multivariate time series objects. In this case you can use the mts (multiple time series) class, an extension of the ts class. The mts class is a combination of the ts and matrix classes, and its advantage is that it shares the attributes of both those classes.\nExample:\n\nlibrary(TSstudio)\ndata(Coffee_Prices)\nts_info(Coffee_Prices)\n\n The Coffee_Prices series is a mts object with 2 variables and 701 observations\n Frequency: 12 \n Start time: 1960 1 \n End time: 2018 5 \n\nclass(Coffee_Prices)\n\n[1] \"mts\"    \"ts\"     \"matrix\"\n\nhead(Coffee_Prices)\n\n       Robusta Arabica\n[1,] 0.6968643  0.9409\n[2,] 0.6887074  0.9469\n[3,] 0.6887074  0.9281\n[4,] 0.6845187  0.9303\n[5,] 0.6906915  0.9200\n[6,] 0.6968643  0.9123\n\n\nYou can utilize and apply any designated function for a matrix object (when applicable) on an mts object. Similarly, the mts object also supports the ts objects functions, such as the frequency, time, start, or end functions.\nNote that two or more series can be merged to an mts object only if all the following prerequisites are met:\n\nAll inputs are regular time series.\nAll inputs have exactly the same frequency or time interval between the observations.\nAll inputs start at the same cycle number and cycle unit (unless missing values are filled with NAs).\n\n\n32.3.1 Creating a mts Object\nThe process of converting a multiple time series object from a data frame to ts format is fairly similar to that of the ts object.\n\nOrder the series by chronological order. Make sure that the series is sorted in chronological order with the arrange function:\nExample:\n\nUS_indicators &lt;- US_indicators |&gt;\n                    arrange(Date)\n\nCreate the ts object. Use the ts function to create the ts objects, where the input data includes two columns (as opposed to a single one for the ts object):\nExample:\n\nUS_indicators_ts &lt;- ts(data = US_indicators[, c(\"Vehicle Sales\", \"Unemployment Rate\")],\n                        start = c(year(min(tvs$Date)), month(min(tvs$Date))),\n                        frequency = 12)\n\nReview the transformed object. Use the ts_info and head functions:\n*Example:**\n\nts_info(US_indicators_ts)\n\n The US_indicators_ts series is a mts object with 2 variables and 528 observations\n Frequency: 12 \n Start time: 1976 1 \n End time: 2019 12 \n\nhead(US_indicators_ts)\n\n     Vehicle Sales Unemployment Rate\n[1,]         885.2               8.8\n[2,]         994.7               8.7\n[3,]        1243.6               8.1\n[4,]        1191.2               7.4\n[5,]        1203.2               6.8\n[6,]        1254.7               8.0\n\n\nSet the frequency. Setting the frequency impacts the structure of the ts object output. As an example we will use the seq.Date method to create a sequence of 3,650 days (or 365 times 10, which is 10 years excluding the extra day during leap years) starting on January 1, 2010. We will then use the rnorm function to generate the corresponding values of the series following a normal distribution with a mean of 15 and a standard deviation (sd) of 2:\nExample:\n\ndaily_df &lt;- data.frame(date = seq.Date(from = as.Date(\"2010-01-01\"),\nlength.out = 365 * 10, by = \"day\"),\ny = rnorm(365 * 10, mean = 15, sd = 2))\n\nstr(daily_df)\n\n'data.frame':   3650 obs. of  2 variables:\n $ date: Date, format: \"2010-01-01\" \"2010-01-02\" ...\n $ y   : num  14.2 14.3 15.2 16.9 13.8 ...\n\n\nWe will assign the first date of the series to a variable (start_date) and use it to set the start point of the series:\nExample:\n\nstart_date &lt;- min(daily_df$date)\n\nFor a daily series with weekly cycles, the frequency of the series should be set to seven (for example, Monday to Sunday) as can be seen in the following example:\nExample:\n\ndays_week_ts &lt;- ts(daily_df$y, \n                  start = c(1, wday(start_date)),\n                  frequency = 7)\n\nts_info(days_week_ts)\n\n The days_week_ts series is a ts object with 1 variable and 3650 observations\n Frequency: 7 \n Start time: 1 6 \n End time: 523 1"
  },
  {
    "objectID": "time_objects.html#data-manipulation-of-ts-objects",
    "href": "time_objects.html#data-manipulation-of-ts-objects",
    "title": "32  TS Objects",
    "section": "32.4 Data Manipulation of ts Objects",
    "text": "32.4 Data Manipulation of ts Objects\nOnce the series is transformed into a ts object, it might still be necessary to continue applying some additional transformation or preprocessing steps. These can include steps such as extracting or subsetting a specific element of the series or aggregating the series to a different frequency (for example, from monthly to quarterly).\nNote that due to the unique structure of the ts object, in most cases, the common extraction methods for data.frame do not apply to ts objects.\n\n32.4.1 The window function (stats)\nThe main purpose of the window function is to subset a ts object based on a time range.\nExample:\n# Extract all the observations of the year 2005\nwindow(NGC, start = c(2005,1), end = c(2005, 4))\nSimilarly, we can extract a specific frequency unit from the series. For example, let’s assume that we are interested in extracting all the observations of the series that occurred in the third quarter of the year.\nExample:\n# Extract observations from the third quarter of the year\nwindow(NGC, start = c(2000, 3), frequency = 1)\n\n# Extract the third quarter between 2006 and 2012\nwindow(NGC, start = c(2006, 3), end = c(2012, 3), frequency = 1)\n\n\n32.4.2 Aggregate ts objects (stats)\nThe aggregate function is a generic function for aggregating ts and `data.frame objects. This splits the data into subsets, computes specific summary statistics (based on the user’s choice), and then aggregates the results to a ts or data.frame object (depending on the input type).\nExample:\n# Transform the NGC series from a quarterly frequency to yearly\nNGC_year &lt;- aggregate(NGC, nfrequency = 1, FUN = \"sum\")\nHandling missing values, if these exist, can be done by using the na.action argument of the aggregate function, which, by default, ignores missing values.\n\n\n32.4.3 Creating lags and leads for ts objects (stats)\nThe use of lags in time series analysis is widespread because, typically, a time series is correlated with some of its previous lags.\nWe can generally distinguish between two types of lags:\n\nPast lags, or simply lags, represent a shift in the series by n steps back, with respect to the original series. For a series with t observations, the n lag of the series begins at time n+1 and ends at time t+n (where the first n observations are missing).\nExample:\nNGC_lag4 &lt;- stats::lag(NGC, k = -4) # Sets the number of lags or leads to be created\nLead (or negative lags) represent a shift in the series by n steps forward, with respect to the original series. In this case, for a series with t observations, the lead n of the series begins at the time n and end at time t-n (where the last n observations are missing)."
  },
  {
    "objectID": "time_objects.html#visualizing-time-series-objects",
    "href": "time_objects.html#visualizing-time-series-objects",
    "title": "32  TS Objects",
    "section": "32.5 Visualizing Time Series Objects",
    "text": "32.5 Visualizing Time Series Objects\nThere are two approaches for visualizing a time series object:\n\nDirect: This approach uses a visualization function to plot the object without any data transformation or conversion to another class. There are few packages that provide direct tools for visualizing time series objects:\n\nstats: In addition to the ts objects, this provides the plot.ts function for visualizing time series objects. This function is an extension of the plot function, which is an R built-in visualization function.\ndygraphs: This R package is an interface for the dygraphs JavaScript visualization library. It provides an interactive application for visualizing ts objects.\nTSstudio: This is a designated package for descriptive and predictive analysis of time series data. This includes rich and interactive visualization applications for time series objects, such as ts, mts, zoo, xts, and other table-based formats (such as data.frame and tibble). Like the previous package, the TSstudio package uses the plotly package engine.\n\nIn-direct: This approach involves applying some data transformation steps to restructure the data as a numeric two-dimensional structure (values over time). This includes the use of packages such as ggplot2, plotly, highcharter, and rbokeh.\n\nThe main difference between the direct and indirect approaches is that the former was designed to work with the ts objects and it therefore automatically transfers the value of the series and timestamp onto a y-axis and x-axis respectively. On the other hand, in the indirect approach, you will have to define those two dimensions (values versus time) manually.\n\n32.5.1 The plot.ts function\nThe plot.ts function from the stats package is built-in R function for visualizing of a ts object. Therefore, most of the arguments of the plot function (such as title and labeling options) can be used with the plot.ts function.\nExample:\n\nplot.ts(tvs_ts,\n        main = \"US Monthly Total Vehicle Sales\",\n        ylab = \"Thousands of Vehicle\",\n        xlab = \"Time\"\n)\n\n\n\n\nSimilarly, we can use the function to plot an mts object:\nExample:\n\nplot.ts(US_indicators_ts,\n        plot.type = \"multiple\",\n        main = \"US Monthly Vehicle Sales vs. Unemployment Rate\",\n        xlab = \"Time\")\n\n\n\n\n\n\n32.5.2 The dygraphs package\nThe dygraphs package is an R interface to the dygraphs JavaScript charting library, and it is completely dedicated to visualizing time series objects, including the ts class. It is highly customized, interactive, and supports HTML implementation (for example, the rmarkdown and Shiny packages).\nExxample:\n\nlibrary(dygraphs)\ndygraph(tvs_ts,\n        main = \"US Monthly Total Vehicle Sales\",\n        ylab = \"Thousands of Vehicle\") %&gt;%\n  dyRangeSelector()\n\n\n\n\n\nFor multiple objects we will use the US_indicators_ts series, adding a second y-axis, which allows us to plot and compare the two series that are not on the same scale (for example, thousands of units versus percentage):\nExample:\n\ndygraph(US_indicators_ts,\n        main = \"US Monthly Vehicle Sales vs. Unemployment Rate\") %&gt;%\n  dyAxis(\"y\", label = \"Vehicle Sales\") %&gt;%\n  dyAxis(\"y2\", label = \"Unemployment Rate\") %&gt;%\n  dySeries(\"Vehicle Sales\", axis = 'y', color = \"green\") %&gt;%\n  dySeries(\"Unemployment Rate\", axis = 'y2', color = \"red\") %&gt;%\n  dyLegend(width = 400)\n\n\n\n\n\nLooking at the plot of the US_indicator_ts series when using two y-axes, you can see that it is easier to identify the relationship between the two series as opposed to plotting them on a separate plot (as we did with the plot.ts function earlier).\nThe dygraph uses an object-oriented approach, which allows you to add a different component to the plot with the pipe operator (%&gt;%). This allows the user to add additional arguments for the plot, as opposed to a large amount of arguments in one function.\n\n\n32.5.3 TSstudio pacckage\nThe TSstudio package provides the ts_plot function for visualizing time series objects using the plotly package visualization engine. In addition, this function supports both time series objects, such as ts, mts, zoo, and xts, and also data frame types such as data.frame, data.table, and tibble.\nExample:\n\nlibrary(TSstudio)\nts_plot(tvs_ts,\n        title = \"US Monthly Total Vehicle Sales\",\n        Ytitle = \"Thousands of Vehicle\",\n        slider = TRUE # Add an interactive slider for the x-axis\n)\n\n\n\n\n\nSimilarly, the ts_plot function supports mts objects:\nExample:\n\nts_plot(US_indicators_ts,\n        title = \"US Monthly Vehicle Sales vs. Unemployment Rate\",\n        type = \"multiple\")"
  },
  {
    "objectID": "time_analysis.html#the-moving-average-function",
    "href": "time_analysis.html#the-moving-average-function",
    "title": "34  Decomposition of TS",
    "section": "34.1 The Moving Average Function",
    "text": "34.1 The Moving Average Function\nThe moving average (MA) is a simple function for smoothing time series data. This function is based on averaging each observation of a series, when applicable, with its surrounding observations, that is, with a past, future, or a combination of both past and future observations, in chronological order.\nThe output of this transformation process is a smoothed version of the original series.\nThe main components of the MA function are as follows:\n\nThe rolling window: This is a generic function that slides across the series in chronological order to extract sequential subsets\nAverage function: This is either a simple or weighted average, which is applied on each subset of the rolling window function.\n\n\n34.1.1 The rolling window function\nThe structure of the rolling window function defines the sub-setting method of series observations, thus playing a pivotal role in the smoothing of time series data process.\nThe most common types of window structures are as follows:\n\nThe one-sided window: This is a sliding window with a width of n, which groups each observation of the series (when applicable) with its past consecutive n-1 observations.\n\nFigure 11. Smoothing time series with moving average with one-sided rolling window\nThe output of the MA function (marked in green) is missing the first two observations (marked in red), which are considered to be the cost of the transformation process when using a one-sided rolling window.\nThe two-sided window: This is a rolling window with a width of n, which groups each observation of the series (when applicable) with its past n1 and future n2 observations. If n is an odd number and n1 is equal to n2, we can define the two-sided rolling window as centered.\n Figure 12. Smoothing time series with moving average with two-sided rolling window\nThe window function groups each observation with its preceding and leading observations. In this case, you cannot transform the first and last observations of the series (marked in red).\n\n\n\n34.1.2 The average function\nThere are two types of averaging methods:\n\nThe arithmetic (or simple) average: This is the most common and basic method for averaging a sequence of observations. It is based on summing all the observations and dividing them by the number of observations.\n\\[\\overline{Y} = \\frac{\\sum_{i=1}^{n}Y_i}{n}\\]\nThe weighted average: This method is based on applying weight to each observation of the series.\n\\[\\overline{Y_t} = {\\sum_{i=1}^{n}w_i Y_i}\\]\nWhere \\(w_i\\) represents the corresponding weight of the \\(i\\) observation of \\(Y\\). A weighted average should be used with time series data when there is a systematic relationship between some observations in the series based on their timestamp distance.\n\n\n\n34.1.3 Main applications of the MA function\nThe MA function has a variety of applications, such as data smoothing, noise reduction, and trend estimation. Also, with some small modifications, this function can be used as a forecasting model.\n\nNoise reduction: The use of the MA method creates a smoothing effect that reduces the series variation, smoothing the random noise and outliers.\nDe-seasonalize: In addition to the noise reduction, MAs can be used to remove the seasonal component (if any). This process has a pivotal role in the classical decomposing process, as it provides an estimation for the trend component, which is then utilized to estimate the seasonal component.\nForecasting: With some small modifications, the MA function can be used to forecast the future observations of the series by averaging the past observations to estimate future values of the series. The averaging method varies from a simple average to more advanced averaging methods.\n\n\n\n34.1.4 Calculating MAs\nTo calculate different types of MAs we will use the ts_ma function from the TSstudio package, but you can be sure that there is more than one package in R that provides an MA transformation function.\nThe ts_ma function from the TSstudio package allows us to generate and plot multiple MA outputs simultaneously, using different orders and methods (SMA, two-sided MA, and so on). The main parameters of this function are as follows:\n\nn: This sets the length of the past and future observations to be used in a two-sided MA function. For example, if n is set to three, the window function will group each observation with its past and future three consecutive observations, which will yield a 7 order MA. In addition, it is possible to set this parameter with multiple values in order to generate multiple two-sided MA functions simultaneously using a different order.\nn_left/n_right: These are used to customize the MA function by setting the length of the past (n_left) manually and/or the future (n_right) side of the window function. If both parameters are defined, the output is a two-sided MA function, either centered or uncentered. If only one of those parameters is set, the function output is a one-sided MA.\n\nExample:\n\nlibrary(TSstudio)\ndata(USVSales)\nts_info(USVSales)\n\n The USVSales series is a ts object with 1 variable and 528 observations\n Frequency: 12 \n Start time: 1976 1 \n End time: 2019 12 \n\n\n\nts_plot(USVSales,\n        title = \"US MOnthly Total Vehicle Sales\",\n        Ytitle = \"Thousands of Units\",\n        Xtitle = \"Years\",\n        Xgrid = TRUE,\n        Ygrid = TRUE)\n\n\n\n\n\n\ntwo_sided_ma &lt;- ts_ma(ts.obj = USVSales,\n                      n = c(2,5), # Sets an order 5 and 11 moving average\n                      n_left = 6, n_right = 5, # Sets an order 12 moving average\n                      plot = TRUE,\n                      multiple = TRUE,\n                      margin = 0.04)\n\n\nFigure 12. Three versions of two-sided MA outputs\nNote that the higher the order of the function, the smoother the output. Of course, the smoothing effect comes at the cost of losing observations from both the start and end of the series. The loss of observations is a function of the order of the function, since the higher the order of the MA function, the higher the loss of observations.\n\n\n34.1.5 One-Sided vs Two-Sided MAs\nThe selection of a specific type of MA method depends on the aim of the transformation. Moreover, the use of a different range of order may change the outcome significantly.Generally, when applying both simple and two-sided MAs with arithmetic average using the same order, the output of both methods will be identical but lagged.\nThe simple moving average (SMA)/One-sided MA is one of the common MA functions, and, as the name implies, it is also one of the simplest MA functions. This transformation method is based on applying an arithmetic average on a one-sided rolling window. Hence, the rolling window groups each observation in the series (when applicable) with its previous n consecutive observations in order to calculate their arithmetic average.\nThe two-sided MA method is based on a two-sided rolling window function, which groups each observation of the series with the past \\(n_1\\) and future \\(n_2\\) consecutive observations (as opposed to the SMA or one-sided MA, which use only past \\(n-1\\) observations). The term two-sided MA refers to the use of the two-sided window function with an arithmetic average.\nThe output of the two-sided MA function could be either of the following:\n\nCentered: This is when \\(n_1\\) and \\(n_2\\) are equal, which ensures that the function output at time t is centered around the t observation of the original y series.\nUncentered: This is when the length of \\(n_1\\) is different from the length of \\(n_2\\), or the order of the MA is an even number.\n\nThe selection of a specific type of MA method depends on the aim of the transformation. Moreover, the use of a different range of order, as we saw in the preceding examples, may change the outcome significantly. Generally, when applying both simple and two-sided MAs with arithmetic average using the same order, as in the following example, the output of both methods will be identical but lagged.\nExample:\n\n# Create one-sided and two-sided MA with an order of 12\none_sided_12 &lt;- ts_ma(USVSales, n = NULL, n_left = 11, plot = FALSE)\ntwo_sided_12 &lt;- ts_ma(USVSales, n = NULL, n_left = 6, n_right = 5,plot =\nFALSE)\none_sided &lt;- one_sided_12$unbalanced_ma_12\ntwo_sided &lt;- two_sided_12$unbalanced_ma_12\n\nWe will now bind the output of the one-sided and two-sided MA functions with the USVSales series and plot it with the ts_plot function:\n\nma &lt;- cbind(USVSales, one_sided, two_sided)\np &lt;- ts_plot(ma,\n          Xgrid = TRUE,\n          Ygrid = TRUE,\n          type = \"single\",\n          title = \"One-Sided vs. Two-Sided Moving Average - Order 12\")\n\nNext, we will use the layout function from the plotly package to set the plot legend and labels:\n\nlibrary(plotly)\np %&gt;% layout(legend = list(x = 0.05, y = 0.95),\n            yaxis = list(title = \"Thousands of Units\"),\n            xaxis = list(title = \"Year\"))\n\n\n\n\n\nConclusion:\n\nThe use of a centered two-sided MA function (or close to centered when the order of the function is even) is generally more appropriate to apply as a smoother or data filter method. As you can see, the output of the two-sided MA function fits better with the overall changes in the series trend with respect to the one-sided function output. This is mainly due to the fact that there is a delay of five periods between the two methods. this comes at the cost of losing the series’ last 5 observations in addition to the first 6 (as opposed to the last of the first 11 observations of the one-sided function). This makes the use of the two-sided function more expensive when you care about the most recent observations of the series.\nIt would make sense to use the one-sided MA function when you need to have the most recent observations (as the loss of observations, in this case, is from the beginning of the series)."
  },
  {
    "objectID": "time_analysis.html#calculating-mas",
    "href": "time_analysis.html#calculating-mas",
    "title": "34  The Decomposition of TS",
    "section": "34.2 Calculating MAs",
    "text": "34.2 Calculating MAs\nTo calculate different types of MAs we will use the ts_ma function from the TSstudio package, but you can be sure that there is more than one package in R that provides an MA transformation function.\nThe ts_ma function from the TSstudio package allows us to generate and plot multiple MA outputs simultaneously, using different orders and methods (SMA, two-sided MA, and so on). The main parameters of this function are as follows:\n\nn: This sets the length of the past and future observations to be used in a two-sided MA function. For example, if n is set to three, the window function will group each observation with its past and future three consecutive observations, which will yield a 7 order MA. In addition, it is possible to set this parameter with multiple values in order to generate multiple two-sided MA functions simultaneously using a different order.\nn_left/n_right: These are used to customize the MA function by setting the length of the past (n_left) manually and/or the future (n_right) side of the window function. If both parameters are defined, the output is a two-sided MA function, either centered or uncentered. If only one of those parameters is set, the function output is a one-sided MA.\n\nExample:\n\nlibrary(TSstudio)\ndata(USVSales)\nts_info(USVSales)\n\n The USVSales series is a ts object with 1 variable and 528 observations\n Frequency: 12 \n Start time: 1976 1 \n End time: 2019 12 \n\n\n\nts_plot(USVSales,\n        title = \"US MOnthly Total Vehicle Sales\",\n        Ytitle = \"Thousands of Units\",\n        Xtitle = \"Years\",\n        Xgrid = TRUE,\n        Ygrid = TRUE)\n\n\n\n\n\n\ntwo_sided_ma &lt;- ts_ma(ts.obj = USVSales,\n                      n = c(2,5), # Sets an order 5 and 11 moving average\n                      n_left = 6, n_right = 5, # Sets an order 12 moving average\n                      plot = TRUE,\n                      multiple = TRUE,\n                      margin = 0.04)\n\n\nFigure 12. Three versions of two-sided MA outputs\nNote that the higher the order of the function, the smoother the output. Of course, the smoothing effect comes at the cost of losing observations from both the start and end of the series. The loss of observations is a function of the order of the function, since the higher the order of the MA function, the higher the loss of observations.\n\n34.2.1 One-Sided vs Two-Sided MAs\nThe selection of a specific type of MA method depends on the aim of the transformation. Moreover, the use of a different range of order may change the outcome significantly.Generally, when applying both simple and two-sided MAs with arithmetic average using the same order, the output of both methods will be identical but lagged.\nThe simple moving average (SMA)/One-sided MA is one of the common MA functions, and, as the name implies, it is also one of the simplest MA functions. This transformation method is based on applying an arithmetic average on a one-sided rolling window. Hence, the rolling window groups each observation in the series (when applicable) with its previous n consecutive observations in order to calculate their arithmetic average.\nThe two-sided MA method is based on a two-sided rolling window function, which groups each observation of the series with the past \\(n_1\\) and future \\(n_2\\) consecutive observations (as opposed to the SMA or one-sided MA, which use only past \\(n-1\\) observations). The term two-sided MA refers to the use of the two-sided window function with an arithmetic average.\nThe output of the two-sided MA function could be either of the following:\n\nCentered: This is when \\(n_1\\) and \\(n_2\\) are equal, which ensures that the function output at time t is centered around the t observation of the original y series.\nUncentered: This is when the length of \\(n_1\\) is different from the length of \\(n_2\\), or the order of the MA is an even number.\n\nThe selection of a specific type of MA method depends on the aim of the transformation. Moreover, the use of a different range of order, as we saw in the preceding examples, may change the outcome significantly. Generally, when applying both simple and two-sided MAs with arithmetic average using the same order, as in the following example, the output of both methods will be identical but lagged.\nExample:\n\n# Create one-sided and two-sided MA with an order of 12\none_sided_12 &lt;- ts_ma(USVSales, n = NULL, n_left = 11, plot = FALSE)\ntwo_sided_12 &lt;- ts_ma(USVSales, n = NULL, n_left = 6, n_right = 5,plot =\nFALSE)\none_sided &lt;- one_sided_12$unbalanced_ma_12\ntwo_sided &lt;- two_sided_12$unbalanced_ma_12\n\nWe will now bind the output of the one-sided and two-sided MA functions with the USVSales series and plot it with the ts_plot function:\n\nma &lt;- cbind(USVSales, one_sided, two_sided)\np &lt;- ts_plot(ma,\n          Xgrid = TRUE,\n          Ygrid = TRUE,\n          type = \"single\",\n          title = \"One-Sided vs. Two-Sided Moving Average - Order 12\")\n\nNext, we will use the layout function from the plotly package to set the plot legend and labels:\n\nlibrary(plotly)\np %&gt;% layout(legend = list(x = 0.05, y = 0.95),\n            yaxis = list(title = \"Thousands of Units\"),\n            xaxis = list(title = \"Year\"))\n\n\n\n\n\nConclusion:\n\nThe use of a centered two-sided MA function (or close to centered when the order of the function is even) is generally more appropriate to apply as a smoother or data filter method. As you can see, the output of the two-sided MA function fits better with the overall changes in the series trend with respect to the one-sided function output. This is mainly due to the fact that there is a delay of five periods between the two methods. this comes at the cost of losing the series’ last 5 observations in addition to the first 6 (as opposed to the last of the first 11 observations of the one-sided function). This makes the use of the two-sided function more expensive when you care about the most recent observations of the series.\nIt would make sense to use the one-sided MA function when you need to have the most recent observations (as the loss of observations, in this case, is from the beginning of the series)."
  },
  {
    "objectID": "products.html#manipulate",
    "href": "products.html#manipulate",
    "title": "34  Building Data Products",
    "section": "34.1 Manipulate",
    "text": "34.1 Manipulate\nThe manipulate package creates a quick interactive graphic that offers simple controls, including sliders, pickers and checkboxes.\nlibrary(manipulate) \nmanipulate(plot(1:x), x = slider(1, 100))\nWith manipulate you can have more than one set of controls by simply adding more arguments to the manipulate function.\nNote that it’s difficult to share a manipulate interactive graphic.\nThis is a link on how manipulate actually works: link"
  },
  {
    "objectID": "products.html#shiny",
    "href": "products.html#shiny",
    "title": "34  Building Data Products",
    "section": "34.2 Shiny",
    "text": "34.2 Shiny\nIt is described by RStudio as “A web application framework for R”. “Turn your analyses into interactive web applications No HTML, CSS, or JavaScript knowledge required”.\nOnly those who have shiny installed and have access to your code could run your web page. However, RStudio offers a service for hosting shiny apps (their servers) on a platform called shinyapps.io.\nIf on Windows, make sure that you have Rtools installed. Then, you can install shiny with library(shiny):\nExample:\n\nlibrary(shiny)\ntextInput(\"name\", \"What is your name?\")\n\n\nWhat is your name?\n\n\n\nnumericInput(\"age\", \"How old are you?\", NA, min = 0, max = 150)\n\n\nHow old are you?\n\n\n\n\nA shiny app consists of two files. First, a file called ui.R that controls the User Interface (hence the ui in the filename) and secondly, a file server.R that controls the shiny server (hence the server in the filename).\n\n34.2.1 Create ui.R file:\nlibrary(shiny)\nshinyUI(\n  pageWithSidebar( \n    headerPanel(\"Hello Shiny!\"), \n    sidebarPanel( \n      h3('Sidebar text')\n    ),\n    mainPanel( \n      h3('Main Panel text')\n    )\n  )\n)\n\n\n34.2.2 Create server.R file:\nlibrary(shiny) \nshinyServer( \n  function(input, output) {\n  }\n)\nThe current version of Rstudio has a “Run app” button in the upper right hand corner of the editor if a ui.R or server.R file is open.\nTo get used to programming shiny apps, you need to throw away a little of your thinking about R; it’s a different style of programming.\n\n\n34.2.3 Types of inputs\n\nNumeric input:\nnumericInput('id1', 'Numeric input, labeled id1', 0, min = 0, max = 10, step= 1)\nCheckbox input:\ncheckboxGroupInput(\"id2\", \"Checkbox\", c(\"Value 1\" = \"1\", \"Value 2\" = \"2\", \"Value 3\" = \"3\"))\nDate input:\ndateInput(\"date\", \"Date:\")\n\n\n\n34.2.4 Sharing your app\nNow that we have a working app we’d like to share it with the world. It’s much nicer to have it display as a standalone web application.\nThis requires running a shiny server to host the app. Instead of creating and deploying our own shiny server, we’ll rely on RStudio’s service: link\n\nAfter login in shinyapps:\ninstall.packages(\"devtools\")\ninstall.packages(\"shinyapps\")\nRun code:\nshinyapps::setAccountInfo(name='&lt;ACCOUNT NAME&gt;', \n      token='&lt;TOKEN&gt;', \n      secret='&lt;SECRET&gt;')\nSubmit code:\ndeployApp(appName = \"myFirstApp\")\n\n\n\n34.2.5 Build a GUI with html\nCheck out this link"
  },
  {
    "objectID": "products.html#reproducible-presentations",
    "href": "products.html#reproducible-presentations",
    "title": "34  Building Data Products",
    "section": "34.3 Reproducible presentations",
    "text": "34.3 Reproducible presentations\n\n34.3.1 Slidify\nSlidify is for building reproducible presentations.\ninstall.packages(\"devtools\") \nlibrary(devtools)/ require(devtools)\ninstall_github('ramnathv/slidify')\ninstall_github('ramnathv'/'slidifyLibraries') o devtools::install_github(pkgs, force = TRUE)\nlibrary(slidify)\n\n\n34.3.2 R studio presenter\nFor more information about R studio presenter, chek out this link\nTip: if you’re sort of a hacker type and you like to tinker with things, use slidify. If you just want to get it done and not worry about it, use RPres. Either way, you really can’t go wrong."
  },
  {
    "objectID": "products.html#interactive-graphs",
    "href": "products.html#interactive-graphs",
    "title": "34  Building Data Products",
    "section": "34.4 Interactive Graphs",
    "text": "34.4 Interactive Graphs\n\n34.4.1 htmlwidgets\nHTML is an interactive format, and you can take advantage of that interactivity with htmlwidgets, R functions that produce interactive HTML visualizations. For example, take the leaflet map below, where you can drag the map around, zoom in and out, etc. You obviously can’t do that in a book, so Quarto automatically inserts a static screenshot for you.\nExample:\n\nlibrary(leaflet)\nleaflet() |&gt;\n  setView(-3.7077, 40.4156, , zoom = 16) |&gt; \n  addTiles() |&gt;\n  addMarkers(-3.7077, 40.4156, popup = \"Plaza Mayor de Madrid\") \n\n\n\n\n\nThere are many packages that provide htmlwidgets, including:\n\ndygraphs for interactive time series visualizations.\nDT for interactive tables.\nthreejs for interactive 3d plots.\nDiagrammeR for diagrams (like flow charts and simple node-link diagrams).\n\nTo see a complete list of the packages that provide htmlwidgets visit https://www.htmlwidgets.org.\n\n\n34.4.2 leaflet\nlink to leaflet\nleaflet seems to be emerging as the most popular R package for creating interactive maps.\nThe map widget (the leaflet() command) starts out a map and then you add elements or modify the map by passing it as arguments to mapping functions.\n\n\n34.4.3 rCharts\nlink to rCharts\nrCharts is a way to create interactive javascript visualizations using R.\nrequire(devtools) \ninstall_github('rCharts', 'ramnathv')\n\n\n34.4.4 googleVis\nlink to googlevis\nGoogle has some nice visualization tools built into their products (e.g. Google Maps). These include maps and interactive graphs.\ninstall.packages(\"googleVis\")\nlibrary(googlevis)\n\n\n34.4.5 plot.ly\nlink to plot.ly\nplotly relies on the platform/website plot.ly for creating interactive graphics.\nrequire(devtools) \ninstall_github(\"ropensci/plotly\")\nPlotly will give you the json data and gives a tremendous number of options for publishing the graph.\nNotably, plotly allows for integration with ggplot2.\nFor interactive graphics, learning some javascript and following that up with D3 would be the logical next step"
  },
  {
    "objectID": "product_repro.html#goal",
    "href": "product_repro.html#goal",
    "title": "35  Reproducible Reporting",
    "section": "35.1 Goal",
    "text": "35.1 Goal\nThe goal is to have independent people to do independent things with different data, different methods, and different laboratories and see if you get the same result. But the problem is that it’s becoming more and more challenging to do replication or to replicate other studies. Part of the reason is because studies are getting bigger and bigger.\nThe idea behind a reproducible reporting is to create a kind of minimum standard or a middle ground where we won’t be replicating a study, but maybe we can do something in between.\nYou need to make the data available for the original study and the computational methods available so that other people can look at your data and run the kind of analysis that you’ve run, and come to the same findings that you found. If you can take someone’s data and reproduce their findings, then you can, in some sense, validate the data analysis.\nUnderstanding what someone did in a data analysis now requires looking at code and scrutinizing the computer programs that people used."
  },
  {
    "objectID": "product_repro.html#the-data-science-pipeline",
    "href": "product_repro.html#the-data-science-pipeline",
    "title": "35  Reproducible Reporting",
    "section": "35.2 The Data Science Pipeline",
    "text": "35.2 The Data Science Pipeline\nThe basic idea behind reproducibility is to focus on the elements in the blue blox: the analytic data and the computational results.\n\n\n\nFigure 8: The Data Science Pipeline\n\n\nWith reproducibility the goal is to allow the author of a report and the reader of that report to “meet in the middle”."
  },
  {
    "objectID": "product_repro.html#organizing-a-data-analysis",
    "href": "product_repro.html#organizing-a-data-analysis",
    "title": "35  Reproducible Reporting",
    "section": "35.3 Organizing a Data Analysis",
    "text": "35.3 Organizing a Data Analysis\n\nRaw Data:\n\nYou want to store this raw data in your analysis folder.\nIf the data were accessed from the web you want to include things like the URL, where you got the data, what the data set is, a brief description of what it’s for, the date that you accessed the URL on, the website, etc.\nYou may want this in a README file.\nIf you’re using git to track things that are going on in your project, add your raw data, if possible. In the log message, when you add it you can talk about what the website was where you got it, what the URL was, etc.\n\nProcessed Data:\n\nYour processed data should be named so that you can easily see what script generated what data.\nIn any README file or any sort of documentation, it’s important to document what code files were used to transform the raw data into the processed data.\n\nFigures:\n\nExploratory figures.\nFinal figures. The final figures usually make a very small subset of the set of exploratory figures that you might generate. You typically don’t want to inundate people with a lot of figures because then the ultimate message of what you’re trying to communicate tends to get lost in a pile of figures.\n\nScripts:\n\nFinal scripts will be much more clearly commented. You’ll likely have bigger comment blocks for whole sections of code.\n\nR Markdown files:\n\nThey may not be exactly required, but they can be very useful to summarize parts of an analysis or an entire analysis.\nYou can embed code and text into the same document and then you process the document into something readable like a webpage or a PDF file.\n\nFinal Report:\n\nThe point of this is to tell the final story of what you generated here.\nYou’ll have a title, an introduction that motivates your problem, the methods that you used to refine, the results and any measures of uncertainty, and then any conclusions that you might draw from the data analysis that you did, including any pitfalls or potential problems."
  },
  {
    "objectID": "product_repro.html#structure-of-a-data-analysis",
    "href": "product_repro.html#structure-of-a-data-analysis",
    "title": "35  Reproducible Reporting",
    "section": "35.4 Structure of a Data Analysis",
    "text": "35.4 Structure of a Data Analysis\n1. Defining the question:\n\nA proper data analysis has a scientific context, and at least some general question that we’re trying to investigate which will narrow down the kind of dimensionality of the problem. Then we’ll apply the appropriate statistical methods to the appropriate data.\nDefining a question is the most powerful dimension reduction tool you can ever employ.\nThe idea is, if you can narrow down your question as specifically as possible, you’ll reduce the kind of noise that you’ll have to deal with when you’re going through a potentially very large data set.\nThink about what type of question you’re interested in answering before you go delving into all the details of your data set. That will lead you to the data. Which may lead you to applied statistics, which you use to analyze the data.\n\n2. Defining the ideal dataset/Determining what data you can access (the real data set):\n\nsometimes you have to go for something that is not quite the ideal data set.\nYou might be able to find free data on the web. You might need to buy some data from a provider.\nIf the data simply does not exist out there, you may need to generate the data yourself in some way.\n\n3. Obtaining the data:\n\nYou have to be careful to reference the source, so wherever you get the data from, you should always reference and keep track of where it came from.\nIf you get data from an Internet source, you should always make sure at the very minimum to record the URL, which is the web site indicator of where you got the data, and the time and date that you accessed it.\n\n4. Cleaning the data:\n\nRaw data typically needs to be processed in some way to get it into a form where you can model it or feed it into a modeling program.\nIf the data is already pre-processed, it’s important that you understand how it was done. Try to get some documentation about what the pre-processing was and how the sampling was done.\nIt is very important that anything you do to clean the data is recorded.\nOnce you have cleaned the data and you have gotten a basic look at it, it is important to determine if the data are good enough to solve your problems.\nIf you determine the data are not good enough for your question, then you’ve got to quit, try again, change the data, or try a different question. It is important to not simply push on with the data you have, just because that’s all that you’ve got, because that can lead to inappropriate inferences or conclusions.\n\n5. Exploratory data analysis:\n\nIt would be useful to look at what are the data, what did the data look like, what’s the distribution of the data, what are the relationships between the variables.\nYou want to look at basic summaries, one dimensional, two dimensional summaries of the data and we want to check for is there any missing data, why is there missing data, if there is, create some exploratory plots and do a little exploratory analyses. o Split the data set into Train and Test data sets:\nlibrary(kernlab) \ndata(spam)\nset.seed(3435) \ntrainIndicator = rbinom(4601, size = 1, prob = 0.5) table(trainIndicator)\ntrainSpam = spam[trainIndicator == 1, ] \ntestSpam = spam[trainIndicator == 0, ]\nWe can make some plots and we can compare, what are the frequencies of certain characteristics between the spam and the non spam emails:\nboxplot(capitalAve ~ type, data = trainSpam)\npairs(log10(trainSpam[, 1:4] + 1))   ## pairs plot of the first four variables\nYou can see that some of them are correlated, some of them are not particularly correlated, and that’s useful to know.\nExplore the predictors space a little bit more by doing a hierarchical cluster analysis, e.g. the Dendrogram just to see how what predictors or what words or characteristics tend to cluster together\nhCluster = hclust(dist(t(trainSpam[, 1:57]))) \nplot(hCluster)\n\n6. Statistical prediction/modeling:\n\nAny statistical modeling that you engage in should be informed by questions that you’re interested in, of course, and the results of any exploratory analysis. The exact methods that you employ will depend on the question of interest.\nwe’re just going to cycle through all the variables in this data set using this for-loop to build a logistic regression model, and then subsequently calculate the cross validated error rate of predicting spam emails from a single variable.\nOnce we’ve done this, we’re going to try to figure out which of the individual variables has the minimum cross validated error rate. It turns out that the predictor that has the minimum cross validated error rate is this variable called charDollar. This is an indicator of the number of dollar signs in the email.\nWe can actually make predictions now from the model on the test data (now we’re going to predict the outcome on the test data set to see how well we do).\nwe can take a look at the predicted values from our model, and then compare them with the actual values from the test data set, because we know which was spam, and which was not. Now we can just calculate the error rate.\n\n7. Interpretation of results:\n\nThink carefully about what kind of language you use to interpret your results. It’s also good to give an explanation for why certain models predict better than others, if possible.\nIf there are coefficients in the model that you need to interpret, you can do that here.\nAnd in particular it’s useful to bring in measures of uncertainty, to calibrate your interpretation of the final results.\n\n8. Challenging of results:\n\nIt’s good to challenge everything, the whole process by which you’ve gone through this problem. Is the question even a valid question to ask? Where did the data come from? How did you get the data? How did you process the data? How did you do the analysis and draw any conclusions?\nAnd if you built models, why is your model the best model? Why is it an appropriate model for this problem? How do you choose the things to include in your model?\nAll these things are questions that you should ask yourself and should have a reasonable answer to, so that when someone else asks you, you can respond in kind.\n\n9. Synthesis and write up:\n\nTypically in any data analysis, there are going to be many, many, many things that you did. And when you present them to another person or to a group you’re going to want to have winnowed it down to the most important aspects to tell a coherent story.\nTypically you want to lead with the question that you were trying to address.\nIt’s important that you don’t include every analysis that you ever did, but only if its needed for telling a coherent story. Talk about the analyses of your data set in the order that’s appropriate for the story you’re trying to tell.\nInclude very well done figures so that people can understand what you’re trying to say in one picture or two.\n\n10. Creating reproducible code:\n\nYou can use tools like RMarkdown and knitr and RStudio to document your analyses as you do them.\nYou can preserve the R code as well as any kind of a written summary of your analysis in a single document using knitr.\nIf someone cannot reproduce your data analysis then the conclusions that you draw will be not as worthy as an analysis where the results are reproducible."
  },
  {
    "objectID": "product_repro.html#r-markdown",
    "href": "product_repro.html#r-markdown",
    "title": "35  Reproducible Reporting",
    "section": "35.5 R Markdown",
    "text": "35.5 R Markdown\nlink to R Markdown guide\nThe benefit of Markdown for writers is that it allows one to focus on writing as opposed to formatting. It has simple and minimal yet intuitive formatting elements and can be easily converted to valid HTML (and other formats) using existing tools.\nR markdown is the integration of R code with Markdown. Documents written in R Markdown have R coded nested inside of them, which allows one to create documents containing “live” R code.\nR markdown can be converted to standard markdown using the knitr package in R. Markdown can subsequently be converted to HTML using the markdown package in R."
  },
  {
    "objectID": "product_repro.html#knitr",
    "href": "product_repro.html#knitr",
    "title": "35  Reproducible Reporting",
    "section": "35.6 Knitr",
    "text": "35.6 Knitr\nFor literate statistical programming, the idea is that a report is viewed as a stream of text and code.\nAnalysis code is divided into code chunks with text surrounding the code chunks explaining what is going on: - In general, literate programs are weaved to produce human-readable documents - and tangled to produce machine- readable documents\nThe requirements for writing literate programs are a documentation language (Markdown) and a programming language (R).\nMy First knitr Document:\n\nOpen an R Markdown document.\nRStudio will prompt you with a dialog box to set some of the metadata for the document.\nWhen you are ready to process and view your R Markdown document the easiest thing to do is click on the Knit HTML button that appears at the top of the editor window.\nNote here that the the code is echoed in the document in a grey background box and the output is shown just below it in a white background box. Notice also that the output is prepended with two pound symbols.\nCode chunks begin with {r} and end with just. Any R code that you include in a document must be contained within these delimiters, unless you have inline code.\nHiding code:\n{r pressure, echo=FALSE}\nHiding results:\n{r pressure, echo=FALSE, results = “hide”}\nRather than try to copy and paste the result into the paragraph, it’s better to just do the computation right there in the text:\nMy favourite random number is r rnorm(1)\nTables can be made in R Markdown documents with the help of the xtable package.\nThe opts_chunk variable sets an option that applies to all chunks in your document. For example, if we wanted the default to be that all chunks do NOT echo their code and always hide their results, we could set:\nknitr::opts_chunk$set(echo = FALSE, results = “hide”)\nGlobal options can always be overridden by any specific options that are set in at the chunk level:\n{r pressure, echo=FALSE, results = “asis”}\nChunk caching. If you have a long document or one involving lengthy computations, then every time you want to view your document in the pretty formatted version, you need to re-compile the document, meaning you need to re- run all the computations. Chunk caching is one way to avoid these lengthy computations.\ncache = TRUE\nIncluding a call to sessionInfo() at the end of each report written in R (perhaps with markdown or knitr) can be useful for communicating to the reader what type of environment is needed to reproduce the contents of the report."
  },
  {
    "objectID": "time_analysis.html#the-time-series-components",
    "href": "time_analysis.html#the-time-series-components",
    "title": "33  Decomposition of Time Series",
    "section": "33.3 The Time Series Components",
    "text": "33.3 The Time Series Components\nAs with other fields of statistics and, in particular, the field of machine learning, one of the primary goals of time series analysis is to identify patterns in data. Those patterns can then be utilized to provide meaningful insights about both past and future events such as seasonal, outliers, or unique events.\nPatterns in time series analysis can be categorized into one of the following:\n\nStructural patterns: These are also known as series components, which represent the core structure of the series. There are three types:\n\nStructural patterns—trend.\nCycle.\nSeasonal.\n\nYou can think about those patterns as binary events, which may or may not exist in the data. This helps to classify the series characteristics and identify the best approach to analyze the series.\nNon-structural: This is also known as the irregular component, and refers to any other types of patterns in the data that are not related to the structural patterns.\n\nWe can use these two groups of patterns (structural and non-structural) to express time series data using the following equation, when the series has an additive structure:\n\\[\\begin{align*}\nY_{t}=T_{t}+S_{t}+C_{t}+I_{t}\n\\end{align*}\\]\nAnd when the series has a multiplicative structure:\n\\[\\begin{align*}\nY_{t}=T_{t} \\times S_{t} \\times C_{t} \\times I_{t}\n\\end{align*}\\]\nWhere \\(Y_{t}\\) represents the series observation at time \\(t\\) and \\(T_{t}\\), \\(S_{t}\\), \\(C_{t}\\), and \\(I_{t}\\) represent the value of the trend, seasonal, cycle, and irregular components of the series at time \\(t\\), respectively."
  },
  {
    "objectID": "time_components.html#the-cycle-component",
    "href": "time_components.html#the-cycle-component",
    "title": "33  TS Components",
    "section": "33.1 The Cycle Component",
    "text": "33.1 The Cycle Component\nA cycle can be described as a sequence of repeatable events over time, where the starting point of a cycle is at a local minimum of the series and the ending point is at the next one, and the ending point of one cycle is the starting point of the following cycle. Unlike the seasonal pattern, cycles do not necessarily occur at equally spaced time intervals, and their length could change from cycle to cycle.\nExample:\n\nlibrary(TSstudio)\ndata(USUnRate)\nts_info(USUnRate)\n\n The USUnRate series is a ts object with 1 variable and 864 observations\n Frequency: 12 \n Start time: 1948 1 \n End time: 2019 12 \n\nunemployment &lt;- window(USUnRate, start = c(1990,1))\nts_plot(unemployment,\n    title = \"US Monthly Unemployment Rate\",\n    Ytitle = \"Unemployment Rate (%)\",\n    Xtitle = \"Year\",\n    Xgrid = TRUE,\n    Ygrid = TRUE)\n\n\n\n\n\nLooking at the preceding series plot, you can observe that the series has had three cycles since 1990 of different length:\n\nThe first cycle occurred between 1990 and 2000, which was close to an 11-year cycle.\nThe second cycle started in 2000 and ended in 2007, which was a 7-year cycle.\nA third cycle, which began in 2007 and as of May 2019 has not been completed yet, which means that this has continued for more than 12 years."
  },
  {
    "objectID": "time_components.html#the-trend-component",
    "href": "time_components.html#the-trend-component",
    "title": "33  TS Components",
    "section": "33.2 The Trend Component",
    "text": "33.2 The Trend Component\nA trend, if it exists in time series data, represents the general direction of the series, either up or down, over time. Furthermore, a trend could have either linear or exponential growth (or close to either one), depending on the series characteristics. To demonstrate this let’s start creating a non-trend series as our baseline data.\nExample:\n\n\n\n\n\n\nThese examples represent time series data with a clear trend component, and it is therefore simple to identify the trend and classify its growth type."
  },
  {
    "objectID": "time_components.html#the-seasonal-component",
    "href": "time_components.html#the-seasonal-component",
    "title": "33  TS Components",
    "section": "33.3 The Seasonal Component",
    "text": "33.3 The Seasonal Component\nThe seasonal component (or seasonality) is another common pattern in time series data. If it exists, it represents a repeated variation in the series, which is related to the frequency units of the series (for example, the months of the year for a monthly series).\nA common examples for a series with a strong seasonality pattern is the demand for electricity or natural gas. In those cases, the seasonal pattern is derived from a variety of seasonal events, such as weather patterns, the season of the year, and sunlight hours.\nExample:\n\n\n\n\n\n\nThe seasonal and cycle components both describe cyclic events over time, where the length of their cycle distinguish the two. The seasonal component has a constant cycle, which is derived and tied to the series frequency. On the other hand, the cycle length of the cycle component is not necessarily constant and can typically vary from one cycle to the next one. A simplistic way to identify whether a cycle pattern exists in a series is with the use of a heatmap for the time series data.\nExample:\n\nts_heatmap(USgas,\n    title = \"Heatmap - the US Natural Gas Consumption\")"
  },
  {
    "objectID": "time_components.html#the-irregular-component",
    "href": "time_components.html#the-irregular-component",
    "title": "33  TS Components",
    "section": "33.5 The Irregular Component",
    "text": "33.5 The Irregular Component\nThis component is the remainder between the series and structural components, and provides an indication of irregular events in the series. This includes non-systematic patterns or events in the data, which cause irregular fluctuation. In addition, the irregular component could provide some indication of the appropriate fit of the other components when using a decomposing method. A high correlation in this component is an indication that some patterns related to one of the other components were leftover due to an inaccurate estimate. On the other hand, if the irregular component is not correlated with its lags (that is, a white noise), we can assume (depending on the series structure) that the estimation of the trend and seasonal components captured the majority of the information about the series structure."
  },
  {
    "objectID": "time_components.html#the-additive-versus-the-multiplicative-model",
    "href": "time_components.html#the-additive-versus-the-multiplicative-model",
    "title": "33  TS Components",
    "section": "33.6 The Additive versus the Multiplicative Model",
    "text": "33.6 The Additive versus the Multiplicative Model\nThese terms describe the model structure. A model is defined as additive whenever we add together its components, namely, whenever there is a growth in the trend (with respect to the previous period), or if the amplitude of the seasonal component roughly remains the same over time:\n\\[\\begin{align*}\nY_{t}=T_{t}+S_{t}+C_{t}+I_{t}\n\\end{align*}\\]\nExample:\nThe US monthly natural gas consumption series is an example of an additive series. You can easily notice that the amplitude of the seasonal component remains the same (or close to the same) over time:\n\n\n\n\n\n\nAs you can see, the amplitude of the USgas series seasonal component over the past 20 years did not change by much (apart from some years, which may be related to some unusual weather patterns). In addition, the series trend seems to be linear, with some structural breaks during 2010.\nSimilarly, a model is defined as multiplicative whenever we multiply its components, namely, whenever the growth of the trend or the magnitude of the seasonal component increases or decreases by some multiplicity from period to period over time:\n\\[\\begin{align*}\nY_{t}=T_{t} \\times S_{t} \\times C_{t} \\times I_{t}\n\\end{align*}\\]\nExample:\nThe AirPassengers dataset (available in the dataset package), which describes the total monthly international airline passengers between the years 1949 and 1960, is an example for multiplicative series. During those years, right after World War II, the improvement in aviation technology contributed to the fast growth in the airline industry. As you can see in the following data, the amplitude of the seasonal component increases from year to year:\n\n\n\n\n\n\nThe typical approach for handling a series with a multiplicative structure is by applying a data transformation on the input series. The most common data transformation approaches for time series data are the following:\n\nLog transformation: This applies a log on both sides of the series equation. The new structure of the series allows us to treat it as a normal additive series:\n\n\\[\\begin{align*}\nlog(Y_{t})=log(T_{t}) + log(S_{t}) + log(C_{t}) + log(I_{t})\n\\end{align*}\\]\n\nBox-Cox transformation: This is based on applying power on the input series using the following formula:\n\n\\[\\begin{align*}\nY^{'}_{t}|\\lambda =\n  \\begin{cases}\n      \\frac{Y^{\\lambda}_{t} -1}{\\lambda} & \\lambda \\neq0 \\\\\n      log(Y_{t})& \\lambda=0\n  \\end{cases}\n\\end{align*}\\]\nAs you can see from the preceding Box-Cox equation, for \\(\\lambda=0\\), the transformation is a log transformation.\nThe forecast package provides several tools for applying a Box-Cox transformation on time series data. The BoxCox.lambda function estimates the value of \\(\\lambda\\), which minimizes the coefficient variation of the input series.\nExample:\nWe will use BoxCox.lambda to identify the \\(\\lambda\\) value for the AirPassenger series:\n\nlibrary(forecast)\nAirPassenger_lambda &lt;- BoxCox.lambda(AirPassengers)\nAirPassenger_lambda\n\n[1] -0.2947156\n\n\nWe can then use this \\(\\lambda\\) value to transform the input series with the BoxCox function and lot it with the ts_plot function:\nExample:\n\nAirPassenger_transform &lt;- BoxCox(AirPassengers, lambda =\n                      AirPassenger_lambda)\nts_plot(AirPassenger_transform,\n      title = \"Monthly Airline Passenger Numbers 1949-1960 with Box-Cox Transformation\",\n      Ytitle = \"Number of Passengers - Scaled\",\n      Xtitle = \"Years\",\n      Xgrid = TRUE,\n      Ygrid = TRUE)\n\n\n\n\n\nAs you can see from the transformation plot of the AirPassenger series, the values of the series are scaled. Most of the forecasting models in the forecast package automatically transform the series before applying the model, and then re-transform the forecast output back to the original scale."
  },
  {
    "objectID": "time_components.html#white-noise",
    "href": "time_components.html#white-noise",
    "title": "33  TS Components",
    "section": "33.4 White Noise",
    "text": "33.4 White Noise\nA series is defined as white noise when there is no correlation between the series observations or patterns. In other words, the relationship between different observations is random. Typically, unless mentioned otherwise, we assume that white noise is an independent and identically distributed random.\nExample:\n\n\n\n\n\n\nThere are a few methods for testing whether a time series is white noise:\n\nThe basic method is carried out by plotting and eyeballing the series to identify whether the variation of the series appears to be random or not.\nWe can measure the correlation between the series and its lags with the autocorrelation function (ACF). A series is considered to be white noise whenever there is no correlation between the series and its lag. The acf function from the stats package calculates the level of correlation between a series and its lags.\nThe Ljung-Box test is another statistical test to evaluate whether the series is correlated with its lags. In this case, the null hypothesis assumes that the lags are not correlated. Therefore, lags with lower p-values (with respect to the level of significance of the test) would be considered as being correlated with the series. The Box.testfunction, another stats package function, performs a Ljung-Box test on a series and a specific lag.\nExample:\n\nlibrary(dplyr)\nx &lt;- lapply(1:24, function(i){\n          p &lt;- Box.test(white_noise, lag = i, type = \"Ljung-Box\")\n          output &lt;- data.frame(lag = i, p_value = p$p.value)\n          return(output) }) %&gt;% bind_rows\nplot(x = x$lag,\n    y = x$p_value, ylim = c(0,1),\n    main = \"Series white_noise - Ljung-Box Test\",\n    xlab = \"Lag\", ylab = \"P-Value\")\nabline(h = 0.05, col=\"red\", lwd=3, lty=2)\n\n\n\n\nYou can see in the preceding Ljung-Box test summary that the p-value of all the lags is above the red dotted line, which indicates that we failed to reject the null hypothesis for a level of significance of 0.05. This indicates that the series is not correlated with its first 24 lags and is, therefore, a white noise series."
  },
  {
    "objectID": "time_analysis.html#seasonal-decomposition",
    "href": "time_analysis.html#seasonal-decomposition",
    "title": "34  The Decomposition of TS",
    "section": "34.2 Seasonal Decomposition",
    "text": "34.2 Seasonal Decomposition"
  },
  {
    "objectID": "time_analysis.html#classical-seasonal-decomposition",
    "href": "time_analysis.html#classical-seasonal-decomposition",
    "title": "34  Decomposition of TS",
    "section": "34.2 Classical Seasonal Decomposition",
    "text": "34.2 Classical Seasonal Decomposition\nClassical decomposition (or classical seasonal decomposition by MA) is one of the most common methods of decomposing a time series down to its components. This method for estimating the three components is based on the use of an MA function followed by simple arithmetic calculations.\nThis is a three-step process, where each step is dedicated to the estimation of one of the components in sequential order (hence, the calculation of each component is derived from the estimate of the previous component):\n\nTrend estimation: This is the first step of the decomposing process, by using the MA function to remove the seasonal component from the series. The order of the MA function is defined by the frequency of the series. For instance, if the frequency of the input series is monthly (or 12), then the order of the MA should be set to 12. Since we are using a two-sided MA, some of the first and last observations of the trend estimation will be missing. You may recall that the loss of observations in this process depends on the order of the MA function.\nSeasonal component estimation: A two-step process, starting with detrending the series by subtracting the trend estimation from the previous step from the series, where:\nIrregular component estimation: This is a straightforward calculation, subtracting the estimation of the trend and seasonal components from the original series. This is for an additive series, and for a multiplicative series. Here, Yt represents the original series at time t, and , , and represent the corresponding trend and seasonal and irregular components estimate."
  },
  {
    "objectID": "time_decomposition.html#the-moving-average-function",
    "href": "time_decomposition.html#the-moving-average-function",
    "title": "34  Decomposition of TS",
    "section": "34.1 The Moving Average Function",
    "text": "34.1 The Moving Average Function\nThe moving average (MA) is a simple function for smoothing time series data. This function is based on averaging each observation of a series, when applicable, with its surrounding observations, that is, with a past, future, or a combination of both past and future observations, in chronological order.\nThe output of this transformation process is a smoothed version of the original series.\nThe main components of the MA function are as follows:\n\nThe rolling window: This is a generic function that slides across the series in chronological order to extract sequential subsets\nAverage function: This is either a simple or weighted average, which is applied on each subset of the rolling window function.\n\n\n34.1.1 The rolling window function\nThe structure of the rolling window function defines the sub-setting method of series observations, thus playing a pivotal role in the smoothing of time series data process.\nThe most common types of window structures are as follows:\n\nThe one-sided window: This is a sliding window with a width of n, which groups each observation of the series (when applicable) with its past consecutive n-1 observations.\n\nFigure 11. Smoothing time series with moving average with one-sided rolling window\nThe output of the MA function (marked in green) is missing the first two observations (marked in red), which are considered to be the cost of the transformation process when using a one-sided rolling window.\nThe two-sided window: This is a rolling window with a width of n, which groups each observation of the series (when applicable) with its past n1 and future n2 observations. If n is an odd number and n1 is equal to n2, we can define the two-sided rolling window as centered.\n Figure 12. Smoothing time series with moving average with two-sided rolling window\nThe window function groups each observation with its preceding and leading observations. In this case, you cannot transform the first and last observations of the series (marked in red).\n\n\n\n34.1.2 The average function\nThere are two types of averaging methods:\n\nThe arithmetic (or simple) average: This is the most common and basic method for averaging a sequence of observations. It is based on summing all the observations and dividing them by the number of observations.\n\\[\\overline{Y} = \\frac{\\sum_{i=1}^{n}Y_i}{n}\\]\nThe weighted average: This method is based on applying weight to each observation of the series.\n\\[\\overline{Y_t} = {\\sum_{i=1}^{n}w_i Y_i}\\]\nWhere \\(w_i\\) represents the corresponding weight of the \\(i\\) observation of \\(Y\\). A weighted average should be used with time series data when there is a systematic relationship between some observations in the series based on their timestamp distance.\n\n\n\n34.1.3 Main applications of the MA function\nThe MA function has a variety of applications, such as data smoothing, noise reduction, and trend estimation. Also, with some small modifications, this function can be used as a forecasting model.\n\nNoise reduction: The use of the MA method creates a smoothing effect that reduces the series variation, smoothing the random noise and outliers.\nDe-seasonalize: In addition to the noise reduction, MAs can be used to remove the seasonal component (if any). This process has a pivotal role in the classical decomposing process, as it provides an estimation for the trend component, which is then utilized to estimate the seasonal component.\nForecasting: With some small modifications, the MA function can be used to forecast the future observations of the series by averaging the past observations to estimate future values of the series. The averaging method varies from a simple average to more advanced averaging methods.\n\n\n\n34.1.4 Calculating MAs\nTo calculate different types of MAs we will use the ts_ma function from the TSstudio package, but you can be sure that there is more than one package in R that provides an MA transformation function.\nThe ts_ma function from the TSstudio package allows us to generate and plot multiple MA outputs simultaneously, using different orders and methods (SMA, two-sided MA, and so on). The main parameters of this function are as follows:\n\nn: This sets the length of the past and future observations to be used in a two-sided MA function. For example, if n is set to three, the window function will group each observation with its past and future three consecutive observations, which will yield a 7 order MA. In addition, it is possible to set this parameter with multiple values in order to generate multiple two-sided MA functions simultaneously using a different order.\nn_left/n_right: These are used to customize the MA function by setting the length of the past (n_left) manually and/or the future (n_right) side of the window function. If both parameters are defined, the output is a two-sided MA function, either centered or uncentered. If only one of those parameters is set, the function output is a one-sided MA.\n\nExample:\n\nlibrary(TSstudio)\ndata(USVSales)\nts_info(USVSales)\n\n The USVSales series is a ts object with 1 variable and 528 observations\n Frequency: 12 \n Start time: 1976 1 \n End time: 2019 12 \n\n\n\nts_plot(USVSales,\n        title = \"US MOnthly Total Vehicle Sales\",\n        Ytitle = \"Thousands of Units\",\n        Xtitle = \"Years\",\n        Xgrid = TRUE,\n        Ygrid = TRUE)\n\n\n\n\n\n\ntwo_sided_ma &lt;- ts_ma(ts.obj = USVSales,\n                      n = c(2,5), # Sets an order 5 and 11 moving average\n                      n_left = 6, n_right = 5, # Sets an order 12 moving average\n                      plot = TRUE,\n                      multiple = TRUE,\n                      margin = 0.04)\n\n\nFigure 12. Three versions of two-sided MA outputs\nNote that the higher the order of the function, the smoother the output. Of course, the smoothing effect comes at the cost of losing observations from both the start and end of the series. The loss of observations is a function of the order of the function, since the higher the order of the MA function, the higher the loss of observations.\n\n\n34.1.5 One-Sided vs Two-Sided MAs\nThe selection of a specific type of MA method depends on the aim of the transformation. Moreover, the use of a different range of order may change the outcome significantly.Generally, when applying both simple and two-sided MAs with arithmetic average using the same order, the output of both methods will be identical but lagged.\nThe simple moving average (SMA)/One-sided MA is one of the common MA functions, and, as the name implies, it is also one of the simplest MA functions. This transformation method is based on applying an arithmetic average on a one-sided rolling window. Hence, the rolling window groups each observation in the series (when applicable) with its previous n consecutive observations in order to calculate their arithmetic average.\nThe two-sided MA method is based on a two-sided rolling window function, which groups each observation of the series with the past \\(n_1\\) and future \\(n_2\\) consecutive observations (as opposed to the SMA or one-sided MA, which use only past \\(n-1\\) observations). The term two-sided MA refers to the use of the two-sided window function with an arithmetic average.\nThe output of the two-sided MA function could be either of the following:\n\nCentered: This is when \\(n_1\\) and \\(n_2\\) are equal, which ensures that the function output at time t is centered around the t observation of the original y series.\nUncentered: This is when the length of \\(n_1\\) is different from the length of \\(n_2\\), or the order of the MA is an even number.\n\nThe selection of a specific type of MA method depends on the aim of the transformation. Moreover, the use of a different range of order, as we saw in the preceding examples, may change the outcome significantly. Generally, when applying both simple and two-sided MAs with arithmetic average using the same order, as in the following example, the output of both methods will be identical but lagged.\nExample:\n\n# Create one-sided and two-sided MA with an order of 12\none_sided_12 &lt;- ts_ma(USVSales, n = NULL, n_left = 11, plot = FALSE)\ntwo_sided_12 &lt;- ts_ma(USVSales, n = NULL, n_left = 6, n_right = 5,plot =\nFALSE)\none_sided &lt;- one_sided_12$unbalanced_ma_12\ntwo_sided &lt;- two_sided_12$unbalanced_ma_12\n\nWe will now bind the output of the one-sided and two-sided MA functions with the USVSales series and plot it with the ts_plot function:\n\nma &lt;- cbind(USVSales, one_sided, two_sided)\np &lt;- ts_plot(ma,\n          Xgrid = TRUE,\n          Ygrid = TRUE,\n          type = \"single\",\n          title = \"One-Sided vs. Two-Sided Moving Average - Order 12\")\n\nNext, we will use the layout function from the plotly package to set the plot legend and labels:\n\nlibrary(plotly)\np %&gt;% layout(legend = list(x = 0.05, y = 0.95),\n            yaxis = list(title = \"Thousands of Units\"),\n            xaxis = list(title = \"Year\"))\n\n\n\n\n\nConclusion:\n\nThe use of a centered two-sided MA function (or close to centered when the order of the function is even) is generally more appropriate to apply as a smoother or data filter method. As you can see, the output of the two-sided MA function fits better with the overall changes in the series trend with respect to the one-sided function output. This is mainly due to the fact that there is a delay of five periods between the two methods. this comes at the cost of losing the series’ last 5 observations in addition to the first 6 (as opposed to the last of the first 11 observations of the one-sided function). This makes the use of the two-sided function more expensive when you care about the most recent observations of the series.\nIt would make sense to use the one-sided MA function when you need to have the most recent observations (as the loss of observations, in this case, is from the beginning of the series)."
  },
  {
    "objectID": "time_decomposition.html#classical-seasonal-decomposition",
    "href": "time_decomposition.html#classical-seasonal-decomposition",
    "title": "34  Decomposition of TS",
    "section": "34.2 Classical Seasonal Decomposition",
    "text": "34.2 Classical Seasonal Decomposition\nClassical decomposition (or classical seasonal decomposition by MA) is one of the most common methods of decomposing a time series down to its components. This method for estimating the three components is based on the use of an MA function followed by simple arithmetic calculations.\nThis is a three-step process, where each step is dedicated to the estimation of one of the components in sequential order (hence, the calculation of each component is derived from the estimate of the previous component):\n\nTrend estimation: This is the first step of the decomposing process, by using the MA function to remove the seasonal component from the series. The order of the MA function is defined by the frequency of the series. For instance, if the frequency of the input series is monthly (or 12), then the order of the MA should be set to 12. Since we are using a two-sided MA, some of the first and last observations of the trend estimation will be missing.\nSeasonal component estimation: A two-step process, starting with de-trending the series by subtracting the trend estimation from the previous step from the series, where:\n\nYou can use \\(Y_t - {\\hat T_t}\\) when using the additive model, and \\(\\frac {Y_t}{\\hat T_t}\\) when the model is multiplicative. Here, \\(Y_t\\) is the original series observation at time \\(t\\) and \\(\\hat {T_t}\\) is the corresponding trend estimation.\nAfter the series is detrended, the next step is to estimate the corresponding seasonal component for each frequency unit (for example, for a monthly series, the seasonal component for January, February, and so on). This calculation is done by grouping the observations by their frequency unit and then averaging each group. The output of this process is a new series with a length that is equal to the series frequency and is ordered accordingly. This series represents the seasonal component of each frequency unit, so this estimation is one-to-many (one estimation for multiple observations).\n\nIrregular component estimation: This is about subtracting the estimation of the trend and seasonal components from the original series. This is \\(\\hat{I_t} = {Y_t} - \\hat{T_t} - \\hat{S_t}\\) for an additive series, and \\(\\hat{I_t} = \\frac {Y_t}{({\\hat T_t} \\times {\\hat S_t})}\\) for a multiplicative series. Here, \\(Y_t\\) represents the original series at time \\(t\\), and \\(\\hat{T_t}\\), \\(\\hat{S_t}\\), and \\(\\hat{I_t}\\) represent the corresponding trend and seasonal and irregular components estimate.\n\nThe decompose function from the stats package implements this method. By default, this function is set to an additive model.\nExample:\n\ndata(USVSales)\nusv_decompose &lt;- decompose(USVSales)\nstr(usv_decompose)\n\nList of 6\n $ x       : Time-Series [1:528] from 1976 to 2020: 885 995 1244 1191 1203 ...\n $ seasonal: Time-Series [1:528] from 1976 to 2020: -225.2 -102.4 143 34.5 147.9 ...\n $ trend   : Time-Series [1:528] from 1976 to 2020: NA NA NA NA NA ...\n $ random  : Time-Series [1:528] from 1976 to 2020: NA NA NA NA NA ...\n $ figure  : num [1:12] -225.2 -102.4 143 34.5 147.9 ...\n $ type    : chr \"additive\"\n - attr(*, \"class\")= chr \"decomposed.ts\"\n\nclass(usv_decompose)\n\n[1] \"decomposed.ts\"\n\n\nFrom the preceding output you can see that the function returns a list of six objects:\n\nx: This is the original series, a ts object.\nseasonal: This is the estimate of the seasonal component, a ts object.\ntrend: This is the estimate of the series trend, a ts object. You can see that the first (and also the last) observations are missing due to the use of the two-sided MA function. The number of missing values is defined by the order of the MA function.\nrandom: This is the estimate of the irregular component, a ts object. This output is nothing but the remainder of the series and the preceding two components. The random object is missing whenever the trend estimation is missing.\nfigure: This is the estimated seasonal figure only.\ntype: This is the type of decomposition, either an additive (the default), or multiplicative model.\n\nNow we can plot the output of the decompose function as plotsupports the decomposed.ts class.\nExample:\n\nplot(usv_decompose)\n\n\n\n\nSimilarly, if the series has multiplicative growth, like the AirPassengers series, you can set the decomposition process with a multiplicative model:\nExample:\n\nair_decompose &lt;- decompose(AirPassengers, type = \"multiplicative\")\nplot(air_decompose)\n\n\n\n\nOne of the downsides of the classical decomposition method is that seasonal component estimation is based on the arithmetic average, which results in a one-to-many estimation, so there is a single seasonal component estimation for each cycle unit (for example, all observations of the series that occurred in January will have the same seasonal component estimation if the series is monthly). This is not problematic when applying this method to an additive series, such as the US vehicle sales or the monthly natural gas consumption datasets, as the magnitude of the seasonal oscillation remains the same (or close to the same) over time. However, this is not the case for a multiplicative series, as the magnitude of the seasonal oscillation grows over time."
  },
  {
    "objectID": "time_seasonality.html",
    "href": "time_seasonality.html",
    "title": "35  Seasonality Analysis",
    "section": "",
    "text": "Seasonality is one of the main components of time series data. Furthermore, this component, when existing in a series, plays a pivotal role in the forecasting process of the future values of the series, since it contains structural patterns.\nWhen seasonality exists in the time series data, we can classify it into one of the following categories:\n\nSingle seasonal pattern: Whenever there is only one dominant seasonal pattern in the series. As the frequency of the series is lower (for example, monthly, quarterly, and so on), it is more likely to have only one dominant seasonal pattern as opposed to a high-frequency series, as there are fewer aggregation options for another type of frequencies.\nMultiple seasonal patterns: If more than one dominant seasonal pattern exists in the series. This type of patterns are more likely to occur whenever the series has a high frequency (for example, daily, hourly, half-hourly, and so on), as there are more options to aggregate the series to a lower frequency. A typical example of multiple seasonality is the hourly demand for electricity, which could have multiple seasonal patterns, as the demand is derived from the hour of the day, the day of the week, or the yearly patterns, such as weather or the amount of daylight throughout the day.\n\nExample:\n\nlibrary(TSstudio)\ndata(USgas)\nts_info(USgas)\n\n The USgas series is a ts object with 1 variable and 238 observations\n Frequency: 12 \n Start time: 2000 1 \n End time: 2019 10"
  },
  {
    "objectID": "time_seasonality.html#type-of-seasonality",
    "href": "time_seasonality.html#type-of-seasonality",
    "title": "35  Seasonality Analysis",
    "section": "35.1 Type of Seasonality",
    "text": "35.1 Type of Seasonality\nWhen seasonality exists in the time series data, we can classify it into one of the following categories:\n\nSingle seasonal pattern: Whenever there is only one dominant seasonal pattern in the series. As the frequency of the series is lower (for example, monthly, quarterly, and so on), it is more likely to have only one dominant seasonal pattern as opposed to a high-frequency series, as there are fewer aggregation options for another type of frequencies.\nMultiple seasonal patterns: If more than one dominant seasonal pattern exists in the series. This type of patterns are more likely to occur whenever the series has a high frequency (for example, daily, hourly, half-hourly, and so on), as there are more options to aggregate the series to a lower frequency. A typical example of multiple seasonality is the hourly demand for electricity, which could have multiple seasonal patterns, as the demand is derived from the hour of the day, the day of the week, or the yearly patterns, such as weather or the amount of daylight throughout the day.\n\nExample:\n\n# USgas dataset represents the total monthly consumptions of naturalgas in the US since # January 2000\nlibrary(TSstudio)\ndata(USgas)\nts_info(USgas)\n\n The USgas series is a ts object with 1 variable and 238 observations\n Frequency: 12 \n Start time: 2000 1 \n End time: 2019 10 \n\nts_plot(USgas,\n            title = \"US Monthly Natural Gas consumption\",\n            Ytitle = \"Billion Cubic Feet\",\n            Xtitle = \"Year\",\n            Xgrid = TRUE,\n            Ygrid = TRUE)"
  },
  {
    "objectID": "time_seasonality.html#seasonal-analysis-with-descriptive-statistics",
    "href": "time_seasonality.html#seasonal-analysis-with-descriptive-statistics",
    "title": "35  Seasonality Analysis",
    "section": "35.2 Seasonal Analysis with Descriptive Statistics",
    "text": "35.2 Seasonal Analysis with Descriptive Statistics\nDescriptive statistics are a simple yet powerful method to describe the key statistical characteristics of the data. This method is based on the use of summary statistics tables and is a summary of the key statistical indicators, such as the mean, median, quantile, and standard deviation, and data visualization tools, such as box plots and bar charts.\nDescriptive statistics can be used to describe the characteristics of the frequency units of a series. This allows us to identify whether we can segment each period of the series by some statistical criteria, for example, the mean, the quantile range, and so on.\nExample:\n\n# We group the USgas series by its frequency units and then summarize the mean and \n# standard deviation of each frequency unit\n\n# Transform the ts object to data.frame object\nUSgas_df &lt;- data.frame(year = floor(time(USgas)), month = cycle(USgas),\n                      USgas = as.numeric(USgas))\n\n# Set the month abbreviation and transforming it to a factor\nUSgas_df$month &lt;- factor(month.abb[USgas_df$month], levels = month.abb)\nhead(USgas_df)\n\n  year month  USgas\n1 2000   Jan 2510.5\n2 2000   Feb 2330.7\n3 2000   Mar 2050.6\n4 2000   Apr 1783.3\n5 2000   May 1632.9\n6 2000   Jun 1513.1\n\n# Summarize the series by its frequency units\nlibrary(dplyr)\nUSgas_summary &lt;- USgas_df %&gt;%\n                  group_by(month) %&gt;%\n                  summarise(mean = mean(USgas), sd = sd(USgas))\nUSgas_summary\n\n# A tibble: 12 × 3\n   month  mean    sd\n   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Jan   2806.  309.\n 2 Feb   2502.  223.\n 3 Mar   2325.  241.\n 4 Apr   1886.  175.\n 5 May   1708.  194.\n 6 Jun   1691.  216.\n 7 Jul   1864.  261.\n 8 Aug   1889.  237.\n 9 Sep   1687.  242.\n10 Oct   1788.  260.\n11 Nov   2015.  284.\n12 Dec   2543.  278.\n\n# Now we plot the summary\nlibrary(plotly)\nplot_ly (data = USgas_summary, x = ~ month, y = ~ mean, type = \"bar\", name = \"Mean\") %&gt;%\n            layout (title = \"USgas - Monthly Average\", yaxis = list(title = \"Mean\",\n            range = c(1500, 2700)))\n\n\n\n\n\nFrom the summary statistic table of the series you can see that, on average, each month is different from the next consecutive month by its standard deviation with the exception of the two pairs May/June and July/August, which are relatively close to each other. This allows us to characterize some months with a distinct behavior from the rest, such as January, February, March, and November.\nThe first indication for the potential existence of multiple seasonal patterns in the series is a high frequency, such as daily, hourly, and minutely. In those cases, there is more than one way to set the frequency of the series.\nExample:\n\n# UKgrid is an hourly time series, which marks it automatically as a suspect of\n# having multiple seasonal patterns\nlibrary(xts)\nlibrary(UKgrid)\n\n# Transform the series from a half-hourly frequency to hourly and will use an xts \n# format\nUKgrid_xts &lt;- extract_grid(type = \"xts\", \n                           columns = \"ND\", \n                           aggregate = \"hourly\", \n                           na.rm = TRUE)\n\nThe national demand of the UK high-voltage electric power transmission network (or the UKgrid series) has a strong seasonal pattern.\n\n# Transform the UKgrid series into data.frame format\nUKgrid_df &lt;- data.frame(time = index(UKgrid_xts), UKgrid = as.numeric(UKgrid_xts))\nstr(UKgrid_df)\n\n'data.frame':   127296 obs. of  2 variables:\n $ time  : POSIXct, format: \"2005-04-01 00:00:00\" \"2005-04-01 01:00:00\" ...\n $ UKgrid: num  65080 68207 69172 66769 64660 ...\n\n# Create seasonal features based on the periods we wish to check\nlibrary(lubridate)\nUKgrid_df$hour &lt;- hour(UKgrid_df$time)\nUKgrid_df$weekday &lt;- wday(UKgrid_df$time, label = TRUE, abbr = TRUE)\nUKgrid_df$month &lt;- factor(month.abb[month(UKgrid_df$time)], levels = month.abb)\nhead(UKgrid_df)\n\n                 time UKgrid hour weekday month\n1 2005-04-01 00:00:00  65080    0   vi\\\\.   Apr\n2 2005-04-01 01:00:00  68207    1   vi\\\\.   Apr\n3 2005-04-01 02:00:00  69172    2   vi\\\\.   Apr\n4 2005-04-01 03:00:00  66769    3   vi\\\\.   Apr\n5 2005-04-01 04:00:00  64660    4   vi\\\\.   Apr\n6 2005-04-01 05:00:00  65209    5   vi\\\\.   Apr\n\n# Summarize the series by its hourly cycle\nUKgrid_hourly &lt;- UKgrid_df %&gt;%\n                dplyr::group_by(hour) %&gt;%\n                dplyr::summarise(mean = mean(UKgrid, na.rm = TRUE), sd = sd(UKgrid, na.rm = TRUE))\n\n# Use the summary statistics table to plot both the hourly mean and its standard\n# deviation\nplot_ly(UKgrid_hourly) %&gt;%\n      add_lines(x = ~ hour, y = ~ mean, name = \"Mean\") %&gt;%\n      add_lines(x = ~ hour, y = ~ sd, name = \"Standard Deviation\", yaxis = \"y2\",\n          line = list(color = \"red\", dash = \"dash\", width = 3)) %&gt;%\n      layout(title = \"The UK Grid National Demand - Hourly Average vs. Standard\nDeviation\",\n      yaxis = list(title = \"Mean\"),\n      yaxis2 = list(overlaying = \"y\",\n      side = \"right\",\n      title = \"Standard Deviation\"),\n      xaxis = list(title=\"Hour of the day\"),\n      legend = list(x = 0.05, y = 0.9),\n      margin = list(l = 50, r = 50)\n      )\n\n\n\n\n\nLooking at the plot of the preceding summary statistic table, we can see the following behavior of the series:\n\nThere is low demand during the nighttime (between midnight and 6 a.m.) and high demand between the morning hours and early evening.\nThere is a strong correlation between the average demand and its standard deviation.\nThe relatively low standard deviation of the demand average during the nighttime could indicate that there is strong sub-seasonal effect during those hours beside the hourly seasonality. This should make sense, as those are normal sleep hours, and therefore, on average, the demand is reasonably the same throughout the weekdays.\nOn the other hand, the high standard deviation throughout the high-demand hours could indicate that the demand is distributed differently on different periodicity views (such as weekday or month of the year).\n\nTo examine the last point, we will subset the series into two groups representing the demand in the middle of the night and the demand throughout the day (3 a.m. and 9 a.m., respectively), and then we will group them by the weekday:\nExample:\n\nUKgrid_weekday &lt;- UKgrid_df %&gt;%\n                    dplyr::filter(hour == 3 | hour == 9) %&gt;%\n                    dplyr::group_by(hour, weekday) %&gt;%\n                    dplyr::summarise(mean = mean(UKgrid, na.rm = TRUE), sd = sd(UKgrid, na.rm = TRUE))\nUKgrid_weekday$hour &lt;- factor(UKgrid_weekday$hour)\nplot_ly(data = UKgrid_weekday, \n        x = ~ weekday, \n        y = ~ mean, \n        type = \"bar\",\n        color = ~ hour) %&gt;%\n            layout(title = \"The Hourly Average Demand by Weekday\", \n                   yaxis = list(title = \"Mean\", range = c(30000, 75000)), \n                   xaxis = list(title = \"Weekday\"))\n\n\n\n\n\nYou will see in the preceding bar chart that the demand for electricity at 3 a.m. is relatively stable throughout all the days of the week, with a slight difference between the average during the weekdays and the days in the weekend (about 2% different). On the other hand, there is a significant difference between the weekday and weekend demand at 9 a.m. (that is, the demand on Monday is higher on average by 28% from the one on Sunday)."
  }
]