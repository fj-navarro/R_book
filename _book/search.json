[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Methods for Research in Economics and Business in R",
    "section": "",
    "text": "Preface\nWelcome to R for Research in Action, a guide designed to empower learners at all levels in mastering the R programming language for their research endeavors. Whether you are a beginner just starting with R or an advanced user seeking to refine your skills through computational methods, this book will serve as a comprehensive companion to help you elevate the quality of your research, making it more robust, in-depth, and reproducible.\nResearch today demands more than just data collection—it requires the ability to analyze, visualize, and interpret findings in ways that are reliable and transparent. R, with its powerful ecosystem of packages and its open-source nature, has become an essential tool across various fields for achieving these goals. From data wrangling to advanced statistical modeling, R offers unparalleled flexibility to researchers, allowing them to explore their data deeply and communicate results with precision.\nThis book covers a broad range of topics, including data manipulation, visualization, statistical analysis, programming, and reproducible research practices. We begin with the basics, progressively introducing more advanced topics to ensure that users at every skill level can benefit. Along the way, you will find practical examples, real-world applications, and step-by-step guidance aimed at bridging the gap between theory and actionable results.\nBy the end of this book, my hope is that you will not only be proficient in R but also more confident in using it to conduct meaningful, impactful research. May this journey enhance the quality of your work and inspire further exploration.\n\n\nLearning Resources\nThese are resources that provide a wide range of learning options, from interactive tutorials to advanced textbooks, making them valuable for R learners at any stage.\n\nRWeekly. A valuable resource for learning and mastering R programming with its curated, up-to-date content on R tutorials, new packages, and advanced topics every week. Its community-driven approach fosters engagement and offers real-world applications, making it easier for users to stay informed and actively participate in the broader R ecosystem. Website\nThe RStudio Education Resources. Offers tutorials, webinars, and cheat sheets, particularly focusing on using R with the RStudio IDE. It’s great for learners who prefer structured courses. Website\nSwirl. An interactive R learning platform that teaches R directly within RStudio. It offers hands-on experience with guided lessons on various R topics. Website\nTidyverse.org. This website focuses on learning and using the tidyverse, a collection of R packages designed for data science. It offers excellent tutorials and documentation. Website\nStack Overflow (R Tag). A large and active community where you can ask questions, browse solutions, and learn from real-world R programming issues faced by others. Website\nR-bloggers. A blog aggregator for R tutorials, tips, and news from various contributors. It covers a wide range of topics and helps you stay up-to-date with the latest trends. Website\nRDocumentation. A centralized source for searching through R packages and functions, providing detailed documentation and examples. Website\n\n\n\nOnline Books\n\nR for Data Science (R4DS) by Hadley Wickham. A highly recommended free book that covers the essentials of R, data manipulation, and visualization with the tidyverse packages. Website\nAdvanced R by Hadley Wickham. Great for those looking to deepen their understanding of R programming. Website\nHands on Programming with R by Garrett Grolemund. An introduction to R as a programming language and is a great place to start if R is your first programming language. Website\nTidy Modeling with R by Max Kuhn and Julia Silge. This book is a guide to using a collection of software in the R programming language for model building called tidymodels. Website\nR Packages. Essential for learning how to build your own R packages. Website"
  },
  {
    "objectID": "intro_objects.html#classes",
    "href": "intro_objects.html#classes",
    "title": "1  R Objects",
    "section": "1.1 Classes",
    "text": "1.1 Classes\nR has five basic or “atomic” classes of objects:\n\ncharacter\nnumeric (real numbers)\ninteger\ncomplex\nlogical (True/False)\n\nEntering 1 in R gives you a numeric object; entering 1L explicitly gives you an integer object."
  },
  {
    "objectID": "intro_objects.html#attributes",
    "href": "intro_objects.html#attributes",
    "title": "1  R Objects",
    "section": "1.2 Attributes",
    "text": "1.2 Attributes\nAttributes of an object (if any) can be accessed using the attributes() function:\n\nnames, dimnames\ndimensions (e.g. matrices, arrays)\nclass (e.g. integer, numeric)\nlength\n\nThe mode() of an object tells us how it’s stored. It could happen that two different objects are stored in the same mode with different classes.\n\nFor vectors the class and mode will always be numeric, logical, or character.\nFor matrices and arrays a class is always a matrix or array, but its mode can be numeric, character, or logical.\n\nThe primary purpose of the class() function is to know how different functions, including generic functions, work (e.g. print, or plot). There is a collection of R commands used to assess whether a particular object belongs to a certain class, these start with is.; for example, is.numeric(), is.logical(), is.character(), is.list(), is.factor(), and is.data.frame()"
  },
  {
    "objectID": "intro_objects.html#mixing-objects",
    "href": "intro_objects.html#mixing-objects",
    "title": "1  R Objects",
    "section": "1.3 Mixing Objects",
    "text": "1.3 Mixing Objects\nThis is not allowed! When different objects are mixed in a vector, coercion occurs so that every element in the vector is of the same class.\n\ny &lt;- c(1.7, \"a\")\nclass(y) \n\n[1] \"character\"\n\n\n\ny &lt;- c(TRUE, 2)\nclass(y) \n\n[1] \"numeric\"\n\n\n\ny &lt;- c(\"a\", TRUE)\nclass(y) \n\n[1] \"character\""
  },
  {
    "objectID": "intro_objects.html#explicit-coercion",
    "href": "intro_objects.html#explicit-coercion",
    "title": "1  R Objects",
    "section": "1.4 Explicit Coercion",
    "text": "1.4 Explicit Coercion\nObjects can be explicitly coerced from one class to another using the as. functions, if available.\n\nx &lt;- 0:6 \nclass(x) \n\n[1] \"integer\"\n\n\n\nas.numeric(x)\n\n[1] 0 1 2 3 4 5 6"
  },
  {
    "objectID": "intro_objects.html#names",
    "href": "intro_objects.html#names",
    "title": "1  R Objects",
    "section": "1.5 Names",
    "text": "1.5 Names\nObject names must start with a letter and can only contain letters, numbers, _, and ..\nYou want your object names to be descriptive, so you’ll need to adopt a convention for multiple words.\nExample:\ni_use_snake_case\notherPeopleUseCamelCase\nsome.people.use.periods"
  },
  {
    "objectID": "intro_objects.html#r-workspace",
    "href": "intro_objects.html#r-workspace",
    "title": "1  R Objects",
    "section": "1.6 R Workspace",
    "text": "1.6 R Workspace\nThe objects that you create using R remain in existence until you explicitly delete them or you conclude the session.\n\nTo list all currently defined objects, use ls() or objects().\nTo remove object x, use rm(x). To remove all currently defined objects, use rm(list = ls()).\nTo save all of your existing objects to a file called fname in the current working directory, use save.image(file = \"fname\").\nTo save specific objects (say x and y) use save(x, y, file = \"fname\").\nTo load a set of saved objects use load(file = \"fname\").\nTo save this history to the file fname use savehistory(file = \"fname\") and to load the history file fname use loadhistory(file = \"fname\")."
  },
  {
    "objectID": "intro_objects.html#expressions-and-assignments",
    "href": "intro_objects.html#expressions-and-assignments",
    "title": "1  R Objects",
    "section": "1.7 Expressions and Assignments",
    "text": "1.7 Expressions and Assignments\nIn R an expression is used to denote a phrase of code that can be executed.\nExample:\n## An expression\nseq(10, 20, 3)\nThe combination of expressions that are saved for evaluation is called an assignment:\n## An assignment\nobject_name &lt;- value\nWhen reading that code, say “object name gets value” in your head. You will make lots of assignments, and &lt;- is a pain to type.\nYou can save time with RStudio’s keyboard shortcut: Alt + - (the minus sign)."
  },
  {
    "objectID": "intro_types.html#vectors",
    "href": "intro_types.html#vectors",
    "title": "2  Data Types",
    "section": "2.1 Vectors",
    "text": "2.1 Vectors\nA vector is the most convenient way to store more than one data value.\nA vector is a contiguous cell that contains data, where each cell can be accessed by an index. In other words, a vector is an indexed set of objects.\nAll the elements of an atomic vector have to be of the same type —numeric, character, or logical— which is called the mode of the vector.\n\n2.1.1 How to create a vector?\nThere are many ways to create a vector, but these are four basic functions for constructing vectors:\n\nThe c() (combine) function can be used to create vectors of objects by concatenating things together.\nExamples:\n\nx &lt;- c(0.5, 0.6) \nclass(x) \n\n[1] \"numeric\"\n\n\n\nx &lt;- c(TRUE, FALSE) \nclass(x) \n\n[1] \"logical\"\n\n\n\nx &lt;- c(T, F)  \nclass(x) \n\n[1] \"logical\"\n\n\n\nx &lt;- c(\"a\", \"b\", \"c\")   \nclass(x) \n\n[1] \"character\"\n\n\n\nx &lt;- 9:29   \nclass(x) \n\n[1] \"integer\"\n\n\n\nx &lt;- c(1+0i, 2+4i)  \nclass(x) \n\n[1] \"complex\"\n\n\nseq(from, to, by):\n\n(x &lt;- seq(1, 20, 2))\n\n [1]  1  3  5  7  9 11 13 15 17 19\n\n\nrep(x, times):\n\n(y &lt;- rep(3, 4))\n\n[1] 3 3 3 3\n\n\nYou can also use the vector() function to initialize vectors:\nExample:\n\nx &lt;- vector(\"numeric\", length = 10) \nclass(x) \n\n[1] \"numeric\"\n\n\n\n\n\n2.1.2 Add a value to a variable\nExample:\nx[4] &lt;- 9\n\n\n2.1.3 Add names to a vector\n\n# Create a vector with the name of each element\nnamed.num.vec &lt;- c(x1=1, x2=3, x3=5) \nnamed.num.vec \n\nx1 x2 x3 \n 1  3  5 \n\n\nThis is another option to add names:\nnames(x) &lt;- c(\"a\", \"b\", \"c)\n\n\n2.1.4 length()\nThe function length(x) gives the number of elements of ‘x’.\n\nx &lt;- 100:100\nlength(x)\n\n[1] 1\n\n\nIt is possible to have a vector with no elements\n\nx &lt;- c()\nlength(x)\n\n[1] 0"
  },
  {
    "objectID": "intro_types.html#factors",
    "href": "intro_types.html#factors",
    "title": "2  Data Types",
    "section": "2.2 Factors",
    "text": "2.2 Factors\nStatisticians typically recognise three basic types of variable: numeric, ordinal, and categorical. In R the data type for ordinal and categorical vectors is factor. The possible values of a factor are referred to as its levels.\nIn practice, a factor is not much different from a character vector, except that the elements of a factor can take only a limited number of values (of which R keeps a record), and in statistical routines R is able to treat a factor differently than a character vector.\nTo create a factor we apply the function factor() to some vector x. By default the distinct values of x become the levels, or we can specify them using the optional levels argument.\nExample:\n\nx &lt;- factor(c(\"yes\", \"yes\", \"no\", \"yes\", \"no\")) \ntable(x)\n\nx\n no yes \n  2   3 \n\nlevels(x)\n\n[1] \"no\"  \"yes\"\n\n\nNote the use of the function table() to calculate the number of times each level of the factor appears. table() can be applied to other modes of vectors as well as factors. The output of the table() function is a one-dimensional array (as opposed to a vector). If more than one vector is passed to table(), then it produces a multidimensional array.\nThe order of the levels of a factor can be set using the levels argument to factor(). By default R arranges the levels of a factor alphabetically. If you specify the levels yourself, then R uses the ordering that you provide.\nExample:\n\nx &lt;- factor(c(\"yes\", \"yes\", \"no\", \"yes\", \"no\"), levels = c(\"yes\", \"no\"), ordered=TRUE) \nx\n\n[1] yes yes no  yes no \nLevels: yes &lt; no\n\n\nUsing factors with labels is better than using integers because factors are self-describing.\nExample:\nphys.act &lt;- factor(phys.act, levels = c(\"L\", \"M\", \"H\"),\n  labels = c(\"Low\", \"Medium\", \"High\"),\n  ordered = TRUE)\nWe check whether or not an object x is a factor using is.factor(x).\nExample:\n\nis.factor(x)\n\n[1] TRUE\n\n\nUsually it is convenient to transform a numeric variable into a data.frame:\nairquality &lt;- transform(airquality, Month = factor(Month))\ncut() is a generic command to create factor variables from numeric variables:\nExample:\n\nnumvar &lt;- rnorm(100) \nnum2factor &lt;- cut(numvar, breaks=5) ## the levels are produced using the actual range of values\nnum2factor\n\n  [1] (0.738,1.58]     (-0.0954,0.738]  (-0.929,-0.0954] (-0.0954,0.738] \n  [5] (-1.76,-0.929]   (-1.76,-0.929]   (0.738,1.58]     (-0.929,-0.0954]\n  [9] (-1.76,-0.929]   (0.738,1.58]     (-0.929,-0.0954] (-0.0954,0.738] \n [13] (-0.929,-0.0954] (-0.0954,0.738]  (-0.929,-0.0954] (0.738,1.58]    \n [17] (0.738,1.58]     (-1.76,-0.929]   (-0.929,-0.0954] (0.738,1.58]    \n [21] (-0.929,-0.0954] (-2.6,-1.76]     (-0.0954,0.738]  (-0.0954,0.738] \n [25] (0.738,1.58]     (-0.0954,0.738]  (-2.6,-1.76]     (-0.929,-0.0954]\n [29] (-0.929,-0.0954] (-0.0954,0.738]  (-0.929,-0.0954] (-0.929,-0.0954]\n [33] (-0.929,-0.0954] (0.738,1.58]     (-0.929,-0.0954] (-1.76,-0.929]  \n [37] (-1.76,-0.929]   (-0.0954,0.738]  (-0.929,-0.0954] (-0.929,-0.0954]\n [41] (-0.929,-0.0954] (-0.929,-0.0954] (-0.929,-0.0954] (0.738,1.58]    \n [45] (0.738,1.58]     (-0.0954,0.738]  (-0.929,-0.0954] (0.738,1.58]    \n [49] (-0.0954,0.738]  (-0.929,-0.0954] (0.738,1.58]     (-0.929,-0.0954]\n [53] (-0.0954,0.738]  (-0.0954,0.738]  (-0.0954,0.738]  (-0.929,-0.0954]\n [57] (-0.929,-0.0954] (-1.76,-0.929]   (-1.76,-0.929]   (-0.929,-0.0954]\n [61] (-0.0954,0.738]  (-0.0954,0.738]  (-0.0954,0.738]  (-0.0954,0.738] \n [65] (-0.0954,0.738]  (-0.0954,0.738]  (-0.0954,0.738]  (0.738,1.58]    \n [69] (0.738,1.58]     (0.738,1.58]     (-0.0954,0.738]  (0.738,1.58]    \n [73] (-0.929,-0.0954] (-0.929,-0.0954] (-0.0954,0.738]  (-2.6,-1.76]    \n [77] (-1.76,-0.929]   (-0.0954,0.738]  (0.738,1.58]     (0.738,1.58]    \n [81] (0.738,1.58]     (0.738,1.58]     (-0.0954,0.738]  (-1.76,-0.929]  \n [85] (0.738,1.58]     (-1.76,-0.929]   (-0.929,-0.0954] (-0.0954,0.738] \n [89] (-1.76,-0.929]   (-0.929,-0.0954] (-0.929,-0.0954] (-0.929,-0.0954]\n [93] (-0.929,-0.0954] (-1.76,-0.929]   (-0.0954,0.738]  (-0.0954,0.738] \n [97] (-0.0954,0.738]  (-1.76,-0.929]   (-2.6,-1.76]     (-0.929,-0.0954]\n5 Levels: (-2.6,-1.76] (-1.76,-0.929] (-0.929,-0.0954] ... (0.738,1.58]\n\n\n\nnum2factor &lt;- cut(numvar, breaks=5, labels= c(\"lowest group\", \"lower middle group\", \"middle group\", \"upper middle\", \"highest group\"))\ndata.frame(table(num2factor)) ## displaying the data in tabular form\n\n          num2factor Freq\n1       lowest group    4\n2 lower middle group   14\n3       middle group   32\n4       upper middle   29\n5      highest group   21"
  },
  {
    "objectID": "intro_types.html#matrices",
    "href": "intro_types.html#matrices",
    "title": "2  Data Types",
    "section": "2.3 Matrices",
    "text": "2.3 Matrices\nMatrices are stored as vectors with an added dimension attribute. The dimension attribute is itself an integer vector of length 2, which gives the number of rows and columns.\nThe matrix elements are stored column-wise in the vector. This means that it is possible to access the matrix elements using a single index:\n\n(A &lt;- matrix(c(3,5,2,3), 2, 2))\n\n     [,1] [,2]\n[1,]    3    2\n[2,]    5    3\n\nA[2]\n\n[1] 5\n\nA[,2]\n\n[1] 2 3\n\n\n\n2.3.1 How to create a matrix?\nMatrices are constructed column-wise, so entries can be thought of starting in the “upper left” corner and running down the columns.\nExample:\n\n(m &lt;- matrix(1:6, nrow = 2, ncol = 3, byrow = FALSE))\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\ndim(m) \n\n[1] 2 3\n\n\nMatrices can be created by column-binding or row-binding with the cbind() and rbind() functions:\nExamples:\n\nx &lt;- 1:3\ny &lt;- 10:12\ncbind(x, y)\n\n     x  y\n[1,] 1 10\n[2,] 2 11\n[3,] 3 12\n\nrbind(x, y)\n\n  [,1] [,2] [,3]\nx    1    2    3\ny   10   11   12\n\n\nMatrices can also be created directly from vectors by adding a dimension attribute:\nExample:\n\nm &lt;- 1:10\ndim(m) &lt;- c(2, 5)\nm\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    5    7    9\n[2,]    2    4    6    8   10\n\n\nTo create a matrixA with one column from a vector x, we use:\nA &lt;- as.matrix(x)\nTo create a vector from a matrix A, we use:\nx &lt;- as.vector(A)\n\n\n2.3.2 Create a diagonal matrix\nTo create a diagonal matrix use diag(x):\n\nB &lt;- diag(c(1,2,3))\n\n\n\n2.3.3 Add names to matrices\nMatrices can have names:\nExample:\ndimnames(m) &lt;- list(c(\"a\", \"b\"), c(\"c\", \"d\", \"e\")) \nm\nThese are other options for column and row names:\ncolnames(m) &lt;- c(\"c\", \"d\", \"e\")\nrownames(m) &lt;- c(\"a\", \"b\")\n\n\n2.3.4 Operations with matrices\nTo perform matrix multiplication we use the operator %*%. Remember that * acts element wise on matrices.\nOther functions for using with matrices are:\n\nnrow(x)\nncol(x)\ndet(x) (the determinant)\nt(x) (the transpose)\nsolve(A, B) (returns x such that A %*% x == B).\nIf A is invertible then solve(A) returns the matrix inverse of A."
  },
  {
    "objectID": "intro_types.html#data-frames",
    "href": "intro_types.html#data-frames",
    "title": "2  Data Types",
    "section": "2.4 Data Frames",
    "text": "2.4 Data Frames\nIt is a list of vectors restricted to be of equal length. Each vector —or column— corresponds to a variable in an experiment, and each row corresponds to a single observation or experimental unit. Each vector can be of any of the basic modes of object.\nThe dataframe is like a matrix but extended to allow for different object modes in different columns. Unlike matrices, data frames can store different classes of objects in each column (matrices must have every element be the same class, e.g. all integers or all numeric). Obviously to work with datasets from real experiments we need a way to group data of differing modes.\nData frames are used to store tabular data in R. Data frames are represented as a special type of list where every element of the list has to have the same length. Each element of the list can be thought of as a column and the length of each element of the list is the number of rows.\nData frames are usually created by:\n\nreading in a dataset using the read.table() or read.csv()\ncreating a dataframe with data.frame():\nExample:\n\n(x &lt;- data.frame(foo = 1:4, bar = c(T, T, F, F)))\n\n  foo   bar\n1   1  TRUE\n2   2  TRUE\n3   3 FALSE\n4   4 FALSE\n\n# To summarise the structure of a list (or dataframe), use str()\nstr(x)\n\n'data.frame':   4 obs. of  2 variables:\n $ foo: int  1 2 3 4\n $ bar: logi  TRUE TRUE FALSE FALSE\n\n\ncoerced from other types of objects like lists:\nx &lt;- as.data.frame(x)\n\nDataframes can be converted to a matrix by calling data.matrix().\nThe dplyr package has an optimized set of functions designed to work efficiently with dataframes.\n\n2.4.1 Columns and Rows\nYou can construct a dataframe from a collection of vectors and/or existing dataframes using the function data.frame, which has the form: data.frame(col1 = x1, col2 = x2, ..., df1, df2, ...). Here col1, col2, etc., are the column names (given as character strings without quotes) and x1, x2, etc., are vectors of equal length. df1, df2, etc., are dataframes, whose columns must be the same length as the vectors x1, x2, etc. Column names may be omitted, in which case R will choose a name for you.\nColumn names indicate the names of the variables or predictors names(). We can also create a new variable within a dataframe, by naming it and assigning it a value:\nExample:\nufc$volume.m3 &lt;- pi * (ufc$dbh.cm / 200)^2 * ufc$height.m / 2\nEquivalently one could assign to ufc[6] or ufc[\"volume.m3\"] or ufc[[6]] or ufc[[\"volume.m3\"]].\nThe command names(df) will return the names of the dataframe df as a vector of character strings.\nExample:\nufc.names &lt;- names(ufc)\n# To change the names of df you pass a vector of character strings to `names(df)`\nnames(ufc) &lt;- c(\"P\", \"T\", \"S\", \"D\", \"H\", \"V\")\nWhen you create dataframes and any one of the column’s classes is a character, it automatically gets converted to factor, which is a default R operation. However, there is one argument, stringsAsFactors=FALSE, that allows us to prevent the automatic conversion of character to factor during data frame creation.\nDataframes have a special attribute called row.names() which indicate information about each row of the data frame. You can change the row names of df by making an assignment to row.names(df).\n\n\n2.4.2 subset()\nThe function subset() is a convenient tool for selecting the rows of a dataframe, especially when combined with the operator %in%.\nExample:\n# Suppose you are only interested in the height of trees of species DF (Douglas Fir) or GF (Grand Fir)\nfir.height &lt;- subset(ufc, subset = species %in% c(\"DF\", \"GF\"),\n                    select = c(plot, tree, height.m))\nhead(fir.height)\n\n\n2.4.3 attach()\nR allows you to attach a dataframe to the workspace. When attached, the variables in the dataframe can be referred to without being prefixed by the name of the dataframe.\nExample:\nattach(ufc)\nmax(height.m[species == \"GF\"])\nWhen you attach a dataframe R actually makes a copy of each variable, which is deleted when the dataframe is detached. Thus, if you change an attached variable you do not change the dataframe. After we use the attach() command, we need to use detach() to remove individual variables from the working environment.\nNonetheless, note that the with() and transform()functions provide a safer alternative."
  },
  {
    "objectID": "intro_types.html#lists",
    "href": "intro_types.html#lists",
    "title": "2  Data Types",
    "section": "2.5 Lists",
    "text": "2.5 Lists\nLists are a special type of vector that can contain elements of different type (we can store single constants, vectors of numeric values, factors, data frames, matrices, and even arrays), namely, a list is a general data storage object that can house pretty much any other kind of R object.\nLike a vector, a list is an indexed set of objects (and so has a length), but unlike a vector the elements of a list can be of different types, including other lists! The mode of a list is list.\nThe power and utility of lists comes from this generality. A list might contain an individual measurement, a vector of observations on a single response variable, a dataframe, or even a list of dataframes containing the results of several experiments.\nIn R lists are often used for collecting and storing complicated function output. Dataframes are special kinds of lists.\n\n2.5.1 How to Create a List?\nLists can be explicitly created using the list() function, which takes an arbitrary number of arguments:\nExample 1:\n\n(x &lt;- list(1, \"a\", TRUE, 1 + 4i))\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] \"a\"\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] 1+4i\n\n\nExample 2:\n\n(my.list &lt;- list(\"one\", TRUE, 3, c(\"f\",\"o\",\"u\",\"r\")))\n\n[[1]]\n[1] \"one\"\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] 3\n\n[[4]]\n[1] \"f\" \"o\" \"u\" \"r\"\n\nmy.list[[2]]\n\n[1] TRUE\n\nmode(my.list[[2]])\n\n[1] \"logical\"\n\nmy.list[[4]][1]\n\n[1] \"f\"\n\nmy.list[4][1]\n\n[[1]]\n[1] \"f\" \"o\" \"u\" \"r\"\n\n\nWe can also create an empty list of a pre-specified length with the vector() function:\nExample:\n\n(x &lt;- vector(mode = \"list\", length = 5)) # the elements are NULL\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n[[5]]\nNULL\n\n\nTo flatten a list x, that is convert it to a vector, we use unlist(x).\nExample:\n\nx &lt;- list(1, c(2, 3), c(4, 5, 6))\nunlist(x)\n\n[1] 1 2 3 4 5 6\n\n\nMany functions produce list objects as their output. For example, when we fit a least squares regression, the regression object itself is a list, and can be manipulated using list operations.\nExample:\n\nlm.xy &lt;- lm(y ~ x, data = data.frame(x = 1:5, y = 1:5))\nmode(lm.xy)\n\n[1] \"list\"\n\nnames(lm.xy)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n\n\n\n2.5.2 Names\nThe elements of a list can be named when the list is created, using arguments of the form name1 = x1, name2 = x2, etc., or they can be named later by assigning a value to the names attribute.\nExample:\n\nmy.list &lt;- list(first = \"one\", second = TRUE, third = 3,\n                fourth = c(\"f\",\"o\",\"u\",\"r\"))\nnames(my.list)\n\n[1] \"first\"  \"second\" \"third\"  \"fourth\"\n\n\nUnlike a dataframe, the elements of a list do not have to be named. Names can be used (within quotes) when indexing with single or double square brackets, or they can be used (with or without quotes) after a dollar sign to extract a list element."
  },
  {
    "objectID": "intro_types.html#arrays",
    "href": "intro_types.html#arrays",
    "title": "2  Data Types",
    "section": "2.6 Arrays",
    "text": "2.6 Arrays\nSometimes, you need to store multiple matrices or data frames into a single object; in this case, we can use arrays to store this data.\nData frames and matrices are of two dimensions only, but an array can be of any number of dimensions.\nHere is a simple example to store three matrices of order 2 x 2 in a single array object:\nExamples:\n\n(mat.array &lt;- array(dim=c(2,2,3)))\n\n, , 1\n\n     [,1] [,2]\n[1,]   NA   NA\n[2,]   NA   NA\n\n, , 2\n\n     [,1] [,2]\n[1,]   NA   NA\n[2,]   NA   NA\n\n, , 3\n\n     [,1] [,2]\n[1,]   NA   NA\n[2,]   NA   NA\n\n(mat.array[,,1] &lt;- rnorm(4))\n\n[1] -0.3704286 -0.1597000  1.2355196  1.6677764\n\n(mat.array[,,1] &lt;- rnorm(4))\n\n[1] -0.6623115 -0.6104567 -1.1273824  0.8379214\n\n(mat.array[,,2] &lt;- rnorm(4))\n\n[1] -1.64012470 -0.46502691  0.97060699 -0.07112538\n\n(mat.array[,,3] &lt;- rnorm(4))\n\n[1] -0.8226163 -0.7321720  0.5282601  1.2565976"
  },
  {
    "objectID": "intro_packages.html",
    "href": "intro_packages.html",
    "title": "3  Packages",
    "section": "",
    "text": "A package is an archive of files that conforms to a certain format and structure and that provides extra functionality, usually extending R in a particular direction. The R community has produced many high-quality R packages for performing specific tasks,\nAny package is in one of three states:\n\nInstalled and loaded. A package that is loaded is directly available to your R session. Find out which packages are loaded using sessionInfo().\nInstalled but not loaded. A package that is installed is available for loading but its contents are not available until it is loaded. The function help.start() gives details of the packages that are installed on your computer.\nNot installed. These packages cannot be loaded. If a package is not installed then the library() function produces an error. If the install status is uncertain at the time of calling library (for example if you are writing a function that requires the package), then use the require() function, which returns FALSE if the package is not installed, rather than an error.\n\nInstalling all available packages would be a waste of space and time, as you would never use most of them. Similarly, loading all installed packages every time you start R would take some time, so by default R only loads the base packages when it starts and requires the user to load any others as and when they are needed.\nPackages are divided into three groups:\n\nBase. Base packages are installed along with R, and their objects are always available.\nRecommended. Recommended packages are installed along with R but must be loaded before they can be used.\nOther. Other packages are not installed by default, and must be installed separately.\n\nThe command to find out what packages are available for loading is installed.packages. The output of the function is quite verbose, but we only need the first column:\nExample:\n\ninstalled.packages()[1:5, 1] # Returns only the first five packages\n\n    anytime     askpass   backports   base64enc          BH \n  \"anytime\"   \"askpass\" \"backports\" \"base64enc\"        \"BH\" \n\n\nAll the packages that are available at a repository, and whose requirements are matched by the currently running version of R, can be listed using the command available.packages(). A package that is available in the repository but has not yet been installed may be installed using the install.packages() function. If we include the argument dependencies = TRUE, then the function will also install packages that are necessary to run the package or packages of interest; such packages are called dependencies.\nMost of the packages frequently get updates. This includes new features, improvements, and error fixing. The packageVersion function returns the version details of the input package:\nExample:\n\npackageVersion(\"tidyverse\")\n\n[1] '2.0.0'\n\n\nThe status of the packages that are installed can be compared with the repository using the old.packages() function, and easily updated using the update.packages() function.\nR can be easily updated with installr package. To update R on MacOS, you need to use updateR” package instead.\nExample:\ninstall.packages(\"installr\")\nlibrary(installr)\nupdateR()"
  },
  {
    "objectID": "intro_style.html#names",
    "href": "intro_style.html#names",
    "title": "4  Code Style",
    "section": "4.1 Names",
    "text": "4.1 Names\nRemember that variable names should use only lowercase letters, numbers, and _. Use _to separate words within a name.\nAs a general rule of thumb, it’s better to prefer long, descriptive names that are easy to understand rather than concise names that are fast to type.\nIn general, if you have a bunch of variables that are a variation on a theme, you’re better off giving them a common prefix rather than a common suffix because autocomplete works best on the start of a variable."
  },
  {
    "objectID": "intro_style.html#spaces",
    "href": "intro_style.html#spaces",
    "title": "4  Code Style",
    "section": "4.2 Spaces",
    "text": "4.2 Spaces\nPut spaces on either side of mathematical operators apart from ^ (i.e. +, -, ==, &lt;, …), and around the assignment operator (&lt;-).\nExample:\n# Strive for\nz &lt;- (a + b)^2 / d\n\n# Avoid\nz&lt;-( a + b ) ^ 2/d\nDon’t put spaces inside or outside parentheses for regular function calls. Always put a space after a comma, just like in standard English.\n# Strive for\nmean(x, na.rm = TRUE)\n\n# Avoid\nmean (x ,na.rm=TRUE)"
  },
  {
    "objectID": "intro_style.html#pipes",
    "href": "intro_style.html#pipes",
    "title": "4  Code Style",
    "section": "4.3 Pipes",
    "text": "4.3 Pipes\n|&gt; should always have a space before it and should typically be the last thing on a line. This makes it easier to add new steps, rearrange existing steps, modify elements within a step, and get a sky view by skimming the verbs on the left-hand side.\n# Strive for \nflights |&gt;  \n  filter(!is.na(arr_delay), !is.na(tailnum)) |&gt; \n  count(dest)\n\n# Avoid\nflights|&gt;filter(!is.na(arr_delay), !is.na(tailnum))|&gt;count(dest)\nIf the function you’re piping into has named arguments (like mutate() or summarize()), put each argument on a new line. If the function doesn’t have named arguments (like select() or filter()), keep everything on one line unless it doesn’t fit, in which case you should put each argument on its own line.\n# Strive for\nflights |&gt;  \n  group_by(tailnum) |&gt; \n  summarize(\n    delay = mean(arr_delay, na.rm = TRUE),\n    n = n()\n  )\n\n# Avoid\nflights |&gt;\n  group_by(\n    tailnum\n  ) |&gt; \n  summarize(delay = mean(arr_delay, na.rm = TRUE), n = n())\nAfter the first step of the pipeline, indent each line by two spaces. RStudio will automatically put the spaces in for you after a line break following a |&gt; . If you’re putting each argument on its own line, indent by an extra two spaces. Make sure )is on its own line, and un-indented to match the horizontal position of the function name.\n# Strive for \nflights |&gt;  \n  group_by(tailnum) |&gt; \n  summarize(\n    delay = mean(arr_delay, na.rm = TRUE),\n    n = n()\n  )\nBe wary of writing very long pipes, say longer than 10-15 lines. Try to break them up into smaller sub-tasks, giving each task an informative name.\nThe same basic rules that apply to the pipe also apply to ggplot2; just treat + the same way as |&gt;."
  },
  {
    "objectID": "manipu_reading.html#tabular-data",
    "href": "manipu_reading.html#tabular-data",
    "title": "5  Reading Data",
    "section": "5.1 Tabular Data",
    "text": "5.1 Tabular Data\nIt is common for data to be arranged in tables, with columns corresponding to variables and rows corresponding to separate observations. These dataframes are usually read into R using the function read.table(), which has the form:\nread.table(file, header = FALSE, sep = \"\") \n# read.table() returns a dataframe\nread_table() reads a common variation of fixed-width files where columns are separated by white space.\nThere are two commonly used variants of read.table():\n\n5.1.1 read.csv()\nread.csv() is for comma-separated data and is equivalent to read.table(file, header = TRUE, sep = \",\").\nSometimes, it could happen that the file extension is .csv, but the data is not comma separated. In that case, we can still use the read.csv() function, but in this case we have to specify the separator:\nExample:\nread.csv(\"iris_semicolon.csv\", stringsAsFactors = FALSE, sep=\";\")\nanscombe_tab_2 &lt;- read.table(\"anscombe.txt\", header=TRUE)\nUsually, read_csv() uses the first line of the data for the column names, which is a very common convention. But it’s not uncommon for a few lines of metadata to be included at the top of the file. You can use skip = n to skip the first n lines or use comment = \"#\" to drop all lines that start with (e.g.) #.\nExample:\nread_csv(\"data/students.csv,\n  skip = 2,\n  comment = \"#\"\n)\nIn other cases, the data might not have column names. You can use col_names = FALSE to tell read_csv() not to treat the first row as headings and instead label them sequentially from \\(X_1\\) to \\(X_n\\). Alternatively, you can pass col_names a character vector which will be used as the column names (e.g. col_names = c(\"x\", \"y\", \"z\")):\nNote that by default read_csv() only recognizes empty strings (““) in as NAs, however you surely want it to also recognize the character string “N/A”.\nExample:\nstudents &lt;- read_csv(\"data/students.csv\", na = c(\"N/A\", \"\"))\nIf a .csv file contains both numeric and character variables, and we use read.csv(), the character variables get automatically converted to the factor type. We can prevent character variables from this automatic conversion to factor, by specifying stringsAsFactors=FALSE within the read.csv() function.\nExample:\nread.csv(\"iris.csv\", stringsAsFactors=F)\nreadr provides a total of nine column types for you to use: col_logical(), col_logical(), col_factor()… Find more information here.\n\n\n5.1.2 read.delim()\nread_delim() reads in files with any delimiter, attempting to automatically guess the delimiter if you don’t specify it. For example, read.delim()is for tab-delimited data and is equivalent to read.table(file, header = TRUE, sep = \"\\t\").\nExample:\n## skips the first 2 lines\nanscombe &lt;- read.csv(\"CSVanscombe.csv\", skip=2) \n\n\n5.1.3 Other file types\nOnce you’ve mastered read_csv(), using readr’s other functions is straightforward; it’s just a matter of knowing which function to reach for:\n\nread_csv2(): reads semicolon-separated files. These use ; instead of , to separate fields and are common in countries that use , as the decimal marker.\nread_tsv(): reads tab-delimited files.\nread_fwf(): reads fixed-width files. You can specify fields by their widths with fwf_widths() or by their positions with fwf_positions().\nread_log(): reads Apache-style log files."
  },
  {
    "objectID": "manipu_reading.html#reading-multiple-files",
    "href": "manipu_reading.html#reading-multiple-files",
    "title": "5  Reading Data",
    "section": "5.2 Reading Multiple Files",
    "text": "5.2 Reading Multiple Files\nSometimes your data is split across multiple files instead of being contained in a single file. For example, you might have sales data for multiple months, with each month’s data in a separate file: 01-sales.csv for January, 02-sales.csv for February, and 03-sales.csv for March.\nWith read_csv()you can read these data in at once and stack them on top of each other in a single data frame.\nExample:\nsales_files &lt;- c(\"data/01-sales.csv\", \"data/02-sales.csv\", \"data/03-sales.csv\")\nread_csv(sales_files, id = \"file\")\nThe id argument adds a new column called file to the resulting data frame that identifies the file the data come from. This is especially helpful in circumstances where the files you’re reading in do not have an identifying column that can help you trace the observations back to their original sources.\nAn alternative to read multiple data made available online is this:\nExample:\nsales_files &lt;- c(\n  \"https://pos.it/r4ds-01-sales\",\n  \"https://pos.it/r4ds-02-sales\",\n  \"https://pos.it/r4ds-03-sales\"\n)\nread_csv(sales_files, id = \"file\")"
  },
  {
    "objectID": "manipu_reading.html#reading-line-by-line",
    "href": "manipu_reading.html#reading-line-by-line",
    "title": "5  Reading Data",
    "section": "5.3 Reading Line by Line",
    "text": "5.3 Reading Line by Line\nText files can be read line by line using the readLines() function:\nExample:\ncon &lt;- gzfile(\"words.gz\") \nx &lt;- readLines(con, 10)\nThis approach is useful because it allows you to read from a file without having to uncompress the file first, which would be a waste of space and time.\nTo read in lines of webpages it can be useful the readLines() function:\nExample:\ncon &lt;- url(\"http://www.jhsph.edu\", \"r\") ## Read the web page \nx &lt;- readLines(con) ## Print out the first few lines head(x)"
  },
  {
    "objectID": "manipu_reading.html#reading-excel-spreadsheets",
    "href": "manipu_reading.html#reading-excel-spreadsheets",
    "title": "5  Reading Data",
    "section": "5.4 Reading Excel Spreadsheets",
    "text": "5.4 Reading Excel Spreadsheets\nIf the dataset is stored in the .xls or .xlsx format you can use the readxl package. This package is non-core tidyverse, so you need to load it explicitly, but it is installed automatically when you install the tidyverse package.\nMost of readxl’s functions allow you to load Excel spreadsheets into R:\n\nread_xls() reads Excel files with xls format.\nread_xlsx() read Excel files with xlsx format.\nread_excel() can read files with both xls and xlsx format. It guesses the file type based on the input.\n\nExample:\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(writexl)\n(students &lt;- read_excel(\"data/students.xlsx\"))\n\n# A tibble: 6 × 5\n  `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4.0  \n2            2 Barclay Lynn     French fries       Lunch only          5.0  \n3            3 Jayendra Lyne    N/A                Breakfast and lunch 7.0  \n4            4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6            6 Güvenç Attila    Ice cream          Lunch only          6.0  \n\n\nAn important feature that distinguishes spreadsheets from flat files is the notion of multiple sheets, called worksheets. You can read a single worksheet from a spreadsheet with the sheet argument in read_excel(). The default, which we’ve been relying on up until now, is the first sheet.\nread_excel(\"data/penguins.xlsx\", sheet = \"Torgersen Island\")\nAlternatively, you can use excel_sheets() to get information on all worksheets in an Excel spreadsheet, and then read the one(s) you’re interested in.\nexcel_sheets(\"data/penguins.xlsx\")\n\n# Read a worksheet individually\npenguins_biscoe &lt;- read_excel(\"data/penguins.xlsx\", sheet = \"Biscoe Island\", na = \"NA\") \nTo know the number of columns and rows of each worksheet you can use the function dim().\ndim(penguins_torgersen)\nWhen working with spreadsheet data, it’s important to keep in mind that the underlying data can be very different than what you see in the cell. For example, Excel has no notion of an integer. All numbers are stored as floating points, but you can choose to display the data with a customizable number of decimal points. Similarly, dates are actually stored as numbers, specifically the number of seconds since January 1, 1970. You can customize how you display the date by applying formatting in Excel. Confusingly, it’s also possible to have something that looks like a number but is actually a string (e.g., type ’10 into a cell in Excel).\n\n5.4.1 Reading part of a sheet\nIt’s quite common to find cell entries in a spreadsheet that are not part of the data you want to read into R. You can use the readxl_example() function to locate the spreadsheet on your system in the directory where the package is installed. This function returns the path to the spreadsheet, which you can use in read_excel()as usual.\nstudents_path &lt;- readxl_example(\"students.xlsx\")\nread_excel(students_path, range = \"A1:C4\")"
  },
  {
    "objectID": "manipu_reading.html#reading-google-sheets",
    "href": "manipu_reading.html#reading-google-sheets",
    "title": "5  Reading Data",
    "section": "5.5 Reading Google Sheets",
    "text": "5.5 Reading Google Sheets\nFor loading data from a Google Sheet you will be using the googlesheets4 package. This package is non-core tidyverse, so you need to load it explicitly.\n\nlibrary(googlesheets4)\nlibrary(tidyverse)\n\nThe main function of the googlesheets4 package is read_sheet(), which reads a Google Sheet from a URL or a file id. This function also goes by the name range_read(). You can also create a brand new sheet with gs4_create() or write to an existing sheet with sheet_write() and friends.\nThe first argument to read_sheet()is the URL of the file to read, and it returns a tibble:\nExample:\n\ngs4_deauth()\nstudents &lt;- read_sheet('https://docs.google.com/spreadsheets/d/1V1nPp1tzOuutXFLb3G9Eyxi3qxeEhnOXUzL5_BcCQ0w')\n\n✔ Reading from \"students\".\n\n\n✔ Range 'Sheet1'.\n\nstudents\n\n# A tibble: 6 × 5\n  `Student ID` `Full Name`      favourite.food     mealPlan            AGE      \n         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;list&gt;   \n1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          &lt;dbl [1]&gt;\n2            2 Barclay Lynn     French fries       Lunch only          &lt;dbl [1]&gt;\n3            3 Jayendra Lyne    N/A                Breakfast and lunch &lt;dbl [1]&gt;\n4            4 Leon Rossini     Anchovies          Lunch only          &lt;NULL&gt;   \n5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch &lt;chr [1]&gt;\n6            6 Güvenç Attila    Ice cream          Lunch only          &lt;dbl [1]&gt;\n\n\nIt’s also possible to read individual sheets from Google Sheets as well. Let’s read the “Torgersen Island” sheet from the penguins Google Sheet:\n\npenguins_sheet_id &lt;- \"1aFu8lnD_g0yjF5O-K6SFgSEWiHPpgvFCF0NY9D6LXnY\"\nread_sheet(penguins_sheet_id, sheet = \"Torgersen Island\")\n\n✔ Reading from \"penguins\".\n\n\n✔ Range ''Torgersen Island''.\n\n\n# A tibble: 52 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;     &lt;list&gt;         &lt;list&gt;        &lt;list&gt;            &lt;list&gt;     \n 1 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n 2 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n 3 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n 4 Adelie  Torgersen &lt;chr [1]&gt;      &lt;chr [1]&gt;     &lt;chr [1]&gt;         &lt;chr [1]&gt;  \n 5 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n 6 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n 7 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n 8 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n 9 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n10 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;         &lt;dbl [1]&gt;  \n# ℹ 42 more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\nYou can obtain a list of all sheets within a Google Sheet with sheet_names():\n\nsheet_names(penguins_sheet_id)\n\n[1] \"Torgersen Island\" \"Biscoe Island\"    \"Dream Island\"    \n\n\nNote that we’re also using the gs4_example() function below to locate an example Google Sheet that comes with the googlesheets4 package.\n\ndeaths_url &lt;- gs4_example(\"deaths\")\ndeaths &lt;- read_sheet(deaths_url, range = \"A5:F15\")\n\n✔ Reading from \"deaths\".\n\n\n✔ Range 'A5:F15'.\n\ndeaths\n\n# A tibble: 10 × 6\n   Name      Profession   Age `Has kids` `Date of birth`     `Date of death`    \n   &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;lgl&gt;      &lt;dttm&gt;              &lt;dttm&gt;             \n 1 David Bo… musician      69 TRUE       1947-01-08 00:00:00 2016-01-10 00:00:00\n 2 Carrie F… actor         60 TRUE       1956-10-21 00:00:00 2016-12-27 00:00:00\n 3 Chuck Be… musician      90 TRUE       1926-10-18 00:00:00 2017-03-18 00:00:00\n 4 Bill Pax… actor         61 TRUE       1955-05-17 00:00:00 2017-02-25 00:00:00\n 5 Prince    musician      57 TRUE       1958-06-07 00:00:00 2016-04-21 00:00:00\n 6 Alan Ric… actor         69 FALSE      1946-02-21 00:00:00 2016-01-14 00:00:00\n 7 Florence… actor         82 TRUE       1934-02-14 00:00:00 2016-11-24 00:00:00\n 8 Harper L… author        89 FALSE      1926-04-28 00:00:00 2016-02-19 00:00:00\n 9 Zsa Zsa … actor         99 TRUE       1917-02-06 00:00:00 2016-12-18 00:00:00\n10 George M… musician      53 FALSE      1963-06-25 00:00:00 2016-12-25 00:00:00\n\n\nWhen you attempt to read in a sheet that requires authentication, googlesheets4 will direct you to a web browser with a prompt to sign in to your Google account and grant permission to operate on your behalf with Google Sheets. However, if you want to specify a specific Google account, authentication scope, etc. you can do so with gs4_auth(), e.g., gs4_auth(email = \"mine@example.com\"), which will force the use of a token associated with a specific email."
  },
  {
    "objectID": "manipu_reading.html#rdata-format",
    "href": "manipu_reading.html#rdata-format",
    "title": "5  Reading Data",
    "section": "5.6 RData Format",
    "text": "5.6 RData Format\nIf you need to store more than one dataset in a single file we can use the *.RData format:\nExample:\n## example to load multiple datasets, and a vector of R objects from a single *.RData\nload(\"robjects.RData\")\n## to check if objects have been loaded correctly\nobjects()"
  },
  {
    "objectID": "manipu_reading.html#import-stataspss-file",
    "href": "manipu_reading.html#import-stataspss-file",
    "title": "5  Reading Data",
    "section": "5.7 Import Stata/SPSS File",
    "text": "5.7 Import Stata/SPSS File\nTo import a Stata/SPSS file into R:\n\nFirst you need to call the foreign library:\ninstall.packages(\"foreign\")\nThen use read.data()/read.spss():\ndata &lt;- read.spss(file=\"data.spss\", to.data.frame=TRUE)\nThe output will always be a data frame:\nwrite.foreign(data, \"mydata.txt\", \"mydata.sps\", package=\"SPSS\")"
  },
  {
    "objectID": "manipu_reading.html#json-file",
    "href": "manipu_reading.html#json-file",
    "title": "5  Reading Data",
    "section": "5.8 JSON File",
    "text": "5.8 JSON File\nTo read a JSON file:\ninstall.packages(\"rjson\") \ndata &lt;- fromJSON(file=\"data.json\")\ndata2 &lt;- as.data.frame(data)"
  },
  {
    "objectID": "manipu_reading.html#view",
    "href": "manipu_reading.html#view",
    "title": "5  Reading Data",
    "section": "5.9 view()",
    "text": "5.9 view()\nTo view the data variable, you can use the view()."
  },
  {
    "objectID": "manipu_reading.html#manual-data-entry",
    "href": "manipu_reading.html#manual-data-entry",
    "title": "5  Reading Data",
    "section": "5.10 Manual Data Entry",
    "text": "5.10 Manual Data Entry\nSometimes you’ll need to assemble a tibble “by hand” doing a little data entry in your R script.\nThere are two useful functions to help you do this which differ in whether you layout the tibble by columns or by rows.\n\ntibble() works by column:\nExample:\n\ntidyr::tibble(\n  x = c(1, 2, 5), \n  y = c(\"h\", \"m\", \"g\"),\n  z = c(0.08, 0.83, 0.60)\n)   \n\n# A tibble: 3 × 3\n      x y         z\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 h      0.08\n2     2 m      0.83\n3     5 g      0.6 \n\n\ntribble() (transposed tibble) lets you lay out your data row by row: column headings start with ~ and entries are separated by commas. This makes it possible to lay out small amounts of data in an easy to read form:\nExample:\n\ntidyr::tribble(\n  ~x, ~y, ~z,\n  1, \"h\", 0.08,\n  2, \"m\", 0.83,\n  5, \"g\", 0.60\n)\n\n# A tibble: 3 × 3\n      x y         z\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 h      0.08\n2     2 m      0.83\n3     5 g      0.6"
  },
  {
    "objectID": "manipu_writing.html#writing-to-excel",
    "href": "manipu_writing.html#writing-to-excel",
    "title": "6  Writing Data",
    "section": "6.1 Writing to Excel",
    "text": "6.1 Writing to Excel\nYou can write data back to disk as an Excel file using the write_xlsx() from the writexl package:\nwrite_xlsx(bake_sale, path = \"data/bake-sale.xlsx\") \nIf you’re interested in additional features like writing to sheets within a spreadsheet and styling, you will want to use the openxlsx package."
  },
  {
    "objectID": "manipu_writing.html#writing-to-google-sheets",
    "href": "manipu_writing.html#writing-to-google-sheets",
    "title": "6  Writing Data",
    "section": "6.2 Writing to Google Sheets",
    "text": "6.2 Writing to Google Sheets\nYou can write from R to Google Sheets with write_sheet(). The first argument is the dataframe to write, and the second argument is the name (or other identifier) of the Google Sheet to write to:\nwrite_sheet(bake_sale, ss = \"bake-sale\")\nIf you’d like to write your data to a specific (work)sheet inside a Google Sheet, you can specify that with the sheet argument as well.\nwrite_sheet(bake_sale, ss = \"bake-sale\", sheet = \"Sales\")"
  },
  {
    "objectID": "manipu_writing.html#writing-a-dataframe",
    "href": "manipu_writing.html#writing-a-dataframe",
    "title": "6  Writing Data",
    "section": "6.3 Writing a Dataframe",
    "text": "6.3 Writing a Dataframe\nTo write a dataframe to a file we use this:\nwrite.table(x, file = \"\", append = FALSE, sep = \" \", row.names = TRUE, col.names = TRUE)\nHere x is the vector to be written. If x is a matrix or array then it is converted to a vector (column by column) before being written. The other parameters are optional.\nWe can identify the complete rows from a two-dimensional object such as a dataframe (that is, rows that have no missing values) via the complete.cases command. We can easily remove rows with missing values using the na.omit function."
  },
  {
    "objectID": "manipu_writing.html#writing-matrices",
    "href": "manipu_writing.html#writing-matrices",
    "title": "6  Writing Data",
    "section": "6.4 Writing Matrices",
    "text": "6.4 Writing Matrices\nBecause write() converts matrices to vectors before writing them, using it to write a matrix to a file can cause unexpected results. Since R stores its matrices by column, you should pass the transpose of the matrix to write if you want the output to reflect the matrix structure.\nx &lt;- matrix(1:24, nrow = 4, ncol = 6)\nwrite(t(x), file = \"../results/out.txt\", ncolumns = 6)"
  },
  {
    "objectID": "manipu_writing.html#cat",
    "href": "manipu_writing.html#cat",
    "title": "6  Writing Data",
    "section": "6.5 cat()",
    "text": "6.5 cat()\nA more flexible command for writing to a file is cat(), which has the form:\ncat(..., file = \"\", sep = \" \", append = FALSE)\nNote that cat does not automatically write a newline after the expressions …. If you want a newline you must explicitly include the string \\n."
  },
  {
    "objectID": "manipu_writing.html#tabular-data",
    "href": "manipu_writing.html#tabular-data",
    "title": "6  Writing Data",
    "section": "6.6 Tabular Data",
    "text": "6.6 Tabular Data\nFor writing tabular data to text files (i.e. CSV) or connections you may use write.table(), which has this format:\nExample:\nwrite.table()"
  },
  {
    "objectID": "manipu_writing.html#dump",
    "href": "manipu_writing.html#dump",
    "title": "6  Writing Data",
    "section": "6.7 dump()",
    "text": "6.7 dump()\nThere is also the very useful dump() function, which creates a text representation of almost any R object that can subsequently be read by source.\nExample:\nx &lt;- matrix(rep(1:5, 1:5), nrow = 3, ncol = 5)\ndump(\"x\", file = \"../results/x.txt\")\nrm(x)\nsource(\"../results/x.txt\")\nx"
  },
  {
    "objectID": "manipu_tidydata.html#installation",
    "href": "manipu_tidydata.html#installation",
    "title": "7  Tidy Data",
    "section": "7.1 Installation",
    "text": "7.1 Installation\n\nInstall all the packages in the tidyverse by running:\ninstall.packages(\"tidyverse\")\nRun library(tidyverse) to load the core tidyverse and make it available in your current R session.\nNote the conflicts message that’s printed when you load the tidyverse. It tells you that dplyr overwrites some functions in base R. If you want to use the base version of these functions after loading dplyr, you’ll need to use their full names: stats::filter() and stats::lag().\nLearn more about the tidyverse package https://tidyverse.tidyverse.org"
  },
  {
    "objectID": "manipu_tidydata.html#core-packages",
    "href": "manipu_tidydata.html#core-packages",
    "title": "7  Tidy Data",
    "section": "7.2 Core Packages",
    "text": "7.2 Core Packages\nlibrary(tidyverse) will load the core tidyverse packages:\n\nggplot2, for data visualisation, more info.\ndplyr, for data manipulation, more info.\ntidyr, for data tidying, more info\nreadr, for data import, more info\npurrr, for functional programming, more info.\ntibble, for tibbles, a modern re-imagining of data frames, more info.\nstringr, for strings, more info\nforcats, for factors, [more info] (https://forcats.tidyverse.org/)\nlubridate, for date/times."
  },
  {
    "objectID": "manipu_tidydata.html#functionalities",
    "href": "manipu_tidydata.html#functionalities",
    "title": "7  Tidy Data",
    "section": "7.3 Functionalities",
    "text": "7.3 Functionalities\n\n7.3.1 Import\nAs well as readr, for reading flat files, the tidyverse package installs a number of other packages for reading data:\n\nDBI for relational databases. You’ll need to pair DBI with a database specific backends like RSQLite, RMariaDB, RPostgres, or odbc. More info here.\nhaven for SPSS, Stata, and SAS data.\nhttr for web APIs.\nreadxl for .xls and .xlsx sheets.\ngooglesheets4 for Google Sheets via the Sheets API v4.\ngoogledrive for Google Drive files.\nrvest for web scraping.\njsonlite for JSON. (Maintained by Jeroen Ooms.)\nxml2 for XML.\n\n\n\n7.3.2 Wrangle\nIn addition to tidyr, and dplyr, there are five packages (including stringr and forcats) which are designed to work with specific types of data:\n\nlubridate for dates and date-times.\nhms for time-of-day values.\nblob for storing blob (binary) data.\n\nThere are also two packages that allow you to interface with different backends using the same dplyr syntax:\n\ndbplyr allows you to use remote database tables by converting dplyr code into SQL.\ndtplyr provides a data.table backend by automatically translating to the equivalent, but usually much faster, data.table code. Program\n\n\n\n7.3.3 Programming\nIn addition to purrr, which provides very consistent and natural methods for iterating on R objects, there are two additional tidyverse packages that help with general programming challenges:\n\nmagrittr provides the pipe, %&gt;% used throughout the tidyverse. It also provide a number of more specialised piping operators (like %$% and %&lt;&gt;%) that can be useful in other places.\nglue provides an alternative to paste() that makes it easier to combine data and strings.\n\n\n\n7.3.4 Modeling\nModeling with the tidyverse uses the collection of tidymodels packages, which largely replace the modelr package used in R4DS.\nVisit the Getting Started Guide for more detailed examples, or go straight to the Learn page."
  },
  {
    "objectID": "manipu_tidydata.html#tidying-data",
    "href": "manipu_tidydata.html#tidying-data",
    "title": "7  Tidy Data",
    "section": "7.4 Tidying Data",
    "text": "7.4 Tidying Data\nWe’ll focus on tidyr, a package that provides a bunch of tools to help tidy up messy datasets. tidyr is a member of the core tidyverse.\nlibrary(tidyverse)\nThere are two main advantages of tidy data:\n\nThere’s a general advantage to picking one consistent way of storing data. If you have a consistent data structure, it’s easier to learn the tools that work with it because they have an underlying uniformity.\nThere’s a specific advantage to placing variables in columns because it allows R’s vectorized nature to shine.\n\n\n7.4.1 Pivot Data\nYou’ll begin by figuring out what the underlying variables and observations are. Sometimes this is easy; other times you’ll need to consult with the people who originally generated the data. Next, you’ll pivot your data into a tidy form, with variables in the columns and observations in the rows.\ntidyr provides two functions for pivoting data: pivot_longer() and pivot_wider(). We’ll first start with pivot_longer() because it’s the most common case. Let’s dive into some examples.\n\n\n7.4.2 Lengthening Data\nLengthening data means increasing the number of rows and decreasing the number of columns. The inverse transformation is pivot_wider().\nIn this dataset, each observation is a song. The first three columns (artist, track and date.entered) are variables that describe the song. Then we have 76 columns (wk1-wk76) that describe the rank of the song in each week1. Here, the column names are one variable (the week) and the cell values are another (the rank).\n\ntidyr::billboard\n\n# A tibble: 317 × 79\n   artist     track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n   &lt;chr&gt;      &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac      Baby… 2000-02-26      87    82    72    77    87    94    99    NA\n 2 2Ge+her    The … 2000-09-02      91    87    92    NA    NA    NA    NA    NA\n 3 3 Doors D… Kryp… 2000-04-08      81    70    68    67    66    57    54    53\n 4 3 Doors D… Loser 2000-10-21      76    76    72    69    67    65    55    59\n 5 504 Boyz   Wobb… 2000-04-15      57    34    25    17    17    31    36    49\n 6 98^0       Give… 2000-08-19      51    39    34    26    26    19     2     2\n 7 A*Teens    Danc… 2000-07-08      97    97    96    95   100    NA    NA    NA\n 8 Aaliyah    I Do… 2000-01-29      84    62    51    41    38    35    35    38\n 9 Aaliyah    Try … 2000-03-18      59    53    38    28    21    18    16    14\n10 Adams, Yo… Open… 2000-08-26      76    76    74    69    68    67    61    58\n# ℹ 307 more rows\n# ℹ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;, …\n\n\nTo tidy this data, we’ll use pivot_longer():\n\ntidyr::billboard |&gt; \n  tidyr::pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE # to get rid of NAs\n  )\n\n# A tibble: 5,307 × 5\n   artist  track                   date.entered week   rank\n   &lt;chr&gt;   &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk1      87\n 2 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk2      82\n 3 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk3      72\n 4 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk4      77\n 5 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk5      87\n 6 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk6      94\n 7 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk7      99\n 8 2Ge+her The Hardest Part Of ... 2000-09-02   wk1      91\n 9 2Ge+her The Hardest Part Of ... 2000-09-02   wk2      87\n10 2Ge+her The Hardest Part Of ... 2000-09-02   wk3      92\n# ℹ 5,297 more rows\n\n\nThis data is now tidy, but we could make future computation a bit easier by converting values of week from character strings to numbers using mutate() and readr::parse_number(). parse_number() is a handy function that will extract the first number from a string, ignoring all other text.\n\nbillboard_longer &lt;- tidyr::billboard |&gt; \n  tidyr::pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE\n  ) |&gt; \n  dplyr::mutate(\n    week = readr::parse_number(week)\n  )\nbillboard_longer\n\n# A tibble: 5,307 × 5\n   artist  track                   date.entered  week  rank\n   &lt;chr&gt;   &lt;chr&gt;                   &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac   Baby Don't Cry (Keep... 2000-02-26       1    87\n 2 2 Pac   Baby Don't Cry (Keep... 2000-02-26       2    82\n 3 2 Pac   Baby Don't Cry (Keep... 2000-02-26       3    72\n 4 2 Pac   Baby Don't Cry (Keep... 2000-02-26       4    77\n 5 2 Pac   Baby Don't Cry (Keep... 2000-02-26       5    87\n 6 2 Pac   Baby Don't Cry (Keep... 2000-02-26       6    94\n 7 2 Pac   Baby Don't Cry (Keep... 2000-02-26       7    99\n 8 2Ge+her The Hardest Part Of ... 2000-09-02       1    91\n 9 2Ge+her The Hardest Part Of ... 2000-09-02       2    87\n10 2Ge+her The Hardest Part Of ... 2000-09-02       3    92\n# ℹ 5,297 more rows\n\n\nOther cases on how to deal with lenghtening data can be found here.\n\n\n7.4.3 Widening data\npivot_wider() makes datasets wider by increasing columns and reducing rows and helps when one observation is spread across multiple rows.\nExample:\n\ntidyr::cms_patient_experience\n\n# A tibble: 500 × 5\n   org_pac_id org_nm                           measure_cd measure_title prf_rate\n   &lt;chr&gt;      &lt;chr&gt;                            &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n 1 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       63\n 2 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       87\n 3 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       86\n 4 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       57\n 5 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       85\n 6 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       24\n 7 0446162697 ASSOCIATION OF UNIVERSITY PHYSI… CAHPS_GRP… CAHPS for MI…       59\n 8 0446162697 ASSOCIATION OF UNIVERSITY PHYSI… CAHPS_GRP… CAHPS for MI…       85\n 9 0446162697 ASSOCIATION OF UNIVERSITY PHYSI… CAHPS_GRP… CAHPS for MI…       83\n10 0446162697 ASSOCIATION OF UNIVERSITY PHYSI… CAHPS_GRP… CAHPS for MI…       63\n# ℹ 490 more rows\n\n\nThe core unit being studied is an organization, but each organization is spread across six rows, with one row for each measurement taken in the survey organization. We can see the complete set of values for measure_cd and measure_title by using distinct():\n\ntidyr::cms_patient_experience |&gt; \n  dplyr::distinct(measure_cd, measure_title)\n\n# A tibble: 6 × 2\n  measure_cd   measure_title                                                    \n  &lt;chr&gt;        &lt;chr&gt;                                                            \n1 CAHPS_GRP_1  CAHPS for MIPS SSM: Getting Timely Care, Appointments, and Infor…\n2 CAHPS_GRP_2  CAHPS for MIPS SSM: How Well Providers Communicate               \n3 CAHPS_GRP_3  CAHPS for MIPS SSM: Patient's Rating of Provider                 \n4 CAHPS_GRP_5  CAHPS for MIPS SSM: Health Promotion and Education               \n5 CAHPS_GRP_8  CAHPS for MIPS SSM: Courteous and Helpful Office Staff           \n6 CAHPS_GRP_12 CAHPS for MIPS SSM: Stewardship of Patient Resources             \n\n\nWe’ll use measure_cd as the source for our new column names for now. Instead of choosing new column names, we need to provide the existing columns that define the values (values_from) and the column name (names_from):\n\ntidyr::cms_patient_experience |&gt; \n  tidyr::pivot_wider(\n    names_from = measure_cd,\n    values_from = prf_rate\n  )\n\n# A tibble: 500 × 9\n   org_pac_id org_nm           measure_title CAHPS_GRP_1 CAHPS_GRP_2 CAHPS_GRP_3\n   &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 0446157747 USC CARE MEDICA… CAHPS for MI…          63          NA          NA\n 2 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          87          NA\n 3 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          86\n 4 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          NA\n 5 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          NA\n 6 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          NA\n 7 0446162697 ASSOCIATION OF … CAHPS for MI…          59          NA          NA\n 8 0446162697 ASSOCIATION OF … CAHPS for MI…          NA          85          NA\n 9 0446162697 ASSOCIATION OF … CAHPS for MI…          NA          NA          83\n10 0446162697 ASSOCIATION OF … CAHPS for MI…          NA          NA          NA\n# ℹ 490 more rows\n# ℹ 3 more variables: CAHPS_GRP_5 &lt;dbl&gt;, CAHPS_GRP_8 &lt;dbl&gt;, CAHPS_GRP_12 &lt;dbl&gt;\n\n\nThe output doesn’t look quite right; we still seem to have multiple rows for each organization. That’s because, we also need to tell pivot_wider() which column or columns have values that uniquely identify each row; in this case those are the variables starting with “org”:\n\ntidyr::cms_patient_experience |&gt; \n  tidyr::pivot_wider(\n    id_cols = starts_with(\"org\"),\n    names_from = measure_cd,\n    values_from = prf_rate\n  )\n\n# A tibble: 95 × 8\n   org_pac_id org_nm CAHPS_GRP_1 CAHPS_GRP_2 CAHPS_GRP_3 CAHPS_GRP_5 CAHPS_GRP_8\n   &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 0446157747 USC C…          63          87          86          57          85\n 2 0446162697 ASSOC…          59          85          83          63          88\n 3 0547164295 BEAVE…          49          NA          75          44          73\n 4 0749333730 CAPE …          67          84          85          65          82\n 5 0840104360 ALLIA…          66          87          87          64          87\n 6 0840109864 REX H…          73          87          84          67          91\n 7 0840513552 SCL H…          58          83          76          58          78\n 8 0941545784 GRITM…          46          86          81          54          NA\n 9 1052612785 COMMU…          65          84          80          58          87\n10 1254237779 OUR L…          61          NA          NA          65          NA\n# ℹ 85 more rows\n# ℹ 1 more variable: CAHPS_GRP_12 &lt;dbl&gt;"
  },
  {
    "objectID": "manipu_transforming.html#headand-tail",
    "href": "manipu_transforming.html#headand-tail",
    "title": "8  Transforming Data",
    "section": "8.1 head()and tail()",
    "text": "8.1 head()and tail()\nAfter reading the file, you can use the head() and tail() functions to examine the object:\nExample:\nhead(dataset)\ntail()"
  },
  {
    "objectID": "manipu_transforming.html#changing-variable-types",
    "href": "manipu_transforming.html#changing-variable-types",
    "title": "8  Transforming Data",
    "section": "8.2 Changing Variable Types",
    "text": "8.2 Changing Variable Types\nA common task after reading in data is to consider variable types. For example, if a variable in a dataset is a categorical variable with a known set of possible values but you see it is represented as a character, then you will want to make it a factor. Or a numeric variable is a character variable because one of the observations is typed out as a character instead of a numeric (e.g. “five” instead of 5).\nExample:\nstudents |&gt;\n  mutate(meal_plan = factor(meal_plan),\n  age = parse_number(if_else(age == \"five\", \"5\", age))\nAfter this, you will that the type of variable denoted underneath the variable name has changed from character () to factor ()."
  },
  {
    "objectID": "manipu_transforming.html#changing-columns-names",
    "href": "manipu_transforming.html#changing-columns-names",
    "title": "8  Transforming Data",
    "section": "8.3 Changing Columns Names",
    "text": "8.3 Changing Columns Names\nNote that you can assign your own column names after reading a dataframe using the names() function, or when you read it in, using the col.names argument, which should be assigned a character vector the same length as the number of columns.\nIf there is no header and no col.names argument, then R uses the names V1, V2, etc."
  },
  {
    "objectID": "manipu_transforming.html#columns-no-syntatic-names",
    "href": "manipu_transforming.html#columns-no-syntatic-names",
    "title": "8  Transforming Data",
    "section": "8.4 Columns No-Syntatic Names",
    "text": "8.4 Columns No-Syntatic Names\nSometimes, you might also notice that the columns names are surrounded by backticks. That’s because they contain spaces, breaking R’s usual rules for variable names; they’re non-syntactic names.\n*Example:*\n\n``` R\nstudents |&gt; \n  rename(\n    student_id = `Student ID`,\n    full_name = `Full Name`\n  )\n```\nAn alternative approach is to use janitor::clean_names() to use some heuristics to turn them all into snake case at once1.\nstudents |&gt; janitor::clean_names()"
  },
  {
    "objectID": "manipu_transforming.html#changing-numbers",
    "href": "manipu_transforming.html#changing-numbers",
    "title": "8  Transforming Data",
    "section": "8.5 Changing Numbers",
    "text": "8.5 Changing Numbers\nIn most cases, numbers will be in one of R’s numeric types: integer or double. In some cases, however, you’ll encounter them as strings, possibly because you’ve created them by pivoting from column headers or because something has gone wrong in your data import process.\nreadr provides two useful functions for parsing strings into numbers: parse_double() and parse_number().\nExample\nExample:\n\nx &lt;- c(\"1.2\", \"5.6\", \"1e3\")\nparse_double(x)\n\n[1]    1.2    5.6 1000.0\n\nx &lt;- c(\"$1,234\", \"USD 3,513\", \"59%\")\nparse_number(x)\n\n[1] 1234 3513   59"
  },
  {
    "objectID": "manipu_transforming.html#removing-duplicates",
    "href": "manipu_transforming.html#removing-duplicates",
    "title": "8  Transforming Data",
    "section": "8.6 Removing Duplicates",
    "text": "8.6 Removing Duplicates\nYou can remove duplicates based on the x variable using:\nx &lt;- c(1, 2, NA, 4, NA, 5) \ndata[!duplicated(data$x), ]"
  },
  {
    "objectID": "manipu_transforming.html#reshaping-datasets",
    "href": "manipu_transforming.html#reshaping-datasets",
    "title": "8  Transforming Data",
    "section": "8.7 Reshaping Datasets",
    "text": "8.7 Reshaping Datasets\nStatistical analysis sometimes requires wide data and sometimes long data. In such cases, we need to be able to fluently and fluidly reshape the data to meet the requirements of statistical analysis. The function reshape() reshapes a dataframe between ‘wide’ format (with repeated measurements in separate columns of the same row) and ‘long’ format (with the repeated measurements in separate rows).\nData reshaping is just a rearrangement of the form of the data—it does not change the content of the dataset.\nExample:\nstudents &lt;- data.frame(sid=c(1,1,2,2), exmterm=c(1,2,1,2), math=c(50,65,75,69), \nliterature=c(40,45,55,59), language=c(70,80,75,78))\n\n# Reshaping dataset using reshape function to wide format\nwide_students &lt;- reshape(students, direction=\"wide\", idvar=\"sid\", timevar=\"exmterm\")\n\n# Now again reshape to long format\nlong_students &lt;- reshape (wide_students, direction=\"long\", idvar=\"id\")\n\n8.7.1 The reshape package\n\nMelting data (molten data):\n\nThough melting can be applied to different R objects, the most common use is to melt a data frame.\nTo perform melting operations using the melt function, we need to know what the identification variables and measured variables in the original input dataset are.\nOne important thing to note is that, whenever we use the melt function, all the measured variables should be of the same type, that is, the measured variables should be either numeric, factor, character, or date.\nTo deal with the implicit missing value, it is good to use na.rm = TRUE with the melt function to remove the structural missing value (i.e., it will fill empty cells in the data table with NA).\n\nExample:\n## the format of the resulting table is id/variable/value\nmelt(students, id=c(\"sid\",\"exmterm\"), measured=c(\"math\", \"literature\", \"language\"))\nCasting molten data:\n\nOnce we have molten data, we can rearrange it in any layout using the cast function from the reshape package.\nThere are two main arguments required to cast molten data: data and formula.\nThe basic casting formula is col_var_1+col_var_2 ~ row_var_1+ row_var_2, which describes the variables to appear in columns and rows.\n\nExample:\n# to return to the original data structure  \ncast(molten_students, sid+exmterm ~ variable)\nFor faster and large data rearrangement, use the reshape2 package and the functions dcast (data frames) and acast (arrays):\nExample:\nacast(molten_students, sid+exmterm~variable)"
  },
  {
    "objectID": "manipu_transforming.html#missing-values",
    "href": "manipu_transforming.html#missing-values",
    "title": "8  Transforming Data",
    "section": "8.8 Missing Values",
    "text": "8.8 Missing Values\nR represents missing observations through the data value NA. It is easiest to think of NA values as place holders for data that should have been there, but, for some reason, are not.\n\n8.8.1 Detect NA\nWe can detect whether variables are missing value using:\n\nis.na() is used to test objects if they are NA.\nis.nan() is used to test for NaN.\n\nExample:\n\nx &lt;- c(1, 2, NaN, NA, 4) \nis.na(x) \n\n[1] FALSE FALSE  TRUE  TRUE FALSE\n\nis.nan(x)\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\n\nTo check if there is any NA in a dataframe:\nExample:\n\nany(is.na(x))\n\n[1] TRUE\n\n\n\n\n8.8.2 Remove NAs\nExample:\n\na &lt;- c(11, NA, 13)\nmean(a, na.rm = TRUE)\n\n[1] 12\n\n\n\n\n8.8.3 Extract NA from a vector\nExample:\n\nx &lt;- c(1, 2, NA, 3) \nz &lt;- is.na(x) \nx[!z]\n\n[1] 1 2 3\n\n\n\n\n8.8.4 NA vs NULL\nNote that NA and NULL are not equivalent:\n\nNA is a place holder for something that exists but is missing.\nNULL stands for something that never existed at all.\n\n\n\n8.8.5 Removing NA values\nYou can remove rows with NA values in any variables:\nna.omit(data)\n\n## Another example\nx &lt;- c(1, 2, NA, 4, NA, 5) \nbad &lt;- is.na(x) \nx[!bad]\n\n[1] 1 2 4 5\n\n\nHow can you take the subset with no missing values in any of those objects?:\nExample:\ngood &lt;- complete.cases(airquality) \nhead(airquality[good,])"
  },
  {
    "objectID": "manipu_subsetting.html#operator",
    "href": "manipu_subsetting.html#operator",
    "title": "9  Subsetting R Objects",
    "section": "9.1 [ operator",
    "text": "9.1 [ operator\nThe [ operator always returns an object of the same class as the original:\n\nx &lt;- c(\"a\", \"b\", \"c\", \"c\", \"d\", \"a\")\n# Extract the first element\nx[1]\n\n[1] \"a\"\n\n\n\n# other examples\nx[1:4]\n\n[1] \"a\" \"b\" \"c\" \"c\"\n\nx[c(1, 3, 49)]\n\n[1] \"a\" \"c\" NA \n\n\n\nx &lt;- 100:110\ni &lt;- c(1, 3 ,2)\nx[i]\n\n[1] 100 102 101\n\n\n\nu &lt;- x &gt; \"a\" \nx[u] ## or x[x &gt; \"a\"]\n\ninteger(0)\n\n\n\nSubsetting a matrix:\nx[1,2] ## Extract the first row, first column \nx[1, ] ## Extract the first row \nx[, 2] ## Extract the second column \nx[1, 2, drop = FALSE] ## keeps the matrix format\nIt can be used to select multiple elements of an object (also in lists or dataframes):\n\n(x &lt;- list(foo = 1:4, bar = 0.6, baz = \"hello\"))\n\n$foo\n[1] 1 2 3 4\n\n$bar\n[1] 0.6\n\n$baz\n[1] \"hello\"\n\nx[c(1, 3)]\n\n$foo\n[1] 1 2 3 4\n\n$baz\n[1] \"hello\"\n\nx[\"foo\"]\n\n$foo\n[1] 1 2 3 4\n\nclass(x[\"foo\"])\n\n[1] \"list\"\n\nx$foo[1]\n\n[1] 1\n\nx$baz[1]\n\n[1] \"hello\"\n\n\n## use of negative subscript removes first element \"3\" \nnum10[-1]"
  },
  {
    "objectID": "manipu_subsetting.html#operator-1",
    "href": "manipu_subsetting.html#operator-1",
    "title": "9  Subsetting R Objects",
    "section": "9.2 [[ operator",
    "text": "9.2 [[ operator\nThe [[ operator is used to extract elements of a list or a dataframe. It can only be used to extract a single element and the class of the returned object will not necessarily be a list or data frame.\n\nSubsetting a Dataframe:\n# Use the notation [[ ]] to extract columns\nx[[\"var1\"]] = x[, 1] = x$var1 # All are equivalent\nSubsetting a List:\nx[[1]] ## Extract single element from a list \nx[[\"bar\"]] ## Extract named index\nx$bar ## Extract named index \nx[[c(1, 3)]] ## Get the 3rd element of the 1st element of the list \nx[[1]][[3]] ## Get the 3rd element of the 1st element of the list\nNow if we want to get access to the individual elements of list_obj[[2]], we have to use the following command:\n\n(data_2variable &lt;- data.frame(x1=c(2,3,4,5,6), x2=c(5,6,7,8,1)))\n\n  x1 x2\n1  2  5\n2  3  6\n3  4  7\n4  5  8\n5  6  1\n\n(list_obj &lt;- list(dat=data_2variable, vec.obj=c(1,2,3))) \n\n$dat\n  x1 x2\n1  2  5\n2  3  6\n3  4  7\n4  5  8\n5  6  1\n\n$vec.obj\n[1] 1 2 3\n\nlist_obj[[2]][1]\n\n[1] 1"
  },
  {
    "objectID": "manipu_subsetting.html#operator-2",
    "href": "manipu_subsetting.html#operator-2",
    "title": "9  Subsetting R Objects",
    "section": "9.3 $ operator",
    "text": "9.3 $ operator\nThe $ operator is used to extract elements of a list or data frame by literal name. Its semantics are similar to that of [[.\nExample:\n\nx[[\"bar\"]] ## Extract named index \n\n[1] 0.6\n\nx$bar ## Extract named index\n\n[1] 0.6"
  },
  {
    "objectID": "manipu_plyrdplyr.html#plyr",
    "href": "manipu_plyrdplyr.html#plyr",
    "title": "10  Plyr & Dplyr",
    "section": "10.1 Plyr",
    "text": "10.1 Plyr\nThe most important utility of the plyr package is that a single line of code can perform all the split(), apply(), and combine() steps.\nThe steps for the split-apply-combine approach of data analysis are as follows:\n\nFirst, we split the dataset into some mutually exclusive groups.\nWe then apply a task on each group and combine all the results to get the desired output.\nThis group-wise task could be generating new variables, summarizing existing variables, or even performing regression analysis on each group.\nFinally, combining approaches helps us get a nice output to compare the results from different groups.\n\nExample:\n\nlibrary(plyr)\nddply(iris, .(Species), function(x) colMeans(x[-5]))\n\n     Species Sepal.Length Sepal.Width Petal.Length Petal.Width\n1     setosa        5.006       3.428        1.462       0.246\n2 versicolor        5.936       2.770        4.260       1.326\n3  virginica        6.588       2.974        5.552       2.026\n\n\n\nThe first argument is the name of the data frame. We put iris, since the iris dataset is in the data frame structure, and we want to work on it.\nThe second argument is for a variable or variables, according to which we want to split our data frame. In this case, we have Species.\nThe third argument is a function that defines what kind of task we want to perform on each subset.\n\nNote that the first letter of the function name specifies the input, and the second letter specifies the output type:\n\n\n\nTable 1: Types of funcctions in the plyr package\n\n\nNote: mapply() can take multiple inputs as separate arguments, whereas a*ply() takes only a single array argument."
  },
  {
    "objectID": "manipu_plyrdplyr.html#dplyr",
    "href": "manipu_plyrdplyr.html#dplyr",
    "title": "10  Plyr & Dplyr",
    "section": "10.2 Dplyr",
    "text": "10.2 Dplyr\nQuite often, in real-life situations, we start our analysis with a dataframe-type structure. What do we do after getting a dataset and what are the basic data-manipulation tasks we usually perform before starting modeling?:\n\nCheck the validity of a dataset based on conditions.\nSort the dataset based on some variables, in ascending or descending order.\nCreate new variables based on existing variables.\nFinally, summarize them.\n\ndplyr can work with other data frame “backends” such as SQL databases. In fact, there is an SQL interface for relational databases via the DBI package\ndplyr can also be integrated with the data.table package for large fast tables.\n\n10.2.1 dplyr Grammar\nAll dplyr functions have a few common characteristics:\n\nThe first argument is a dataframe.\nThe subsequent arguments typically describe which columns to operate on using the variable names (without quotes).\nThe output is a new datafram (dplyr doesn’t modify the existing original dataset because dplyr functions never modify their inputs).\n\ndplyr’s verbs are organized into four groups based on what they operate on: rows, columns, groups, or tables.\n\nTable 2. Dplyr functions\n\n\nColumns\nRows\n\n\n\n\nselect()\nfilter() (base: subset)\n\n\nrename()\narrange()\n\n\nmutate()\ndistinct()\n\n\nrelocate()\nslice()\n\n\n\nBecause each verb does one thing well, solving complex problems will usually require combining multiple verbs, and we’ll do so with the pipe, |&gt;.\nFor verbs to work at all, dataframes must be properly formatted and annotated. In particular, the data must be tidy, that is, the data comply with the following three interrelated rules:\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\nNote that there’s a specific advantage to placing variables in columns because it allows R’s vectorized nature to shine. dplyr, ggplot2, and all the other packages in the tidyverse are designed to work with tidy data.\n\n\n10.2.2 The Pipe\nThe real power of dplyr arises when you start to combine multiple verbs. For example, imagine that you wanted to find the fastest flights to Houston’s IAH airport: you need to combine filter(), mutate(), select(), and arrange():\nThe pipe takes the thing on its left and passes it along to the function on its right so that x |&gt; f(y) is equivalent to f(x, y), and x |&gt; f(y) |&gt; g(z) is equivalent to g(f(x, y), z). The easiest way to pronounce the pipe is “then”.\nExample:\nflights |&gt; \n  filter(dest == \"IAH\") |&gt; \n  mutate(speed = distance / air_time * 60) |&gt; \n  select(year:day, dep_time, carrier, flight, speed) |&gt; \n  arrange(desc(speed))\nEven though this pipeline has four steps, it’s easy to skim because the verbs come at the start of each line: start with the flights data, then filter, then mutate, then select, then arrange.\nIMPORTANT: Chaining (|&gt;) is a powerful feature of dplyr that allows the output from one verb to be piped into the input of another verb using a short, easy-to-read syntax. To add the pipe to your code, we recommend using the built-in keyboard shortcut Ctrl/Cmd + Shift + M. You’ll need to make one change to your RStudio options to use |&gt; instead of %&gt;%.\n\n\n10.2.3 filter()\nThe filter() function is used to extract subsets of rows from a dataframe. This function is similar to the existing subset().\nExample:\nchic.f &lt;- filter(chicago, pm25tmean2 &gt; 30 & tmpd &gt; 80) \nsummary(chic.f$pm25tmean2)\nThe tidyverse alternative writing:\n# Flights with a departure dely higher than 120 mins\nflights |&gt; \n  filter(dep_delay &gt; 120)\nSometimes, it is more important to subset the dataframe based on values of a variable or multiple variables.\nExample:\nfilter(iris, Species==\"virginica\")\nfilter(iris, Species==\"virginica\" & Sepal.Length &lt;6 & Sepal. Width &lt;=2.7)\nTidyverse:\n# Flights that departed on January 1\nflights |&gt; \n  filter(month == 1 & day == 1)\n# Flights that departed in January or February\nflights |&gt; \n  filter(month == 1 | month == 2)\nThere’s a useful shortcut when you’re combining | and ==: %in%. It keeps rows where the variable equals one of the values on the right:\nflights |&gt; \n  filter(month %in% c(1, 2))\n\n\n10.2.4 arrange()\nThe arrange() function is used to change the order of the rows of a dataframe according to the value of the variables/columns (while preserving corresponding order of other columns).\nExample 1:\nchicago &lt;- arrange(chicago, date) \n## Columns can be arranged in descending order\nchicago &lt;- arrange(chicago, desc(date))\narrange(iris, Sepal.Length, desc(Sepal.Width))\nIt takes a data frame and a set of column names (or more complicated expressions) to order by. If you provide more than one column name, each additional column will be used to break ties in the values of the preceding columns.\nTidyverse:\n# Sorts by the departure time, which is spread over four columns\n# We get the earliest years first, then within a year, the earliest months, etc.\nflights |&gt; \n  arrange(year, month, day, dep_time)\n# Use desc() to re-order the df based on that column in big-to-small order  \nflights |&gt; \n  arrange(desc(dep_delay))\nNote that the number of rows has not changed – we’re only arranging the data, we’re not filtering it.\n\n\n10.2.5 distinct()\nSometimes, we might encounter duplicate observations in a data frame. The distinct() function helps eliminates these observations (finds all the unique rows in a dataset).\nExample:\ndistinct(iris, Species, Petal.Width)\nTidyverse:\nflights |&gt; \n  distinct()\nMost of the time, however, you’ll want the distinct combination of some variables, so you can also optionally supply column names:\nflights |&gt; \n  distinct(origin, dest)\n  \n# Keep other columns when filtering for unique rows\nflights |&gt; \n  distinct(origin, dest, .keep_all = TRUE)\nIf you want to find the number of occurrences instead, you’re better off swapping distinct() for count(). With the sort = TRUE argument, you can arrange them in descending order of the number of occurrences.\nExample:\nflights |&gt;\n  count(origin, dest, sort = TRUE)\n\n\n10.2.6 slice()\nYou can extract the subset of a dataframe using the slice() function.\nExample:\nslice(iris, 95:105)\nThere are five handy functions that allow you to extract specific rows within each group:\n# Takes the first row from each group.\ndf |&gt; slice_head(n = 1)\n# Takes the last row in each group\ndf |&gt; slice_tail(n = 1) .\n# Takes the row with the smallest value of column x\ndf |&gt; slice_min(x, n = 1) \n# Takes the row with the largest value of column x\ndf |&gt; slice_max(x, n = 1) \n# Takes one random row\ndf |&gt; slice_sample(n = 1)\nYou can vary n to select more than one row, or instead of n =, you can use prop = 0.1 to select (e.g.) 10% of the rows in each group. For example, the following code finds the flights that are most delayed upon arrival at each destination:\nExample:\nflights |&gt; \n  group_by(dest) |&gt; \n  slice_max(arr_delay, n = 1) |&gt;\n  relocate(dest)\n\n\n10.2.7 select()\nMost of the time, you do not work on all the variables in a dataframe. Selecting a few columns could make the analysis process less complicated. The select() function can be used to select columns of a data frame that you want to focus on.\nExample:\nchicago &lt;- readRDS(\"chicago.rds\")\nnames(chicago)[1:3] \nselect(chicago, c(\"city\", \"tmpd\"))\nselect(chicago, c(1, 3))\nsubset &lt;- select(chicago, city:dptp)\nTidyverse:\n\nSelect columns by name:\nflights |&gt; \n  select(year, month, day)\nSelect all columns between year and day (inclusive):\nflights |&gt; \n  select(year:day)\nSelect all columns except those from year to day (inclusive):\nflights |&gt; \n  select(!year:day)\nSelect all columns that are characters:\nflights |&gt; \n  select(where(is.character))\n\nThere are a number of helper functions you can use within select():\n\nstarts_with(\"abc\"): matches names that begin with “abc”.\nends_with(\"xyz\"): matches names that end with “xyz”.\ncontains(\"ijk\"): matches names that contain “ijk”.\nnum_range(\"x\", 1:3): matches x1, x2 and x3.\n\nYou can rename variables as you select() them by using =. The new name appears on the left-hand side of the =, and the old variable appears on the right-hand side\nExample:\nflights |&gt; \n  select(tail_num = tailnum)\nYou can also omit variables using the select() function by using the negative sign:\nExample:\nselect(chicago, -(city:dptp))\n\n\n10.2.8 rename()\nRenaming a variable in a dataframe in R is surprisingly hard to do! The rename() function is designed to make this process easier.\nExample:\nchicago &lt;- rename(chicago, dewpoint = dptp, pm25 = pm25tmean2)\nThe syntax inside the rename() function is to have the new name on the left-hand side of the = sign and the old name on the right-hand side.\nTidyverse\nflights |&gt; \n  rename(tail_num = tailnum)\n\n\n10.2.9 mutate()\nThe mutate() function exists to compute transformations of variables in a dataframe. Very often, you want to create new variables that are derived from existing variables and mutate() provides a clean interface for doing that (it adds new columns that are calculated from the existing columns).\nExample:\n# Create a pm25detrend variable that subtracts the mean from the pm25 variable\nchicago &lt;- mutate(chicago, pm25detrend = pm25 - mean(pm25, na.rm = TRUE))\n# Other example\nmutate(iris, SLm=Sepal.Length/100, SWm= Sepal.Width/100, PLm=Petal. Length/100, PWm= Petal.Width/100 )\nTidyverse:\nflights |&gt; \n  mutate(\n    gain = dep_delay - arr_delay,\n    speed = distance / air_time * 60\n  )\nYou can use the .before argument to add the variables to the left-hand side.\nExample:\nflights |&gt; \n  mutate(\n    gain = dep_delay - arr_delay,\n    speed = distance / air_time * 60,\n    .before = 1\n  )\nYou can also use .after to add after a variable, and in both .before and .after you can use the variable name instead of a position.\nNote that since we haven’t assigned the result of the above computation back to flights, the new variables gain, and speed will only be printed but will not be stored in a dataframe.\nAlternatively, you can control which variables are kept with the .keep argument. A particularly useful argument is \"used\" which specifies that we only keep the columns that were used in the “create” step with mutate(). For example, the following output will contain only the variables dep_delay, arr_delay, air_time, gain, hours, and gain_per_hour.\nflights |&gt; \n  mutate(\n    gain = dep_delay - arr_delay,\n    hours = air_time / 60,\n    gain_per_hour = gain / hours,\n    .keep = \"used\"\n  )\nIf you want to keep only the new variables and drop the old ones, we could use the transmute() function:\nExample:\n## Here we detrend the PM10 and ozone (O3) variables\ntransmute(chicago, pm10detrend = pm10tmean2 - mean(pm10tmean2, na.rm = TRUE), \n    o3detrend = o3tmean2 - mean(o3tmean2, na.rm = TRUE)))\n## Note that there are only two columns in the transmuted data frame\n\n\n10.2.10 relocate()\nUse relocate() to move variables around. You might want to collect related variables together or move important variables to the front. By default relocate() moves variables to the front.\nTidyverse\nflights |&gt; \n  relocate(time_hour, air_time)\nYou can also specify where to put them using the .before and .after arguments, just like in mutate():\nflights |&gt; \n  relocate(year:dep_time, .after = time_hour)\nflights |&gt; \n  relocate(starts_with(\"arr\"), .before = dep_time)\n\n\n10.2.11 group_by()\nThe group_by() function is used to divide your dataset into groups meaningful for your analysis. You will usually use the group_by() function in conjunction with the summarize() function.\nExample 1:\nflights |&gt; \n  group_by(month)\nYou can create groups using more than one variable. For example, we could make a group for each date.\nExample:\ndaily &lt;- flights |&gt;  \n  group_by(year, month, day)\nNote that group_by() doesn’t change the data but, if you look closely at the output, you’ll notice that the output indicates that it is “grouped by” month. This means subsequent operations will now work “by month”. group_by() adds this grouped feature (referred to as class) to the dataframe, which changes the behavior of the subsequent verbs applied to the data.\nYou might also want to remove grouping from a dataframe without using summarize(). You can do this with ungroup().\nExample:\ndaily |&gt; \n  ungroup()\n\n\n10.2.12 summarize()\nThe summarize() function is used to calculate a single summary statistic, and reduces the data frame to have a single row for each group.\nExample:\nflights |&gt; \n  group_by(month) |&gt; \n  summarize(\n    avg_delay = mean(dep_delay)\n  )\nYou can create any number of summaries in a single call to summarize(). One very useful summary is n(), which returns the number of rows in each group.\nExample:\nflights |&gt; \n  group_by(month) |&gt; \n  summarize(\n    avg_delay = mean(dep_delay, na.rm = TRUE), \n    n = n()\n  )\n\n\n10.2.13 Chaining (%&gt;%)\nSometimes, it could be necessary to use multiple functions to perform a single task. The pipeline operator %&gt;% is very handy for stringing together multiple dplyr functions in a sequence of operations.\nThis nesting is not a natural way to think about a sequence of operations:\nthird(second(first(x)))\nThe %&gt;% operator allows you to string operations in a left-to-right fashion:\nfirst(x) %&gt;% second(x) %&gt;% third(x)\nThis way we don’t have to create a set of temporary variables along the way or create a massive nested sequence of function calls.\nOnce you travel down the pipeline with %&gt;%, the first argument is taken to be the output of the previous element in the pipeline."
  },
  {
    "objectID": "manipu_date.html#date-and-time-classes",
    "href": "manipu_date.html#date-and-time-classes",
    "title": "11  Date Manipulation",
    "section": "11.1 Date and Time Classes",
    "text": "11.1 Date and Time Classes\nYou can use the Sys.Date and Sys.time functions to pull date and time objects respectively:\n\n(date &lt;- Sys.Date())\n\n[1] \"2024-10-15\"\n\nclass(date)\n\n[1] \"Date\"\n\n(time_ct &lt;- Sys.time())\n\n[1] \"2024-10-15 20:45:44 CEST\"\n\nclass(time_ct)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nBy default, the Sys.time function returns an object of the POSIXct, POSIXt, or POSIXct class. We can use the as.POSIXlt function to convert the object to a POSIXlt object:\n\n(time_lt &lt;- as.POSIXlt(time_ct))\n\n[1] \"2024-10-15 20:45:44 CEST\"\n\nclass(time_lt)\n\n[1] \"POSIXlt\" \"POSIXt\" \n\n\nWhile both the POSIXct and POSIXlt objects have the same representation, the key difference between the two is in the method in which each object is stored internally in the time details. The POSIXct object stored the numeric distance of the time object from the origin point. On the other hand, POSIXlt returned a detailed list with the different time components:\n\nunclass(time_ct)\n\n[1] 1729017945\n\nunclass(time_lt)\n\n$sec\n[1] 44.55685\n\n$min\n[1] 45\n\n$hour\n[1] 20\n\n$mday\n[1] 15\n\n$mon\n[1] 9\n\n$year\n[1] 124\n\n$wday\n[1] 2\n\n$yday\n[1] 288\n\n$isdst\n[1] 1\n\n$zone\n[1] \"CEST\"\n\n$gmtoff\n[1] 7200\n\nattr(,\"tzone\")\n[1] \"\"     \"CET\"  \"CEST\"\nattr(,\"balanced\")\n[1] TRUE\n\n# You can pull the day of the year\nunclass(time_lt)$yday\n\n[1] 288"
  },
  {
    "objectID": "manipu_date.html#creating-date-and-time-objects",
    "href": "manipu_date.html#creating-date-and-time-objects",
    "title": "11  Date Manipulation",
    "section": "11.2 Creating Date and Time Objects",
    "text": "11.2 Creating Date and Time Objects\nCreating a new object or converting the existing object to Date, POSIXlt, or POSIXct can be done with as.Date, as.POSIXlt, or as.POSIXct respectively.\nThe built-in R function as.Date() can handle only dates but not time. For instance, we can convert the “2014-5-12” string into a Date object using the as.Date function:\nExample:\n\nclass(\"2014-5-12\")\n\n[1] \"character\"\n\nclass(as.Date(\"1970-01-01\"))\n\n[1] \"Date\"\n\n# You can convert the Date object into a number\nclass(as.numeric(as.Date(\"1970-01-01\")))\n\n[1] \"numeric\"\n\n\nNote that if the format of the input object is different from the ISO 8601 standard, using the as.Date function without declaring the object format structure would return incorrect results (and, in some instances, an error). For example, let’s try to convert the 31-01-2018 date to Date format without reformatting it:\nExample:\n\nas.Date(\"31-01-2018\")\n\n[1] \"0031-01-20\"\n\n\nOne way to solve this issue is by adding the format argument in the as.Date function in order to map the different components of the input object to the structure of the Date object. The mapping should be according to the date components order of the input object:\n\nas.Date(\"31-01-2018\", format = \"%d-%m-%Y\")\n\n[1] \"2018-01-31\"\n\n\nYou can find a summary of the main arguments for date objects using ?strptime.\nThe as.POSIXct or as.POSIXlt function works in a similar way:\n\n(time_ct &lt;- as.POSIXct(\"2014-5-12 20:05:35\", tz = \"EST\"))\n\n[1] \"2014-05-12 20:05:35 EST\"\n\nclass(time_ct)\n\n[1] \"POSIXct\" \"POSIXt\""
  },
  {
    "objectID": "manipu_date.html#reformatting-and-converting-date-objects",
    "href": "manipu_date.html#reformatting-and-converting-date-objects",
    "title": "11  Date Manipulation",
    "section": "11.3 Reformatting and Converting Date Objects",
    "text": "11.3 Reformatting and Converting Date Objects\nReformatting or converting date objects is the process of transforming a non-date (or POSIXct/lt) object such as character or numeric to a Date format (or POSIXct/lt).\nExample:\n\nurl &lt;- \"https://github.com/PacktPublishing/Hands-On-Time-Series-Analysis-with-R/raw/refs/heads/master/Chapter02/dates_formats.csv\"\ndates_df &lt;- read.csv(url, stringsAsFactors = FALSE)\nstr(dates_df)\n\n'data.frame':   22 obs. of  7 variables:\n $ Japanese_format     : chr  \"2017/1/20\" \"2017/1/21\" \"2017/1/22\" \"2017/1/23\" ...\n $ US_format           : chr  \"1/20/2017\" \"1/21/2017\" \"1/22/2017\" \"1/23/2017\" ...\n $ US_long_format      : chr  \"Friday, January 20, 2017\" \"Saturday, January 21, 2017\" \"Sunday, January 22, 2017\" \"Monday, January 23, 2017\" ...\n $ CA_mix_format       : chr  \"January 20, 2017\" \"January 21, 2017\" \"January 22, 2017\" \"January 23, 2017\" ...\n $ SA_mix_format       : chr  \"20 January 2017\" \"21 January 2017\" \"22 January 2017\" \"23 January 2017\" ...\n $ NZ_format           : chr  \"20/01/2017\" \"21/01/2017\" \"22/01/2017\" \"23/01/2017\" ...\n $ Excel_Numeric_Format: int  42755 42756 42757 42758 42759 42760 42761 42762 42763 42764 ...\n\n\nThe first six columns are character objects and the seventh object is numeric. Let’s convert each one of the columns to a date object by identifying the date structure and reformat it accordingly.\n\n# Convert 1st column to Date format (ISO 8601 format)\ndates_df$Japanese_format_new &lt;- as.Date(dates_df$Japanese_format)\nclass(dates_df$Japanese_format)\n\n[1] \"character\"\n\nclass(dates_df$Japanese_format_new)\n\n[1] \"Date\"\n\n\n\n# Convert the 2nd column (no ISO 8601 format)\ndates_df$US_format_new &lt;- as.Date(dates_df$US_format, format = \"%m/%d/%Y\")\n\nThe same logic applies to the remaining columns:\n# Convert the 3rd column\ndates_df$US_long_format_new &lt;- as.Date(dates_df$US_long_format, format = \"%A, %B %d, %Y\")\n# Convert the 4th column\ndates_df$CA_mix_format_new &lt;- as.Date(dates_df$CA_mix_format, format = \"%B %d, %Y\")\n# Convert the 5th column\ndates_df$SA_mix_format_new &lt;- as.Date(dates_df$SA_mix_format, format = \"%d %B %Y\")\n# Convert the 6th column\ndates_df$NZ_format_new &lt;- as.Date(dates_df$NZ_format, format = \"%d/%m/%Y\")\nIn Excel, the numeric value of the origin point (that is January 1st, 1900) set to 1, as opposed to other programming languages such as R which define the origin point as 0. Therefore, when importing a date or time objects from Excel, in order to align to R origin point definition, the origin point should be set as December 31st, 1899 (which equivalent to 0 numeric value). Therefore, since Excel is using a different origin point than R (December 30, 1899 versus January 1, 1970), we will have to add the origin argument and specify the original date that is used to generate the data:\nExample:\n\nhead(dates_df$Excel_Numeric_Format)\n\n[1] 42755 42756 42757 42758 42759 42760\n\ndates_df$Excel_Numeric_Format_new &lt;- as.Date(dates_df$Excel_Numeric_Format, origin = as.Date(\"1899-12-30\"))\nhead(dates_df$Excel_Numeric_Format_new)\n\n[1] \"2017-01-20\" \"2017-01-21\" \"2017-01-22\" \"2017-01-23\" \"2017-01-24\"\n[6] \"2017-01-25\""
  },
  {
    "objectID": "manipu_date.html#reformatting-and-converting-time-objects",
    "href": "manipu_date.html#reformatting-and-converting-time-objects",
    "title": "11  Date Manipulation",
    "section": "11.4 Reformatting and Converting Time Objects",
    "text": "11.4 Reformatting and Converting Time Objects\nSimilar to the as.Date function, the as.POSIXct or as.POSIXlt functions are the base package applications for reformatting and conversion of any time input to a POSIXct or POSIXlt objects, respectively.\nThe POSIX classes are an extension of the Date class, with the addition of four elements (in addition to the date elements): hours, minutes, seconds, and time zone. The mapping will now include seven elements instead of four elements, as was the case with Date class.\n\n# Input follows the ISO 8601 standard\ntime_str &lt;- \"2018-12-31 23:59:59\"\nclass(time_str)\n\n[1] \"character\"\n\ntime_posix_ct1 &lt;- as.POSIXct(time_str)\nclass(time_posix_ct1)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nLet’s convert now the number 1546318799 to a time object using the as.POSIXct function with the origin argument:\nExample:\n\ntime_numeric &lt;- 1546318799\nclass(time_numeric)\n\n[1] \"numeric\"\n\ntime_posix_ct2 &lt;- as.POSIXct(time_numeric, origin = \"1970-01-01\")\nprint(c(time_posix_ct1, time_posix_ct2))\n\n[1] \"2018-12-31 23:59:59 CET\" \"2019-01-01 05:59:59 CET\"\n\n\nHowever, whenever the format of the input object does not follow a YYYY-m-d H:M:S structure, you will have to use the format argument to map the object’s elements.\nExample:\n\n# Monday, December 31, 2018 11:59:59 PM.\ntime_US_str &lt;- \"Monday, December 31, 2018 11:59:59 PM\"\n(time_posix_ct3 &lt;- as.POSIXct(time_US_str, format = \"%A, %B %d, %Y %I:%M:%S %p\"))\n\n[1] NA\n\n\nThe full arguments list can also be found using ?strptime."
  },
  {
    "objectID": "manipu_date.html#time-zone-setting",
    "href": "manipu_date.html#time-zone-setting",
    "title": "11  Date Manipulation",
    "section": "11.5 Time Zone Setting",
    "text": "11.5 Time Zone Setting\nThe time zone is the seventh element of the POSIX classes and can be set by either the tz argument of the as.POSIXct/as.POSIXlt functions or by the format argument.\nExample:\n\nSys.timezone()\n\n[1] \"Europe/Madrid\"\n\ntime_str &lt;- \"2024-09-30 11:25:49\"\ntime_default_tz &lt;- as.POSIXct(time_str)\n\n# Load a date object from different time zone\ntime_assign_tz &lt;- as.POSIXct(time_str, tz = \"GMT\")\n\nprint(c(time_default_tz, time_assign_tz))\n\n[1] \"2024-09-30 11:25:49 CEST\" \"2024-09-30 13:25:49 CEST\"\n\n\nA full list of the 592 time zones available in R (both location and abbreviation formats) can be found in the OlsonNames function:\n\nhead(OlsonNames(), 20)\n\n [1] \"Africa/Abidjan\"       \"Africa/Accra\"         \"Africa/Addis_Ababa\"  \n [4] \"Africa/Algiers\"       \"Africa/Asmara\"        \"Africa/Asmera\"       \n [7] \"Africa/Bamako\"        \"Africa/Bangui\"        \"Africa/Banjul\"       \n[10] \"Africa/Bissau\"        \"Africa/Blantyre\"      \"Africa/Brazzaville\"  \n[13] \"Africa/Bujumbura\"     \"Africa/Cairo\"         \"Africa/Casablanca\"   \n[16] \"Africa/Ceuta\"         \"Africa/Conakry\"       \"Africa/Dakar\"        \n[19] \"Africa/Dar_es_Salaam\" \"Africa/Djibouti\""
  },
  {
    "objectID": "manipu_date.html#creating-a-date-or-time-index-vector",
    "href": "manipu_date.html#creating-a-date-or-time-index-vector",
    "title": "11  Date Manipulation",
    "section": "11.6 Creating a Date or Time Index Vector",
    "text": "11.6 Creating a Date or Time Index Vector\nThe base package provides two pairs of functions, seq.Date and seq.POSIXt, to create a time index vector with Date or POSIX objects respectively. The main difference between the two functions (besides the class of the output) is the units of the time interval.\nIt will make sense to use the seq.Date function to generate a time sequence with daily frequency or lower (for example, weekly, monthly, and so on) and as.POSIXt in other instances (for higher frequencies than daily, such as hourly, half-hourly, or by minutes).\n\ndaily_index &lt;- seq.Date(from = as.Date(\"2016-01-01\"),\n                        to = as.Date(\"2018-12-31\"), \n                        by = \"day\")\nhead(daily_index)\n\n[1] \"2016-01-01\" \"2016-01-02\" \"2016-01-03\" \"2016-01-04\" \"2016-01-05\"\n[6] \"2016-01-06\"\n\ndaily_3_index &lt;- seq.Date(from = as.Date(\"2016-01-01\"),\n                          to = as.Date(\"2018-12-31\"),\n                          by = \"3 days\")\nhead(daily_3_index)\n\n[1] \"2016-01-01\" \"2016-01-04\" \"2016-01-07\" \"2016-01-10\" \"2016-01-13\"\n[6] \"2016-01-16\"\n\n\nFor example, let’s create an hourly sequence with a length of 48 hours, using the seq.POSIXt function:\n\nhourly_index &lt;- seq.POSIXt(from = as.POSIXct(\"2018-06-01\"), \n                           by = \"hours\",\n                           length.out = 48)\nstr(hourly_index)\n\n POSIXct[1:48], format: \"2018-06-01 00:00:00\" \"2018-06-01 01:00:00\" \"2018-06-01 02:00:00\" ..."
  },
  {
    "objectID": "manipu_date.html#lubridate-package",
    "href": "manipu_date.html#lubridate-package",
    "title": "11  Date Manipulation",
    "section": "11.7 lubridate Package",
    "text": "11.7 lubridate Package\nThe lubridate package offers alternative tools and applications for reformatting, converting, and handling date and time objects.\n\n11.7.1 Reformatting date and time objects\nTo see how simple it is to reformat date and time objects with the lubridate package,let’s go back to the complex time object (Monday, December 31, 2018 11:59:59 PM) we converted earlier to a POSIXct class:\nExample:\n\ntime_US_str &lt;- \"Monday, December 31, 2018 11:59:59 PM\"\nclass(time_US_str)\n\n[1] \"character\"\n\n\nNow let’s use the ymd_hms (which stands for a year, month, day, hour, minute, and second) family of conversion functions from the lubridate package to convert the object to a POSIXct object:\nExample:\n\nlibrary(lubridate)\ntime_lubridate &lt;- mdy_hms(time_US_str, tz = \"EST\")\nclass(time_lubridate)\n\n[1] \"POSIXct\" \"POSIXt\" \n\ntime_lubridate\n\n[1] \"2018-12-31 23:59:59 EST\"\n\n\nThis is much simpler than the as.POSIXct conversion. The ymd_hms function is able to automatically map the different time components of the input object, even when some components are redundant (such as the full weekday name in the example previously).\nThe conversion with ymd functions of the previous date_df example (https://github.com/PacktPublishing/Hands-On-Time-Series-Analysis-with-R/raw/refs/heads/master/Chapter02/dates_formats.csv) is as follows:\nExample:\ndates_df$Japanese_format_new &lt;- ymd(dates_df$Japanese_format)\ndates_df$US_format_new &lt;- mdy(dates_df$US_format)\ndates_df$US_long_format_new &lt;- mdy(dates_df$US_long_format)\ndates_df$CA_mix_format_new &lt;- mdy(dates_df$CA_mix_format)\ndates_df$SA_mix_format_new &lt;- dmy(dates_df$SA_mix_format)\ndates_df$NZ_format_new &lt;- dmy(dates_df$NZ_format)\nThe ymd function easily handles the different types of date formats; however, it is not designed to convert numeric values of date objects. This type of conversion can be done with the as_date function for date objects (or as_datetime for time objects), which works in the same manner as as.Date (or the as.POSIXct/as.POSIXlt functions) works with date numeric values:\nExample:\ndates_df$Excel_Numeric_Format_new &lt;- as_date(dates_df$Excel_Numeric_Format,\n        origin = ymd(\"1899-12-30\"))\nNote that you will get the same results with the ymd and the as_date functions as the results with as.Date.\nHowever, those functions (both ymd and ymd_hms) are not able to handle rare or extreme cases where one of the separators is a double string (such as a double apostrophe, ““) or some of the formats are not written well in a specific format of year, month, and day, such as 201811-01, 201811-1, or 111-2018 (as opposed to 2018-11-01 or 01-11-2018).\n\n\n11.7.2 Extraction functionality\n\nThis extracts the day of the year:\nyday(time_obj)\nThis extracts the day of the quarter:\nqday(time_obj)\nThis extracts the day of the month:\nday(time_obj)\nThis extracts the day of the week:\nwday(time_obj, label = TRUE)\n\n\n\n11.7.3 Modification functionality\n\nThis modifies the hour of the time object:\nhour(time_obj) &lt;- 11\nThis rounds the time object by the minute, hour, and day:\n\n# Origin: 2018-12-31 23:59:59 CET\n\nround_date(as.POSIXct(\"2018-12-31 23:59:59 CET\"), unit = \"minute\")\n\n[1] \"2019-01-01 CET\"\n\nfloor_date(as.POSIXct(\"2018-12-31 23:59:59 CET\"), unit = \"hour\")\n\n[1] \"2018-12-31 23:00:00 CET\"\n\nceiling_date(as.POSIXct(\"2018-12-31 23:59:59 CET\"), unit = \"day\")\n\n[1] \"2019-01-01 CET\"\n\n\n\nThose sets of functions are very useful when extracting mainly time objects from a different type of format."
  },
  {
    "objectID": "manipu_text.html#sources-of-text-data",
    "href": "manipu_text.html#sources-of-text-data",
    "title": "12  Text Manipulation",
    "section": "12.1 Sources of Text Data",
    "text": "12.1 Sources of Text Data\nText data can be found on tweets from any individual, or from any company, Facebook status updates, RSS feeds from any news site, Blog articles, Journal articles, Newspapers, Verbatim transcripts of an in-depth interview.\nFor example, t extract Twitter data, we can use tweetR() and, to extract data from Facebook, we could use facebookR()."
  },
  {
    "objectID": "manipu_text.html#getting-text-data",
    "href": "manipu_text.html#getting-text-data",
    "title": "12  Text Manipulation",
    "section": "12.2 Getting Text Data",
    "text": "12.2 Getting Text Data\nThe easiest way to get text data is to import from a .csv file where some of the variables contain character data. We have to protect automatic factor conversion by specifying the stringsAsFactors = FALSE argument\nExample 1: The tweets.txt file is the plain text file. We will import this file using the generic readLines() function. It is a vector of characters (not a data.frame).\nExample 2: Html (this is also a character string):\nconURL &lt;- \"http://en.wikipedia.org/wiki/R_%28programming_language%29\"\n# Establish the connection with the URL \nlink2URL &lt;- url(conURL) \n# Reading html code\nhtmlCode &lt;- readLines(link2URL)\n# Closing the connection\nclose(link2URL)\n# Printing the result \nhtmlCode\nThe tm text mining library has some other functions to import text data from various files such as PDF files, plain text files, and even from doc files."
  },
  {
    "objectID": "manipu_databases.html#database-basics",
    "href": "manipu_databases.html#database-basics",
    "title": "13  R and Data Bases",
    "section": "13.1 Database Basics",
    "text": "13.1 Database Basics\nAt the simplest level, you can think about a database as a collection of data frames, called tables in database terminology. Like a data frame, a database table is a collection of named columns, where every value in the column is the same type. There are three high level differences between data frames and database tables:\n\nR stores everything in RAM, and a typical personal computer consists of limited RAM. R is RAM intensive, and for that reason, the size of a dataset should be much smaller than its RAM. Database tables are stored on disk and can be arbitrarily large.\nDatabase tables almost always have indexes. Much like the index of a book, a database index makes it possible to quickly find rows of interest without having to look at every single row. Data frames and tibbles don’t have indexes, but data.tables do, which is one of the reasons that they’re so fast.\nMost classical databases are optimized for rapidly collecting data, not analyzing existing data. These databases are called row-oriented because the data is stored row-by-row, rather than column-by-column like R. More recently, there’s been much development of column-oriented databases that make analyzing the existing data much faster.\n\nDatabases are run by database management systems (DBMS’s for short), which come in three basic forms:\n\nClient-server DBMS’s run on a powerful central server, which you connect from your computer (the client). They are great for sharing data with multiple people in an organization. Popular client-server DBMS’s include PostgreSQL, MariaDB, SQL Server, and Oracle.\nCloud DBMS’s, like Snowflake, Amazon’s RedShift, and Google’s BigQuery, are similar to client server DBMS’s, but they run in the cloud. This means that they can easily handle extremely large datasets and can automatically provide more compute resources as needed.\nIn-process DBMS’s, like SQLite or duckdb, run entirely on your computer. They’re great for working with large datasets where you’re the primary user."
  },
  {
    "objectID": "manipu_databases.html#connecting-to-a-database",
    "href": "manipu_databases.html#connecting-to-a-database",
    "title": "13  R and Data Bases",
    "section": "13.2 Connecting to a Database",
    "text": "13.2 Connecting to a Database\nTo connect to the database from R, you’ll use a pair of packages:\n\nYou’ll always use DBI (database interface) because it provides a set of generic functions that connect to the database, upload data, run SQL queries, etc.\nYou’ll also use a package tailored for the DBMS you’re connecting to. This package translates the generic DBI commands into the specifics needed for a given DBMS. There’s usually one package for each DBMS, e.g. RPostgres for PostgreSQL and RMariaDB for MySQL.\n\nIf you can’t find a specific package for your DBMS, you can usually use the odbc package instead. This uses the ODBC protocol supported by many DBMS. odbc requires a little more setup because you’ll also need to install an ODBC driver and tell the odbc package where to find it.\nYou can create a database connection using DBI::dbConnect(). The first argument selects the DBMS, then the second and subsequent arguments describe how to connect to it (i.e. where it lives and the credentials that you need to access it). The following code shows a couple of typical examples:\nExample:\ncon &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(), \n  username = \"foo\"\n)\ncon &lt;- DBI::dbConnect(\n  RPostgres::Postgres(), \n  hostname = \"databases.mycompany.com\", \n  port = 1234\n)\n\n13.2.1 In-process DBMS\nSetting up a client-server or cloud DBMS would be a pain for this book, so we’ll instead use an in-process DBMS that lives entirely in an R package: duckdb. The only difference between using duckdb and any other DBMS is how you’ll connect to the database.\nduckdb is a high-performance database that’s designed very much for the needs of a data scientist. We use it here because it’s very easy to get started with, but it’s also capable of handling gigabytes of data with great speed. If you want to use duckdb for a real data analysis project, you’ll also need to supply the dbdir argument to make a persistent database and tell duckdb where to save it.\nConnecting to duckdb is particularly simple because the defaults create a temporary database that is deleted when you quit R. That’s great for learning because it guarantees that you’ll start from a clean slate every time you restart R:\ncon &lt;- DBI::dbConnect(duckdb::duckdb(), dbdir = \"duckdb\")\nSince this is a new database, we need to start by adding some data. Here we’ll add mpg and diamonds datasets from ggplot2 using DBI::dbWriteTable(). The simplest usage of dbWriteTable() needs three arguments: a database connection, the name of the table to create in the database, and a data frame of data.\nExample:\ndbWriteTable(con, \"mpg\", ggplot2::mpg)\ndbWriteTable(con, \"diamonds\", ggplot2::diamonds)\nYou can check that the data is loaded correctly by using a couple of other DBI functions: dbListTables() lists all tables in the database and dbReadTable() retrieves the contents of a table.\nExample:\ndbListTables(con)\n\ncon |&gt; \n  dbReadTable(\"diamonds\") |&gt; \n  as_tibble()\ndbReadTable() returns a data.frame so we use as_tibble() to convert it into a tibble so that it prints nicely.\nNow you can use (SQL language) dbGetQuery() to get the results of running a query on the database:\nExample:\nsql &lt;- \"\n  SELECT carat, cut, clarity, color, price \n  FROM diamonds \n  WHERE price &gt; 15000\n\"\nas_tibble(dbGetQuery(con, sql))\n\n\n13.2.2 Excel/MSAccess\nAn Excel file can be imported into R using ODBC. Remember Excel cannot deal with relational databases.\nWe will now create an ODBC connection with an MS Excel file with the connection string xlopen:\n\nIn our computer: To use the ODBC approach on an Excel file, we firstly need to create the connection string using the system administrator. We need to open the control panel of the operating system and then open Administrative Tools and then choose ODBC. A dialog box will now appear. Click on the Add button and select an appropriate ODBC driver and then locate the desired file and give a data source name. In our case, the data source name is xlopen.\nIn R:\n# calling ODBC library into R \nlibrary(RODBC)\n# creating connection with the database using odbc package. \nxldb &lt;- odbcConnect(\"xlopen\") / odbcConnect(\"accessdata\")\n# Now that the connection is created, we will use this connection and import the data xldata&lt;- sqlFetch(xldb, \"CSVanscombe\")\n\n\n\n13.2.3 Relational databases in R\n\nThere are packages to interface between R and different database software packages that use relational database management systems, such as MySQL (RMySQL), PostgreSQL (RPgSQL), and Oracle (ROracle).\nOne of the most popular packages is RMySQL. This package allows us to make connections between R and the MySQL server. In order to install this package properly, we need to download both the MySQL server and RMySQL.\nThere are several R packages available that allow direct interactions with large datasets within R, such as filehash, ff, and bigmemory. The idea is to avoid loading the whole dataset into memory."
  },
  {
    "objectID": "manipu_databases.html#dbplyr",
    "href": "manipu_databases.html#dbplyr",
    "title": "13  R and Data Bases",
    "section": "13.3 dbplyr",
    "text": "13.3 dbplyr\ndbplyr is a dplyr backend, which means that you keep writing dplyr code but the backend executes it differently. In this, dbplyr translates to SQL; other backends include dtplyr which translates to data.table, and multidplyr which executes your code on multiple cores.\nTo use dbplyr, you must first use tbl() to create an object that represents a database table:\nExample:\ndiamonds_db &lt;- tbl(con, \"diamonds\")\ndiamonds_db\nbig_diamonds_db &lt;- diamonds_db |&gt; \n  filter(price &gt; 15000) |&gt; \n  select(carat:clarity, price)\nbig_diamonds_db"
  },
  {
    "objectID": "manipu_databases.html#packages",
    "href": "manipu_databases.html#packages",
    "title": "13  R and Data Bases",
    "section": "13.4 Packages",
    "text": "13.4 Packages\n\n13.4.1 filehash package\nIt is used for solving large-data problems. The idea behind the development of this package was to avoid loading the dataset into a computer’s virtual memory. Instead, we dump the large dataset into the hard drive and then assign an environment name for the dumped objects.\nlibrary(filehash) \ndbCreate(\"exampledb\")\nfilehash_db&lt;- dbInit(\"exampledb\")  ## db needs to be initialized before accessing\ndbInsert(filehash_db, \"xx\", rnorm(50)) \nvalue&lt;- dbFetch(filehash_db, \"xx\")  ## to retrieve db values\nsummary(value)\nThis file connection will remain open until the database is closed via dbDisconnect or the database object in R is removed.\n\n\n13.4.2 ff package\nThis package extends the R system and stores data in the form of native binary flat files in persistent storage such as hard disks, CDs, or DVDs rather than in the RAM.\nThis package enables users to work on several large datasets simultaneously. It also allows the allocation of vectors or arrays that are larger than the RAM.\n\n\n13.4.3 sqldf package\nThe sqldf package is an R package that allows users to run SQL statements within R.\nWe can perform any type of data manipulation to an R data frame either in memory or during import.\nIf the dataset is too large and cannot entirely be read into the R environment, we can import a portion of that dataset using sqldf."
  },
  {
    "objectID": "stats_descriptive.html#summary-and-str",
    "href": "stats_descriptive.html#summary-and-str",
    "title": "14  Descriptive Statistics",
    "section": "14.1 summary() and str()",
    "text": "14.1 summary() and str()\nThe summary() and str() functions are the fastest ways to get descriptive statistics of the data.\n\nThe summary() function gives the basic descriptive statistics of the data.\nThe str() function gives the structure of the variables."
  },
  {
    "objectID": "stats_descriptive.html#measures-of-centrality",
    "href": "stats_descriptive.html#measures-of-centrality",
    "title": "14  Descriptive Statistics",
    "section": "14.2 Measures of Centrality",
    "text": "14.2 Measures of Centrality\n\n14.2.1 Mode\nThe mode is a value in data that has the highest frequency:\nExample:\n\na &lt;- c(1, 2, 3, 4, 5, 5, 5, 6, 7, 8)\n# To get mode in a vector you create a frequency table\n(y &lt;- table(a)) \n\na\n1 2 3 4 5 6 7 8 \n1 1 1 1 3 1 1 1 \n\nnames(y)[which(y==max(y))]\n\n[1] \"5\"\n\n\n\n\n14.2.2 Median\nThe median is the middle or midpoint of the data and is also the 50 percentile of the data.\nThe median is affected by the outliers and skewness of the data.\n\nmedian(a)\n\n[1] 5\n\n\n\n\n14.2.3 Mean\nThe mean is the average of the data. The mean works best if the data is distributed in a normal distribution or distributed evenly.\n\nmean(a)\n\n[1] 4.6"
  },
  {
    "objectID": "stats_descriptive.html#measures-of-variability",
    "href": "stats_descriptive.html#measures-of-variability",
    "title": "14  Descriptive Statistics",
    "section": "14.3 Measures of Variability",
    "text": "14.3 Measures of Variability\nThe measures of variability are the measures of the spread of the data. These are encompasses:\n\nVariance.\nStandard deviation.\nRange.\nInterquartile range.\nand more.\n\n\n14.3.1 Variance\nThe variance is the average of squared differences from the mean, and it is used to measure the spreadness of the data:\n\nPopulation variance:\n\n\nA &lt;- c(1, 2, 3, 4, 5, 5, 5, 6, 7, 8)\nN &lt;- length(A)\nvar(A) * (N - 1) / N\n\n[1] 4.24\n\n\n\nSample variance:\n\n\nvar(A)\n\n[1] 4.711111\n\n\n\n\n14.3.2 Standard deviation\nThe standard deviation is the square root of a variance and it measures the spread of the data.\n\nPopulation standard deviation:\n\n\nA &lt;- c(1, 2, 3, 4, 5, 5, 5, 6, 7, 8)\nN &lt;- length(A)\nvariance &lt;- var(A) * (N - 1) / N\nsqrt(variance)\n\n[1] 2.059126\n\n\n\nSample standard deviation:\n\n\nsd(A)\n\n[1] 2.170509\n\n\n\n\n14.3.3 range()\nThe range is the difference between the largest and smallest points in the data:\n\nrange(A)    \n\n[1] 1 8\n\nres &lt;- range(A)\ndiff(res)\n\n[1] 7\n\nmin(A)\n\n[1] 1\n\nmax(A)\n\n[1] 8\n\n\n\n\n14.3.4 Interquartile Range\nThe interquartile range is the measure of the difference between the 75 percentile or third quartile and the 25 percentile or first quartile.\n\nIQR(A)\n\n[1] 2.5\n\n\nYou can get the quartiles by using the quantile() function:\n\nquantile(A)\n\n  0%  25%  50%  75% 100% \n1.00 3.25 5.00 5.75 8.00"
  },
  {
    "objectID": "stats_descriptive.html#distributions",
    "href": "stats_descriptive.html#distributions",
    "title": "14  Descriptive Statistics",
    "section": "14.4 Distributions",
    "text": "14.4 Distributions\n\n14.4.1 Normal Distribution\nIf the points do not deviate away from the line, the data is normally distributed.\n\n\n\nFigure 1: The normal distribution\n\n\nTo see whether data is normally distributed, you can use the qqnorm() and qqline() functions:\nqqnorm(data$x) #You must first draw the distribution to draw the line afterwards \nqqline(data$x)\nYou can also use a Shapiro Test to test whether the data is normally distributed. If the p-value is more than 0.05, you can conclude that the data does not deviate from normal distribution:\nshapiro.test(data$x)\n\n\n14.4.2 Modality\nThe modality of a distribution can be seen by the number of peaks when we plot the histogram.\n\n\n\nFigure 2: The modality of a distribution\n\n\n\n\n14.4.3 Skewness\nSkewness is a measure of how symmetric a distribution is and how much the distribution is different from the normal distribution.\nNegative skew is also known as left skewed, and positive skew is also known as right skewed: - A positive skewness indicates that the size of the right-handed tail is larger than the left-handed tail. - A negative skewness indicates that the left-hand tail will typically be longer than the right-hand tail.\n\n\n\nFigure 3: Skewness of a distribution\n\n\nThe Pearson’s Kurtosis measure is used to see whether a dataset is heavy tailed, or light tailed. High kurtosis means heavy tailed, so there are more outliers in the data.\n\nWhen kurtosis is close to 0, then a normal distribution is often assumed. These are called mesokurtic distributions.\n\nWhen kurtosis&gt;0, then the distribution has heavier tails and is called a leptokurtic distribution.\nWhen kurtosis&lt;0, then the distribution is light tails and is called a platykurtic distribution.\n\nTo find the kurtosis and skewness in R, you must install the moments package:\ninstall.packages(\"moments\")\nskewness(data$x)\nkurtosis(data$x)\n\n\n14.4.4 Binomial Distribution\nA binomial distribution has two outcomes, success or failure, and can be thought of as the probability of success or failure in a survey that is repeated various times.\n\ndbinom(32, 100, 0.5)\n\n[1] 0.000112817"
  },
  {
    "objectID": "stats_exploratory.html#formulate-questions",
    "href": "stats_exploratory.html#formulate-questions",
    "title": "15  Exploratory Analysis Pipeline",
    "section": "15.1 Formulate Questions",
    "text": "15.1 Formulate Questions\nA sharp question or hypothesis can serve as a dimension reduction tool that can eliminate variables that are not immediately relevant to the question.\nIt’s usually a good idea to spend a few minutes to figure out what is the question you’re really interested in and narrow it down to be as specific as possible (without becoming uninteresting). One of the most important questions you can answer with an exploratory data analysis is: “Do I have the right data to answer this question?”\nThere is no universal rule about which questions you should ask to guide your research. However, two types of questions will always be useful for making discoveries within your data:\n\nWhat type of variation occurs within my variables?\nWhat type of covariation occurs between my variables?"
  },
  {
    "objectID": "stats_exploratory.html#read-in-your-data",
    "href": "stats_exploratory.html#read-in-your-data",
    "title": "15  Exploratory Analysis Pipeline",
    "section": "15.2 Read In Your Data",
    "text": "15.2 Read In Your Data\nSometimes the data will come in a very messy format, and you’ll need to do some cleaning and transformations.\nThe readr package is a nice package for reading in flat files very fast, or at least much faster than R’s built-in functions"
  },
  {
    "objectID": "stats_exploratory.html#check-the-packaging",
    "href": "stats_exploratory.html#check-the-packaging",
    "title": "15  Exploratory Analysis Pipeline",
    "section": "15.3 Check the “Packaging”",
    "text": "15.3 Check the “Packaging”\nAssuming you don’t get any warnings or errors when reading in the dataset, you should now have an object in your workspace, e.g. named “ozone”. It’s usually a good idea to poke at that object a little bit before we break open the wrapping paper. For example, you can check the number of rows and columns."
  },
  {
    "objectID": "stats_exploratory.html#run-str",
    "href": "stats_exploratory.html#run-str",
    "title": "15  Exploratory Analysis Pipeline",
    "section": "15.4 Run str()",
    "text": "15.4 Run str()\nThis is usually a safe operation in the sense that even with a very large dataset, running str() shouldn’t take too long.\nYou can examine the classes of each of the columns to make sure they are correctly specified (i.e., numbers are numeric, and strings are character, etc.)."
  },
  {
    "objectID": "stats_exploratory.html#look-top-and-bottom-of-your-data",
    "href": "stats_exploratory.html#look-top-and-bottom-of-your-data",
    "title": "15  Exploratory Analysis Pipeline",
    "section": "15.5 Look Top and Bottom of Your Data",
    "text": "15.5 Look Top and Bottom of Your Data\nThis lets me know if the data were read in properly, things are properly formatted, and that everything is there. If your data are time series data, then make sure the dates at the beginning and end of the dataset match what you expect the beginning and ending time period are.\nYou can peek at the top and bottom of the data with the head() and tail() functions. Sometimes there’s weird formatting at the end or some extra comment lines that someone decided to stick at the end."
  },
  {
    "objectID": "stats_exploratory.html#check-ns-frequency",
    "href": "stats_exploratory.html#check-ns-frequency",
    "title": "15  Exploratory Analysis Pipeline",
    "section": "15.6 Check “n”s (frequency)",
    "text": "15.6 Check “n”s (frequency)\nTo do this properly, you need to identify some landmarks that can be used to check against your data. For example, if you are collecting data on people, such as in a survey or clinical trial, then you should know how many people there are in your study (i.e., in an ozone monitoring data system we can take a look at the Time.Local variable to see what time measurements are recorded as being taken.)\ntable(ozone$Time.Local)"
  },
  {
    "objectID": "stats_exploratory.html#validate-with-external-data-source",
    "href": "stats_exploratory.html#validate-with-external-data-source",
    "title": "15  Exploratory Analysis Pipeline",
    "section": "15.7 Validate With External Data Source",
    "text": "15.7 Validate With External Data Source\nExternal validation can often be as simple as checking your data against a single number.Is the data are at least of the right order of magnitude (i.e., the units are correct)? or, is the range of the distribution roughly what we’d expect, given the regulation around ambient pollution levels?"
  },
  {
    "objectID": "stats_exploratory.html#check-the-variation-of-data",
    "href": "stats_exploratory.html#check-the-variation-of-data",
    "title": "15  Exploratory Analysis Pipeline",
    "section": "15.8 Check the Variation of Data",
    "text": "15.8 Check the Variation of Data\nVariation is the tendency of the values of a variable to change from measurement to measurement.\nEvery variable has its own pattern of variation, which can reveal interesting information about how that it varies between measurements on the same observation as well as across observations. The best way to understand a pattern is to visualize the distribution of the variable’s values.\nExample:\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n# Visualizing the carat from the diamonds dataset\nggplot(diamonds, aes(x = carat)) +\n  geom_histogram(binwidth = 0.5)\n\n\n\n\nNow that you can visualize variation, what should you look for in your plot? And what type of follow-up questions should you ask?\nThis histogram suggests several interesting questions:\n\nWhy are there more diamonds at whole carats and common fractions of carats?\nWhy are there more diamonds slightly to the right of each peak than there are slightly to the left of each peak?\n\nVisualizations can also reveal clusters, which suggest that subgroups exist in your data. To understand the subgroups, ask:\n\nHow are the observations within each subgroup similar to each other?\nHow are the observations in separate clusters different from each other?\nHow can you explain or describe the clusters?\n\n\n15.8.1 Typical values\nIn both bar charts and histograms, tall bars show the common values of a variable, and shorter bars show less-common values. Places that do not have bars reveal values that were not seen in your data. Now you can turn this information into useful questions:\n\nWhich values are the most common? Why?\nWhich values are rare? Why? Does that match your expectations?\nCan you see any unusual patterns? What might explain them?\n\nLet’s take a look at the distribution of carat for smaller diamonds.\nExample:\n\nsmaller &lt;- diamonds |&gt; \n              filter(carat &lt; 3)\nggplot(smaller, aes(x = carat)) +\n  geom_histogram(binwidth = 0.01)\n\n\n\n\nThis histogram suggests several interesting questions:\n\nWhy are there more diamonds at whole carats and common fractions of carats?\nWhy are there more diamonds slightly to the right of each peak than there are slightly to the left of each peak?\n\nVisualizations can also reveal clusters, which suggest that subgroups exist in your data. To understand the subgroups, ask:\n\nHow can you explain or describe the clusters?\nHow are the observations within each subgroup similar to each other?\nHow are the observations in separate clusters different from each other?\n\n\n\n15.8.2 Unusual values\nOutliers are observations that are unusual; data points that don’t seem to fit the pattern. Sometimes outliers are data entry errors, sometimes they are simply values at the extremes that happened to be observed in this data collection, and other times they suggest important new discoveries.\nWhen you have a lot of data, outliers are sometimes difficult to see in a histogram. For example, take the distribution of the y variable from the diamonds dataset. The only evidence of outliers is the unusually wide limits on the x-axis.\nExample:\n\nggplot(diamonds, aes(x = y)) + \n  geom_histogram(binwidth = 0.5) \n\n\n\n\nTo make it easy to see unusual values:\ncoord_cartesian(ylim = c(0, 50))\nIf you’ve encountered unusual values in your dataset, and simply want to move on to the rest of your analysis, you have two options.\n\nDrop the entire row with the strange values:\nExample:\n\ndiamonds2 &lt;- diamonds |&gt; \n  filter(between(y, 3, 20))\n\nThis is not recommended because one invalid value doesn’t imply that all the other values for that observation are also invalid.\nInstead, we recommend replacing the unusual values with missing values. The easiest way to do this is to use mutate() to replace the variable with a modified copy.\nExample:\n\ndiamonds2 &lt;- diamonds |&gt; \n    mutate(y = if_else(y &lt; 3 | y &gt; 20, NA, y))\nggplot(diamonds2, aes(x = x, y = y)) + \n    geom_point()\n\nWarning: Removed 9 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\nTo suppress that warning, set na.rm = TRUE."
  },
  {
    "objectID": "stats_exploratory.html#check-the-covariation",
    "href": "stats_exploratory.html#check-the-covariation",
    "title": "15  Exploratory Analysis Pipeline",
    "section": "15.9 Check the Covariation",
    "text": "15.9 Check the Covariation\nIf variation describes the behavior within a variable, covariation describes the behavior between variables. Covariation is the tendency for the values of two or more variables to vary together in a related way.\nThe best way to spot covariation is to visualize the relationship between two or more variables.\n\n15.9.1 Categorical and numerical variable\nFor example, let’s explore how the price of a diamond varies with its quality (measured by cut) using geom_freqpoly():\nExample:\n\nggplot(diamonds, aes(x = price)) + \n  geom_freqpoly(aes(color = cut), binwidth = 500, linewidth = 0.75)\n\n\n\n\nNote that ggplot2 uses an ordered color scale for cut because it’s defined as an ordered factor variable in the data.\nTo make the comparison easier we need to swap what is displayed on the y-axis. Instead of displaying count, we’ll display the density, which is the count standardized so that the area under each frequency polygon is one.\nExample:\n\nggplot(diamonds, aes(x = price, y = after_stat(density))) + \n  geom_freqpoly(aes(color = cut), binwidth = 500, linewidth = 0.75)\n\n\n\n\nThere’s something rather surprising about this plot: it seems that fair diamonds (the lowest quality) have the highest average price!\n\n\n15.9.2 Two categorical variables\nTo visualize the covariation between categorical variables, you’ll need to count the number of observations for each combination of levels of these categorical variables. One way to do that is to rely on the built-in geom_count():\n\nggplot(diamonds, aes(x = cut, y = color)) +\n  geom_count()\n\n\n\n\nThe size of each circle in the plot displays how many observations occurred at each combination of values. Covariation will appear as a strong correlation between specific x values and specific y values.\nAnother approach for exploring the relationship between these variables is computing the counts with dplyr:\n\ndiamonds |&gt; \n  count(color, cut)\n\n# A tibble: 35 × 3\n   color cut           n\n   &lt;ord&gt; &lt;ord&gt;     &lt;int&gt;\n 1 D     Fair        163\n 2 D     Good        662\n 3 D     Very Good  1513\n 4 D     Premium    1603\n 5 D     Ideal      2834\n 6 E     Fair        224\n 7 E     Good        933\n 8 E     Very Good  2400\n 9 E     Premium    2337\n10 E     Ideal      3903\n# ℹ 25 more rows\n\n\nThen visualize with geom_tile() and the fill aesthetic:\n\ndiamonds |&gt; \n  count(color, cut) |&gt;  \n  ggplot(aes(x = color, y = cut)) +\n  geom_tile(aes(fill = n))\n\n\n\n\nFor larger plots, you might want to try the heatmaply package, which creates interactive plots.\n\n\n15.9.3 Two numerical variables\nDrawing a scatterplot with geom_point() is one great way to visualize the covariation between two numerical variables. For example, you can see a positive relationship between the carat size and price of a diamond (diamonds with more carats have a higher price):\nExample:\n\nggplot(smaller, aes(x = carat, y = price)) +\n  geom_point()\n\n\n\n\nScatterplots become less useful as the size of your dataset grows, because points begin to overplot, and pile up into areas of uniform black, making it hard to judge differences in the density of the data across the 2-dimensional space as well as making it hard to spot the trend.\nTwo possible solutions are:\n\nUsing transparency: R     geom_point(alpha = 1 / 100)\nAnother solution is to use bin:\n\nExample:\n\n# geom_bin2d() creates rectangular bins\nggplot(smaller, aes(x = carat, y = price)) +\n  geom_bin2d()\n\n\n\n\n# geom_hex() creates hexagonal bins\n# You will need to install.packages(\"hexbin\")\nggplot(smaller, aes(x = carat, y = price)) +\n  geom_hex()"
  },
  {
    "objectID": "stats_exploratory.html#challenge-your-solutionbootstrap-sample",
    "href": "stats_exploratory.html#challenge-your-solutionbootstrap-sample",
    "title": "15  Exploratory Analysis Pipeline",
    "section": "15.10 Challenge Your Solution/Bootstrap Sample",
    "text": "15.10 Challenge Your Solution/Bootstrap Sample\nThe easy solution is generally nice because it is easy, but you should never allow those results to hold the day. You should always be thinking of ways to challenge the results, especially if those results collide with your prior expectation.\nFor example, How stable are the ozone rankings from year to year? We can imagine that from year to year, the ozone data are somewhat different randomly, but generally follow similar patterns across the country. So, the shuffling process could approximate the data changing from one year to the next. This not an ideal solution, but it could give us a sense of how stable the rankings are:\n\nFirst, we set our random number generator and resample the indices of the rows of the data frame with replacement. The statistical jargon for this approach is a bootstrap sample:\nWe use the resampled indices to create a new dataset, ozone2, that shares many of the same qualities as the original but is randomly perturbed.\nset.seed(10234) \nN &lt;- nrow(ozone) \nidx &lt;- sample(N, N, replace = TRUE) \nozone2 &lt;- ozone[idx, ]\nWe reconstruct our rankings of the counties based on this resampled data:\nranking2 &lt;- group_by(ozone2, State.Name, County.Name) %&gt;% \n  summarize(ozone = mean(Sample.Measurement)) %&gt;%\n  as.data.frame %&gt;%\n  arrange(desc(ozone))\nWe can then compare the top 10 counties from our original ranking and the top 10 counties from our ranking based on the resampled data.\ncbind(head(ranking, 10)\nhead(ranking2, 10))\nWe can see that the rankings based on the resampled data are very close to the original, with the first 7 being identical. Numbers 8 and 9 get flipped in the resampled rankings but that’s about it. This might suggest that the original rankings are somewhat stable."
  },
  {
    "objectID": "stats_exploratory.html#follow-up",
    "href": "stats_exploratory.html#follow-up",
    "title": "15  Exploratory Analysis Pipeline",
    "section": "15.11 Follow Up",
    "text": "15.11 Follow Up\nAt this point it’s useful to consider a few follow up questions:\n\nDo you have the right data?\nDo you need other data?\nDo you have the right question?\n\nThe goal of exploratory data analysis is to get you thinking about your data and reasoning about your question. At this point, we can refine our question or collect new data, all in an iterative process to get at the truth."
  },
  {
    "objectID": "stats_inferential.html#correlation",
    "href": "stats_inferential.html#correlation",
    "title": "16  Inferential Statistics",
    "section": "16.1 Correlation",
    "text": "16.1 Correlation\nCorrelations are statistical associations to find how close two variables are and to derive the linear relationships between them.\nYou can use correlation to find which variables are more related to the target variable and use this to reduce the number of variables.\nCorrelation does not mean a causal relationship, it does not tell you the how and why of the relationship.\ncor(data$var1, data$var2)\nThe correlation has a range from -1.0 to 1.0."
  },
  {
    "objectID": "stats_inferential.html#covariance",
    "href": "stats_inferential.html#covariance",
    "title": "16  Inferential Statistics",
    "section": "16.2 Covariance",
    "text": "16.2 Covariance\nCovariance is a measure of variability between two variables.\nThe greater the value of one variable and the greater of other variable means it will result in a covariance that is positive.\ncov(data$var1, data$var2)\nCovariance does not have a range. When two variables are independent of each other, the covariance is zero."
  },
  {
    "objectID": "stats_inferential.html#hypothesis-testing-and-p-value",
    "href": "stats_inferential.html#hypothesis-testing-and-p-value",
    "title": "16  Inferential Statistics",
    "section": "16.3 Hypothesis Testing and P-Value",
    "text": "16.3 Hypothesis Testing and P-Value\nBased on the research question, the hypothesis can be a null hypothesis, H0 (μ1= μ2) and an alternate hypothesis, Ha (μ1 ≠ μ2).\nFor data normally distributed:\n\np-value:\n\nA small p-value &lt;= alpha, which is usually 0.05, indicates that the observed data is sufficiently inconsistent with the null hypothesis, so the null hypothesis may be rejected. The alternate hypothesis is true at the 95% confidence interval.\nA larger p-value means that you failed to reject null hypothesis.\n\nt-test continuous variables of data.\nchi-square test for categorical variables or data.\nANOVA\n\nFor data not normally distributed:\n\nnon-parametric tests."
  },
  {
    "objectID": "stats_inferential.html#t-test",
    "href": "stats_inferential.html#t-test",
    "title": "16  Inferential Statistics",
    "section": "16.4 T-Test",
    "text": "16.4 T-Test\nA t-test is used to determine whether the mean between two data points or samples are equal to each other.\n\n\\(H_0\\) (\\(μ_1\\) = \\(μ_2\\)): The null hypothesis means that the two means are equal.\n\\(H_a\\) (\\(μ_1\\) ≠ \\(μ_2\\)): The alternative means that the two means are different.\n\nIn t-test there are two assumptions:\n\nThe population is normally distributed.\nThe samples are randomly sampled from their population.\n\nType I and Type II Errors:\n\nA Type I error is a rejection of the null hypothesis when it is really true.\nA Type II error is a failure to reject a null hypothesis that is false.\n\n\n16.4.1 One-Sample T-Test\nA one-sample t-test is used to test whether the mean of a population is equal to a specified mean.\nYou can use the t statistics and the degree of freedom (\\(df = n -1\\)) to estimate the p-value using a t-table.\nt.test(data$var1, mu=0.6) \n\n\n16.4.2 Two-Sample Independent T-Test (unpaired, paired = FALSE)\nThe two-sample unpaired t-test is when you compare two means of two independent samples. The degrees of freedom formula is \\(df = nA – nB – 2\\)\nIn the two-sample unpaired t-test, when the variance is unequal, you use the Welch t-test.\nt.test(data$var1, data\\$var2, var.equal=FALSE, paired=FALSE) \n\n\n16.4.3 Two-Sample Dependent T-Test (paired = TRUE)\nA two-sample paired t-test is used to test the mean of two samples that depend on each other. The degree of freedom formula is \\(df = n-1\\)\nt.test(data$var1, data$var2, paired=TRUE)"
  },
  {
    "objectID": "stats_inferential.html#chi-square-test",
    "href": "stats_inferential.html#chi-square-test",
    "title": "16  Inferential Statistics",
    "section": "16.5 Chi-Square Test",
    "text": "16.5 Chi-Square Test\nThe chi-square test is used to compare the relationships between two categorical variables.\nThe null hypothesis means that there is no relationship between the categorical variables.\n\n16.5.1 Goodness of Fit Test\nWhen you have only one categorical variable from a population and you want to compare whether the sample is consistent with a hypothesized distribution, you can use the goodness of fit test.\n\n\\(H_0\\): No significant difference between the observed and expected values.\n\\(H_A\\): There is a significant difference between the observed and expected values.\n\nTo use the goodness of fit chi-square test in R, you can use the chisq.test() function:\ndata &lt;- c(B=200, c=300, D=400)\nchisq.test(data)\n\n\n16.5.2 Contingency Test\nIf you have two categorical variables and you want to compare whether there is a relationship between two variables, you can use the contingency test.\n\n\\(H_0\\): the two categorical variables have no relationship. The two variables are independent.\n\\(H_A\\): the two categorical variables have a relationship. The two variables are not independent.\n\nvar1 &lt;- c(\"Male\", \"Female\", \"Male\", \"Female\", \"Male\")\nvar2 &lt;- c(\"chocolate\", \"strawberry\", \"strawberry\", \"strawberry\", \"chocolate\")\ndata &lt;- data.frame(var1, var2)\ndata.table &lt;- table(data$var1, data$var2)\ndata.table &gt; chisq.test(data.table)"
  },
  {
    "objectID": "stats_inferential.html#anova",
    "href": "stats_inferential.html#anova",
    "title": "16  Inferential Statistics",
    "section": "16.6 ANOVA",
    "text": "16.6 ANOVA\nANOVA is the process of testing the means of two or more groups. ANOVA also checks the impact of factors by comparing the means of different samples.\nIn ANOVA, you use two kinds of means:\n\nSample means.\nGrand mean (the mean of all of the samples’ means).\n\nHypothesis: - \\(H_0\\): \\(μ_1\\)= \\(μ_2\\) = … = \\(μ_L\\) ; the sample means are equal or do not have significant differences. - \\(H_A\\): \\(μ_1\\) ≠ \\(μ_m\\); is when the sample means are not equal.\nYou assume that the variables are sampled, independent, and selected or sampled from a population that is normally distributed with unknown but equal variances.\n\n16.6.1 Between Group Variability\nThe distribution of two samples, when they overlap, their means are not significantly different. Hence, the difference between their individual mean and the grand mean is not significantly different.\n\n\n\nFigure 6: Between group variability\n\n\nThis variability is called the between-group variability, which refers to the variations between the distributions of the groups or levels.\n\n\n16.6.2 Within Group Variability\nFor the following distributions of samples, as their variance increases, they overlap each other and become part of a population.\n\n\n\nFigure 7: Within group variability\n\n\nThe F-statistics are the measures if the means of samples are significantly different. The lower the F-statistics, the more the means are equal, so you cannot reject the null hypothesis.\n\n\n16.6.3 One-Way ANOVA\nOne-way ANOVA is used when you have only one independent variable.\n\nlibrary(graphics)\nset.seed(123) \nvar1 &lt;- rnorm(12, 2, 1) \nvar2 &lt;- c(\"B\", \"B\", \"B\", \"B\", \"C\", \"C\", \"C\", \"C\", \"C\", \"D\", \"D\", \"B\")\ndata &lt;- data.frame(var1, var2) \nfit &lt;- aov(data$var1 ~ data$var2, data = data)\nfit \n\nCall:\n   aov(formula = data$var1 ~ data$var2, data = data)\n\nTerms:\n                data$var2 Residuals\nSum of Squares   0.162695  9.255706\nDeg. of Freedom         2         9\n\nResidual standard error: 1.014106\nEstimated effects may be unbalanced\n\nsummary(fit)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\ndata$var2    2  0.163  0.0813   0.079  0.925\nResiduals    9  9.256  1.0284               \n\n\n\n\n16.6.4 Two-Way ANOVA\nTwo-way ANOVA is used when you have two independent variables. (continuar en el ejemplo anterior):\n\nvar3 &lt;- c(\"D\", \"D\", \"D\", \"D\", \"E\", \"E\", \"E\", \"E\", \"E\", \"F\", \"F\", \"F\")\ndata &lt;- data.frame(var1, var2, var3) \nfit &lt;- aov(data$var1 ~ data$var2 + data$var3, data=data)\nfit\n\nCall:\n   aov(formula = data$var1 ~ data$var2 + data$var3, data = data)\n\nTerms:\n                data$var2 data$var3 Residuals\nSum of Squares   0.162695  0.018042  9.237664\nDeg. of Freedom         2         1         8\n\nResidual standard error: 1.074573\n1 out of 5 effects not estimable\nEstimated effects may be unbalanced\n\nsummary(fit)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\ndata$var2    2  0.163  0.0813   0.070  0.933\ndata$var3    1  0.018  0.0180   0.016  0.904\nResiduals    8  9.238  1.1547               \n\n## var1 does not depend on var2's mean and var3's mean\n\n\n\n16.6.5 MANOVA\nThe multivariate analysis of variance is when there are multiple response variables that you want to test.\nExample:\n\nres &lt;- manova(cbind(iris$Sepal.Length, iris$Petal.Length) ~ iris$Species, data=iris) \nsummary(res)\n\n              Df Pillai approx F num Df den Df    Pr(&gt;F)    \niris$Species   2 0.9885   71.829      4    294 &lt; 2.2e-16 ***\nResiduals    147                                            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.aov(res)\n\n Response 1 :\n              Df Sum Sq Mean Sq F value    Pr(&gt;F)    \niris$Species   2 63.212  31.606  119.26 &lt; 2.2e-16 ***\nResiduals    147 38.956   0.265                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response 2 :\n              Df Sum Sq Mean Sq F value    Pr(&gt;F)    \niris$Species   2 437.10 218.551  1180.2 &lt; 2.2e-16 ***\nResiduals    147  27.22   0.185                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## Hence, you have two response variables, Sepal.Length and Petal.Length\n\nThe p-value is 2.2e-16, which is less than 0.05. Hence, you reject the null hypothesis"
  },
  {
    "objectID": "stats_inferential.html#nonparametric-test",
    "href": "stats_inferential.html#nonparametric-test",
    "title": "16  Inferential Statistics",
    "section": "16.7 Nonparametric Test",
    "text": "16.7 Nonparametric Test\nThe nonparametric test is a test that does not require the variable and sample to be normally distributed.\nYou use nonparametric tests when you do not have normally distributed data and the sample data is big.\n\nTable 3: Types of nonparametric tests\n\n\n\n\n\n\n\nNonparametric Test\nFunction\nMethod replaced\n\n\n\n\nWilcoxon Signed Rank Test\nwilcox.test(data[,1], mu=0, alternatives=\"two.sided\")\none-sample\n\n\nt-testundefined\n\n\n\n\nWilcoxon-Mann-Whitney Test\nwilcox.test(data[,1], data[,2], correct=FALSE)\nsubstitute\n\n\nto the two-sample t-test\n\n\n\n\nKruskal-Wallis Test\nkruskal.test(airquality$Ozone ~ airquality$Month)\none-way\n\n\n\n\n16.7.1 Wilcoxon Signed Rank Test\nThe Wilcoxon signed rank test is used to replace the one-sample t-test.\nHypothesis:\n\n\n\\(H_0\\): \\(μ_1\\)= \\(μ_o\\); the null hypothesis is that the population median has the specified value of \\(μ_0\\)\n\\(H_a\\): \\(μ_1\\) ≠ \\(μ_o\\)\n\nTo use the Wilcoxon signed rank test in R, you can first generate the data set using random.org packages, so that the variables are not normally distributed.\nExample:\ninstall.packages(\"random\") \nlibrary(random) \nvar1 &lt;- randomNumbers(n=100, min=1, max=1000, col=1)\nvar2 &lt;- randomNumbers(n=100, min=1, max=1000, col=1) \nvar3 &lt;- randomNumbers(n=100, min=1, max=1000, col=1) \ndata &lt;- data.frame(var1[,1], var2[,1], var3[,1]) \nwilcox.test(data[,1], mu=0, alternatives=\"two.sided\")\n\n\n16.7.2 Wilcoxon-Mann-Whitney Test\nThe Wilcoxon-Mann-Whitney test is a nonparametric test to compare two samples. It is a powerful substitute to the two-sample t-test.\nTo use the Wilcoxon-Matt-Whitney test (or the Wilcoxon rank sum test or the Mann-Whitney test) in R, you can use the wilcox.test() function:\nwilcox.test(data[,1], data[,2], correct=FALSE)\nThere are not significant differences in the median for first variable median and second variable median.\n\n\n16.7.3 Kruskal-Wallis Test\nThe Kruskal-Wallis test is a nonparametric test that is an extension of the Mann-Whitney U test for three or more samples.\nThe test requires samples to be identically distributed.\nKruskal-Wallis is an alternative to one-way ANOVA.\nThe Kruskal-Wallis test tests the differences between scores of k independent samples of unequal sizes with the ith sample containing li rows:\n\n\\(H_0\\): \\(μ_o\\) = \\(μ_1\\)= \\(μ_2\\) = … = \\(μ_k\\); The null hypothesis is that all the medians are the same.\n\\(H_a\\): \\(μ_1\\) ≠ \\(μ_k\\); The alternate hypothesis is that at least one median is different.\n\n\nkruskal.test(airquality$Ozone ~ airquality$Month)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  airquality$Ozone by airquality$Month\nKruskal-Wallis chi-squared = 29.267, df = 4, p-value = 6.901e-06"
  },
  {
    "objectID": "stats_regression.html#linear-regressions",
    "href": "stats_regression.html#linear-regressions",
    "title": "17  Regression Analysis",
    "section": "17.1 Linear Regressions",
    "text": "17.1 Linear Regressions\nThe linear regression equation is \\(y = b_0 + b_1 x\\), where y is the dependent variable, x is the independent variable, \\(b_0\\) is the intercept, and \\(b_1\\) is the slope.\nTo use linear regression in R, you use the lm() function:\n\nset.seed(123)\nx &lt;- rnorm(100, mean=1, sd=1)\ny &lt;- rnorm(100, mean=2, sd=2)\ndata &lt;- data.frame(x, y);\nmod &lt;- lm(data$y ~ data$x, data=data)\nmod\n\n\nCall:\nlm(formula = data$y ~ data$x, data = data)\n\nCoefficients:\n(Intercept)       data$x  \n     1.8993      -0.1049  \n\nsummary(mod)\n\n\nCall:\nlm(formula = data$y ~ data$x, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.815 -1.367 -0.175  1.161  6.581 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.8993     0.3033   6.261 1.01e-08 ***\ndata$x       -0.1049     0.2138  -0.491    0.625    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.941 on 98 degrees of freedom\nMultiple R-squared:  0.002453,  Adjusted R-squared:  -0.007726 \nF-statistic: 0.241 on 1 and 98 DF,  p-value: 0.6246\n\n\nWhen the p-value is less than 0.05, the model is significant:\n\n\\(H_0\\): Coefficient associated with the variable is equal to zero\n\\(H_a\\): Coefficient is not equal to zero (there is a relationship)\n\nFurthermore:\n\nThe higher the R-squared and the adjusted R-squared, the better the linear model.\nThe lower the standard error, the better the model"
  },
  {
    "objectID": "stats_regression.html#multiple-linear-regressions",
    "href": "stats_regression.html#multiple-linear-regressions",
    "title": "17  Regression Analysis",
    "section": "17.2 Multiple Linear Regressions",
    "text": "17.2 Multiple Linear Regressions\nMultiple linear regression is used when you have more than one independent variable.\nThe equation of a multiple linear regression is:\n\\(y = b_0 + b_1 x_1 + b_2 x_2 + ... + b_ k x_k + ϵ\\)\nWhen you have n observations or rows in the data set, you have the following model:\n\n\n\nFigure 4: Model for multiple regressions\n\n\nUsing a matrix, you can represent the equations as: \\(y = Xb + ϵ\\)\n\n\n\nFigure 5: Representation of equations as a matrix\n\n\nTo calculate the coefficients: ^b = (X’ X)-1 X’ y\n\nset.seed(123)\nx &lt;- rnorm(100, mean=1, sd=1)\nx2 &lt;- rnorm(100, mean=2, sd=5)\ny &lt;- rnorm(100, mean=2, sd=2)\ndata &lt;- data.frame(x, x2, y)\nmod &lt;- lm(data$y ~ data$x + data$x2, data=data)\nmod\n\n\nCall:\nlm(formula = data$y ~ data$x + data$x2, data = data)\n\nCoefficients:\n(Intercept)       data$x      data$x2  \n   2.517425    -0.266343     0.009525  \n\nsummary(mod)\n\n\nCall:\nlm(formula = data$y ~ data$x + data$x2, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7460 -1.3215 -0.2489  1.2427  4.1597 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.517425   0.305233   8.248 7.97e-13 ***\ndata$x      -0.266343   0.209739  -1.270    0.207    \ndata$x2      0.009525   0.039598   0.241    0.810    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.903 on 97 degrees of freedom\nMultiple R-squared:  0.01727,   Adjusted R-squared:  -0.00299 \nF-statistic: 0.8524 on 2 and 97 DF,  p-value: 0.4295"
  },
  {
    "objectID": "program_habits.html#useful-guidelines",
    "href": "program_habits.html#useful-guidelines",
    "title": "18  Programming Habits",
    "section": "18.1 Useful Guidelines",
    "text": "18.1 Useful Guidelines\nWe find the following to be useful guidelines:\n\nStart each program with some comments giving the name of the program, the author, the date it was written, and what the program does. A description of what a program does should explain what all the inputs and outputs are.\nVariable names should be descriptive, that is, they should give a clue as to what the value of the variable represents. Avoid using reserved names or function names as variable names (in particular t, c, and q are all function names in R). You can find out whether or not your preferred name for an object is already in use by the exists() function.\nUse blank lines to separate sections of code into related parts, and use indenting to distinguish the inside part of an if statement or a for or while loop.\nDocument the programs that you use in detail, ideally with citations for specific algorithms. There is no worse feeling than returning to undocumented code that had been written several years earlier to try to find and then explain an anomaly."
  },
  {
    "objectID": "program_logic.html#logical-expressions",
    "href": "program_logic.html#logical-expressions",
    "title": "19  Logic",
    "section": "19.1 Logical expressions",
    "text": "19.1 Logical expressions\nA logical expression is formed using:\n\nthe comparison operators &lt; , &gt; , &lt;=, &gt;=, == (equal to), != (not equal to), &&, || (sequentially evaluated versions of & and |, respectively).\nthe logical operators & (and), | (or), ! (not), xor() is exclusive or (i.e xor(x, y) is true if x is true, or y is true, but not both).\n\nAs well as & and |, R also has && and ||. Don’t use them in dplyr functions! These are called short-circuiting operators and only ever return a single TRUE or FALSE. They’re important for programming, not data science.\nFigure 10 shows the complete set of Boolean operations and how they work:  Figure 10. The complete set of Boolean operations. x is the left-hand circle, y is the right-hand circle, and the shaded regions show which parts each operator selects\nExample:\n# Find all rows where x is not missing\ndf |&gt; filter(!is.na(x))\n\n# Find all rows where x is smaller than -10 or bigger than 0\ndf |&gt; filter(x &lt; -10 | x &gt; 0) \nThe order of operations can be controlled using parentheses ( ).\nThe value of a logical expression is either TRUE or FALSE (the integers 1 and 0 can also be used to represent TRUE and FALSE, respectively).\nExample:\n\n## Note that A|B is TRUE if A or B or both are TRUE\nc(0,0,1,1)|c(0,1,0,1)\n\n[1] FALSE  TRUE  TRUE  TRUE\n\n## If you want exclusive disjunction, that is either A or B is TRUE but not both, then use xor(A,B)\nxor(c(0,0,1,1), c(0,1,0,1))\n\n[1] FALSE  TRUE  TRUE FALSE\n\n\n\n19.1.1 Order of operations\nNote that the order of operations doesn’t work like English. Take the following code that finds all flights that departed in November or December:\nflights |&gt; \n   filter(month == 11 | month == 12)\nYou might be tempted to write it like you’d say in English: “Find all flights that departed in November or December.”:\nflights |&gt; \n   filter(month == 11 | 12)\nThis code doesn’t error but it also doesn’t seem to have worked. What’s going on? Here, R first evaluates month == 11 creating a logical vector, which we call nov. It computes nov | 12. When you use a number with a logical operator it converts everything apart from 0 to TRUE, so this is equivalent to nov | TRUE which will always be TRUE, so every row will be selected:\nflights |&gt; \n  mutate(\n    nov = month == 11,\n    final = nov | 12,\n    .keep = \"used\"\n  )\n\n\n19.1.2 %in%\nAn easy way to avoid the problem of getting your ==s and |s in the right order is to use %in%. x %in% y returns a logical vector the same length as x that is TRUE whenever a value in x is anywhere in y.\nExample:\n\n1:12 %in% c(1, 5, 11)\n\n [1]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n\nletters[1:10] %in% c(\"a\", \"e\", \"i\", \"o\", \"u\")\n\n [1]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE\n\n\nSo to find all flights in November and December we could write:\n\nlibrary(nycflights13)\nlibrary(dplyr)\ndata(flights)\nflights |&gt; \n  filter(month %in% c(11, 12))\n\n# A tibble: 55,403 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013    11     1        5           2359         6      352            345\n 2  2013    11     1       35           2250       105      123           2356\n 3  2013    11     1      455            500        -5      641            651\n 4  2013    11     1      539            545        -6      856            827\n 5  2013    11     1      542            545        -3      831            855\n 6  2013    11     1      549            600       -11      912            923\n 7  2013    11     1      550            600       -10      705            659\n 8  2013    11     1      554            600        -6      659            701\n 9  2013    11     1      554            600        -6      826            827\n10  2013    11     1      554            600        -6      749            751\n# ℹ 55,393 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n19.1.3 Sequential && and ||\nTo evaluate x & y, R first evaluates x and y, then returns TRUE if x and y are both TRUE, FALSE otherwise.\nTo evaluate x && y, R first evaluates x. If x is FALSE then R returns FALSE without evaluating y. If x is TRUE then R evaluates y and returns TRUE if y is TRUE, FALSE otherwise.\nSequential evaluation of x and y is useful when y is not always well defined, or when y takes a long time to compute.\nNote that && and || only work on scalars, whereas & and | work on vectors on an element-by-element basis.\n\n\n19.1.4 Index position\nIf you wish to know the index positions of TRUE elements of a logical vector x, then use which(x):\nExample:\n\nx &lt;- c(1, 1, 2, 3, 5, 8, 13)\nwhich(x %% 2 == 0)\n\n[1] 3 6"
  },
  {
    "objectID": "program_logic.html#if-else",
    "href": "program_logic.html#if-else",
    "title": "19  Logic",
    "section": "19.2 if-else",
    "text": "19.2 if-else\nA natural extension of the if command includes an else part. Braces { } are used to group together one or more expressions. If there is only one expression then the braces are optional.\nWhen an if expression is evaluated, if logical_expression is TRUE then the first group of expressions is executed and the second group of expressions is not executed. Conversely, if logical_expression is FALSE then only the second group of expressions is executed.:\nif (logical_expression) {\n  expression_1 # do something\n  ...\n} else {\n  expression_2 # do something different\n  ...\n}\nExample:\n\nlibrary(tidyverse)\nx &lt;- c(-3:3, NA)\nif_else(x &gt; 0, \"+ve\", \"-ve\")\n\n[1] \"-ve\" \"-ve\" \"-ve\" \"-ve\" \"+ve\" \"+ve\" \"+ve\" NA   \n\n\n\n19.2.1 elseif\nWhen the else expression contains an if, then it can be written equivalently (and more clearly) as follows:\nif (logical_expression_1) {\n  expression_1\n  ...\n} else if (logical_expression_2) {\n  expression_2\n  ...\n} else {\n  expression_3\n  ...\n}\n\n\n19.2.2 case_when()\ndplyr’s case_when() is inspired by SQL’s CASE statement and provides a flexible way of performing different computations for different conditions. It takes pairs that look like condition ~ output. condition must be a logical vector; when it’s TRUE, output will be used.\nThis means we could recreate our previous nested if_else() as follows:\n\nx &lt;- c(-3:3, NA)\ncase_when(\n  x == 0   ~ \"0\",\n  x &lt; 0    ~ \"-ve\", \n  x &gt; 0    ~ \"+ve\",\n  is.na(x) ~ \"???\"\n)\n\n[1] \"-ve\" \"-ve\" \"-ve\" \"0\"   \"+ve\" \"+ve\" \"+ve\" \"???\""
  },
  {
    "objectID": "program_logic.html#comparisions",
    "href": "program_logic.html#comparisions",
    "title": "19  Logic",
    "section": "19.3 Comparisions",
    "text": "19.3 Comparisions\nA very common way to create a logical vector is via a numeric comparison with &lt;, &lt;=, &gt;, &gt;=, !=, and ==. For example, the following filter finds all daytime departures that arrive roughly on time:\nExample:\n\nlibrary(nycflights13)\nflights |&gt; \n  filter(dep_time &gt; 600 & dep_time &lt; 2000 & abs(arr_delay) &lt; 20)\n\n# A tibble: 172,286 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      601            600         1      844            850\n 2  2013     1     1      602            610        -8      812            820\n 3  2013     1     1      602            605        -3      821            805\n 4  2013     1     1      606            610        -4      858            910\n 5  2013     1     1      606            610        -4      837            845\n 6  2013     1     1      607            607         0      858            915\n 7  2013     1     1      611            600        11      945            931\n 8  2013     1     1      613            610         3      925            921\n 9  2013     1     1      615            615         0      833            842\n10  2013     1     1      622            630        -8     1017           1014\n# ℹ 172,276 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nYou can also use mutate() to explicitly create the underlying logical variables and add more intermediate steps to make it easier to both read your code and check that each step has been computed correctly.\nExample:\n\nflights |&gt; \n  mutate(\n    daytime = dep_time &gt; 600 & dep_time &lt; 2000,\n    approx_ontime = abs(arr_delay) &lt; 20,\n  ) |&gt; \n  filter(daytime & approx_ontime)\n\n# A tibble: 172,286 × 21\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      601            600         1      844            850\n 2  2013     1     1      602            610        -8      812            820\n 3  2013     1     1      602            605        -3      821            805\n 4  2013     1     1      606            610        -4      858            910\n 5  2013     1     1      606            610        -4      837            845\n 6  2013     1     1      607            607         0      858            915\n 7  2013     1     1      611            600        11      945            931\n 8  2013     1     1      613            610         3      925            921\n 9  2013     1     1      615            615         0      833            842\n10  2013     1     1      622            630        -8     1017           1014\n# ℹ 172,276 more rows\n# ℹ 13 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, daytime &lt;lgl&gt;,\n#   approx_ontime &lt;lgl&gt;"
  },
  {
    "objectID": "program_logic.html#logical-summaries",
    "href": "program_logic.html#logical-summaries",
    "title": "19  Logic",
    "section": "19.4 Logical summaries",
    "text": "19.4 Logical summaries\nThe following functions describe some useful techniques for summarizing logical vectors. There are two main logical summaries: any() and all():\n\nany(x) is the equivalent of |; it’ll return TRUE if there are any TRUE’s in x.\nall(x) is equivalent of &; it’ll return TRUE only if all values of x are TRUE’s. Like all summary functions, they’ll return NA if there are any missing values present, and as usual you can make the missing values go away with na.rm = TRUE.\n\nExample:\nflights |&gt; \n  group_by(year, month, day) |&gt; \n  summarize(\n    all_delayed = all(dep_delay &lt;= 60, na.rm = TRUE),\n    any_long_delay = any(arr_delay &gt;= 300, na.rm = TRUE),\n    .groups = \"drop\"\n  )"
  },
  {
    "objectID": "program_functions.html#building-functions",
    "href": "program_functions.html#building-functions",
    "title": "20  Functions",
    "section": "20.1 Building Functions",
    "text": "20.1 Building Functions\nFunctions are often used to encapsulate a sequence of expressions that need to be executed numerous times, perhaps under slightly different conditions. Functions are also often written when code must be shared with others or the public.\nThe writing of a function allows a developer to create an interface to the code, that is explicitly specified with a set of parameters.\nA function has the general form:\nname &lt;- function(argument_1, argument_2, ...) {\n  expression_1\n  expression_2\n  &lt;some other expressions&gt;\n  return(output)\n}\nHere argument_1, argument_2, etc., are the names of variables and expression_1, expression_2, and output are all regular R expressions.\nname is the name of the function. Because all function arguments have names, they can be specified using their name. Specifying an argument by its name is sometimes useful if a function has many arguments and it may not always be clear which argument is being specified.\nExample:\nf(num = 2)\nSome functions may have no arguments, and that the braces are only necessary if the function comprises more than one expression.\nExample:\n\nf &lt;- function(num = 1) {   \n## if the function is called without the num argument being explicitly specified, then it will print “Hello, world!” to the console once.\n  hello &lt;- \"Hello, world!\\n\" \n  for(i in seq_len(num)) {\n    cat(hello)\n  }\n  chars &lt;- nchar(hello) * num \n  chars\n}\n\nIn R, the return value of a function is always the very last expression that is evaluated (in the example is chars).\nThe formals() function returns a list of all the formal arguments of a function.\nNote that functions have their own class.\n\n20.1.1 Vector functions\nVector functions take one or more vectors and return a vector result. For example, take a look at this code. What does it do?\n\ndf &lt;- tibble(\n  a = rnorm(5),\n  b = rnorm(5),\n  c = rnorm(5),\n  d = rnorm(5),\n)\n\ndf |&gt; mutate(\n  a = (a - min(a, na.rm = TRUE)) / \n    (max(a, na.rm = TRUE) - min(a, na.rm = TRUE)),\n  b = (b - min(b, na.rm = TRUE)) / \n    (max(b, na.rm = TRUE) - min(b, na.rm = TRUE)),\n  c = (c - min(c, na.rm = TRUE)) / \n    (max(c, na.rm = TRUE) - min(c, na.rm = TRUE)),\n  d = (d - min(d, na.rm = TRUE)) / \n    (max(d, na.rm = TRUE) - min(d, na.rm = TRUE)),\n)\n\n# A tibble: 5 × 4\n      a     b     c     d\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.763 0.613 1     0.762\n2 1     1     0.396 0    \n3 0.461 0.467 0     0.347\n4 0.431 0.226 0.632 1    \n5 0     0     0.601 0.493\n\n\nTo write a function you need to first analyse your repeated code to figure what parts are constant and what parts vary. If we take the code above and pull it outside of mutate(), it’s a little easier to see the pattern because each repetition is now one line:\n(█ - min(█, na.rm = TRUE)) / (max(█, na.rm = TRUE) - min(█, na.rm = TRUE))\nThen you create a function by following the template:\nname &lt;- function(arguments) {\n  body\n}\nExample:\n\nrescale01 &lt;- function(x) {\n  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n}\n\nYou might test with a few simple inputs to make sure you’ve captured the logic correctly:\n\nrescale01(c(-10, 0, 10))\n\n[1] 0.0 0.5 1.0\n\n\nThen you can rewrite the call to mutate() as:\n\ndf |&gt; mutate(\n  a = rescale01(a),\n  b = rescale01(b),\n  c = rescale01(c),\n  d = rescale01(d),\n)\n\n# A tibble: 5 × 4\n      a     b     c     d\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.763 0.613 1     0.762\n2 1     1     0.396 0    \n3 0.461 0.467 0     0.347\n4 0.431 0.226 0.632 1    \n5 0     0     0.601 0.493\n\n\n\n\n20.1.2 Data frames functions\nData frame functions work like dplyr verbs: they take a data frame as the first argument, some extra arguments that say what to do with it, and return a data frame or a vector.\nWhen you start writing functions that use dplyr verbs you rapidly hit the problem of indirection. Let’s illustrate the problem with a very simple function: grouped_mean(). The goal of this function is to compute the mean of mean_var grouped by group_var:\nExample:\ngrouped_mean &lt;- function(df, group_var, mean_var) {\n  df |&gt; \n    group_by(group_var) |&gt; \n    summarize(mean(mean_var))\n}\nIf we try and use it, we get an error:\ndiamonds |&gt; grouped_mean(cut, carat)\n\n#&gt; Error in `group_by()`:\n#&gt; ! Must group by variables found in `.data`.\n#&gt; ✖ Column `group_var` is not found.\nHere we need some way to tell group_by() and summarize() not to treat group_var and mean_var as the name of the variables, but instead look inside them for the variable we actually want to use.\nSo to make grouped_mean() work, we need to surround group_var and mean_var with { }. Embracing a variable tells dplyr to use the value stored inside the argument, not the argument as the literal variable name. One way to remember what’s happening is to think of { } as looking down a tunnel — { var } will make a dplyr function look inside of var rather than looking for a variable called var.\nExample:\n\ngrouped_mean &lt;- function(df, group_var, mean_var) {\n  df |&gt; \n    group_by({{ group_var }}) |&gt; \n    summarize(mean({{ mean_var }}))\n}\n\n\ndiamonds |&gt; grouped_mean(cut, carat)\n\n# A tibble: 5 × 2\n  cut       `mean(carat)`\n  &lt;ord&gt;             &lt;dbl&gt;\n1 Fair              1.05 \n2 Good              0.849\n3 Very Good         0.806\n4 Premium           0.892\n5 Ideal             0.703\n\n\nSo the key challenge in writing data frame functions is figuring out which arguments need to be embraced. Fortunately, this is easy because you can look it up from the documentation.\n\n\n20.1.3 Plot functions\nYou might want to return a plot. For example, imagine that you’re making a lot of histograms:\ndiamonds |&gt; \n  ggplot(aes(x = carat)) +\n  geom_histogram(binwidth = 0.1)\n\ndiamonds |&gt; \n  ggplot(aes(x = carat)) +\n  geom_histogram(binwidth = 0.05)\nWouldn’t it be nice if you could wrap this up into a histogram function? This is easy as pie once you know that aes() is a data-masking function and you need to embrace:\n\nhistogram &lt;- function(df, var, binwidth = NULL) {\n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(binwidth = binwidth)\n}\n\ndiamonds |&gt; histogram(carat, 0.1)\n\n\n\n\nNote that histogram() returns a ggplot2 plot, meaning you can still add on additional components if you want. Just remember to switch from |&gt; to +:\n\ndiamonds |&gt; \n  histogram(carat, 0.1) +\n  labs(x = \"Size (in carats)\", y = \"Number of diamonds\")"
  },
  {
    "objectID": "program_functions.html#call-or-run-a-function",
    "href": "program_functions.html#call-or-run-a-function",
    "title": "20  Functions",
    "section": "20.2 Call or Run a Function",
    "text": "20.2 Call or Run a Function\nTo call or run the function we type (for example) name(x1, x2). The value of this expression is the value of the expression output. To calculate the value of output the function first copies the value of x1 to argument_1, x2 to argument_2, and so on. The arguments then act as variables within the function. We say that the arguments have been passed to the function. Next the function evaluates the grouped expressions contained in the braces { }; the value of the expression output is returned as the value of the function.\nTo use the function we first load it (using source or by copying and pasting into R), then call it, supplying suitable arguments.\nExample:\nrm(list=ls())\nsource(\"../scripts/quad3.r\")\nquad3(1,0,-1)\nNote that the name of the function does not have to match the name of the program file, but when a program consists of a single function this is conventional."
  },
  {
    "objectID": "program_functions.html#a-function-return",
    "href": "program_functions.html#a-function-return",
    "title": "20  Functions",
    "section": "20.3 A Function Return",
    "text": "20.3 A Function Return\nA function always returns a value. For some functions the value returned is unimportant, for example if the function has written its output to a file then there may be no need to return a value as well. In such cases one usually omits the return statement, or returns NULL.\nA function may have more than one return statement, in which case it stops after executing the first one it reaches. If there is no return(output) statement then the value returned by the function is the value of the last expression in the braces.\nIf, when called, the value returned by a function (or any expression) is not assigned to a variable, then it is printed. The expression invisible(x) will return the same value as x, but its value is not printed. For example, some versions of the summary() function use invisible on their returned object."
  },
  {
    "objectID": "program_functions.html#arguments",
    "href": "program_functions.html#arguments",
    "title": "20  Functions",
    "section": "20.4 Arguments",
    "text": "20.4 Arguments\nWe think about arguments both when the functions are written and when they are called. The arguments of an existing function can be obtained by calling the formals function.\nIn order to simplify calling functions, some arguments may be assigned default values, which are used in case the argument is not provided in the call to the function:\nExample:\n\ntest3 &lt;- function(x = 1) {\n  return(x)\n  }\ntest3(2)\n\n[1] 2\n\ntest3()\n\n[1] 1\n\n\nSometimes you will want to define arguments so that they can take only a small number of different values, and the function will stop informatively if an inappropriate value is passed.\nExample:\n\nfunk &lt;- function(vibe = c(\"Do\",\"Be\",\"Dooby\",\"Dooo\")) {\n  vibe &lt;- match.arg(vibe)\n  return(vibe)\n  }\nfunk()\n\n[1] \"Do\"\n\n\nfunk(Peter)\nError in match.arg(vibe) (from #2) :\n'arg' should be one of \"Do\", \"Be\", \"Dooby\", \"Dooo\"\n\n20.4.1 Argument Matching\nR functions arguments can be matched (called) positionally or by name:\n\nPositional matching just means that R assigns the first value to the first argument, the second value to second argument, etc.:\nExample:\n# Positional match first by argument, default for 'na.rm'\nsd(mydata)\n# Specify 'x' argument by name, default for 'na.rm'\nsd(x = mydata)\n# Specify both arguments by name\nsd(x = mydata, na.rm = FALSE)\nWhen specifying the function arguments by name, it doesn’t matter in what order you specify them. Named arguments are useful on the command line when you have a long argument list and you want to use the defaults for everything except for an argument near the end of the list:\n## Specify both arguments by name\nsd(na.rm = FALSE, x = mydata)  \nR also provides partial matching of arguments, where doing so is not ambiguous. This means that argument names in the call to the function do not have to be complete. Reliance on partial matching makes code more fragile.\n\ntest6 &lt;- function(a = 1, b.c.d = 1) {\n  return(a + b.c.d)\n  }\ntest6()\n\n[1] 2\n\ntest6(b = 5)\n\n[1] 6\n\n\n\n\n\n20.4.2 The … Argument\n...indicates a variable number of arguments that are usually passed on to other functions, it is often used when extending another function and you don’t want to copy the entire argument list of the original function.\nThe ... argument is necessary when the number of arguments passed to the function cannot be known in advance, e.g. paste(), cat().\nAny arguments that appear after ... on the argument list must be named explicitly and cannot be partially matched or matched positionally.\nR provides a very useful means of passing arguments, unaltered, from the function that is being called to the functions that are called within it. These arguments do not need to be named explicitly in the outer function, hence providing great flexibility. To use this facility you need to include ... in your argument list. These three dots (an ellipsis) act as a placeholder for any extra arguments given to the function.\nExample:\n\ntest4 &lt;- function(x, ...) {\n  return(sd(x, ...))\n  }\ntest4(1:3)\n\n[1] 1\n\n# Arguments that do not match those in test4 are provided, in order, to any function within test4 that has the dots in the list of arguments to the function call.\ntest4(c(1:2,NA), na.rm = TRUE)\n\n[1] 0.7071068\n\n\nUsing the dots in this way means that the user has access to all the function arguments without our needing to list them when we define the function.\nIn general, naming the arguments in the function call is good practice, because it increases the readability and eliminates one potential source of errors."
  },
  {
    "objectID": "program_functions.html#scoping-rules",
    "href": "program_functions.html#scoping-rules",
    "title": "20  Functions",
    "section": "20.5 Scoping Rules",
    "text": "20.5 Scoping Rules\nArguments and variables that are defined within a function exist only within that function. That is, if you define and use a variable x inside a function, it does not exist outside the function. If variables with the same name exist inside and outside a function, then they are separate and do not interact at all. You can think of a function as a separate environment that communicates with the outside world only through the values of its arguments and its output expression. For example if you execute the command rm(list=ls()) inside a function (which is only rarely a good idea), you only delete those objects that are defined inside the function.\nExample:\n\ntest &lt;- function(x) {\n  y &lt;- x + 1\n  return(y)\n  }\ntest(1)\n\n[1] 2\n\n\nx\nError: Object \"x\" not found\nThat part of a program in which a variable is defined is called its scope. Restricting the scope of variables within a function provides an assurance that calling the function will not modify variables outside the function, except by assigning the returned value.\nBeware, however, the scope of a variable is not symmetric. That is, variables defined inside a function cannot be seen outside, but variables defined outside the function can be seen inside the function, provided there is not a variable with the same name defined inside.This arrangement makes it possible to write a function whose behavior depends on the context within which it is run.\nExample:\n\ntest2 &lt;- function(x) {\n  y &lt;- x + z\n  return(y)\n  }\nz &lt;- 1\ntest2(1)\n\n[1] 2\n\n\nThe moral of this example is that it is generally advisable to ensure that the variables you use in a function either are declared as arguments, or have been defined in the function."
  },
  {
    "objectID": "program_functions.html#style",
    "href": "program_functions.html#style",
    "title": "20  Functions",
    "section": "20.6 Style",
    "text": "20.6 Style\nIdeally, the name of your function will be short, but clearly evoke what the function does. Generally, function names should be verbs, and arguments should be nouns. There are some exceptions: nouns are ok if the function computes a very well known noun (i.e. mean() is better than compute_mean()), or accessing some property of an object (i.e. coef() is better than get_coefficients()). Use your best judgement and don’t be afraid to rename a function if you figure out a better name later.\nExample:\n# Too short\nf()\n\n# Not a verb, or descriptive\nmy_awesome_function()\n\n# Long, but clear\nimpute_missing()\nAdditionally, function() should always be followed by squiggly brackets ({}), and the contents should be indented by an additional two spaces. This makes it easier to see the hierarchy in your code by skimming the left-hand margin.\n# Missing extra two spaces\ndensity &lt;- function(color, facets, binwidth = 0.1) {\ndiamonds |&gt; \n  ggplot(aes(x = carat, y = after_stat(density), color = {{ color }})) +\n  geom_freqpoly(binwidth = binwidth) +\n  facet_wrap(vars({{ facets }}))\n}\ncollapse_years()\n\n# Pipe indented incorrectly\ndensity &lt;- function(color, facets, binwidth = 0.1) {\n  diamonds |&gt; \n  ggplot(aes(x = carat, y = after_stat(density), color = {{ color }})) +\n  geom_freqpoly(binwidth = binwidth) +\n  facet_wrap(vars({{ facets }}))\n}"
  },
  {
    "objectID": "program_iteration.html#loops",
    "href": "program_iteration.html#loops",
    "title": "21  Iteration",
    "section": "21.1 Loops",
    "text": "21.1 Loops\nR is set up so that such programming tasks can be accomplished using vector operations rather than looping. Using vector operations is more efficient computationally, as well as more concise literally.\nExample:\n\n# Find the sum of the first n squares using a loop\nn &lt;- 100\nS &lt;- 0\nfor (i in 1:n) {\n  S &lt;- S + i^2\n}\nS\n\n[1] 338350\n\n# Alternatively, use vector operations\nsum((1:n)^2)\n\n[1] 338350\n\n\n\n21.1.1 for Loops\nThe for command has the following form, where x is a simple variable and vector is a vector.\nfor (x in vector) {\n  expression_1\n  ...\n}\nExample:\nx &lt;- c(\"a\", \"b\", \"c\", \"d\")\nfor(i in 1:4) / for(i in seq_along(x)) / for(letter in x) { ## set an iterator variable and assign it successive values over the elements of an object (list, vector, etc.) \n  print(x[i])\n  print(letter)\n}\nWhen executed, the for command executes the group of expressions within the braces { } once for each element of vector. The grouped expressions can use x, which takes on each of the values of the elements of vector as the loop is repeated.\n\n\n21.1.2 nested for loops\nExample:\nx &lt;- matrix(1:6, 2, 3) \nfor(i in seq_len(nrow(x))) { \n  for(j in seq_len(ncol(x))) { \n    print(x[i,j]) \n  }\n}\n\n\n21.1.3 while Loops\nOften we do not know beforehand how many times we need to go around a loop. That is, each time we go around the loop, we check some condition to see if we are done yet. In this situation we use a while loop, which has the form:\nwhile (logical_expression) {\n  expression_1\n  ...\n}\nWhen a while command is executed, logical_expression is evaluated first. If it is TRUE then the group of expressions in braces { } is executed. Control is then passed back to the start of the command: if logical_expression is still TRUE then the grouped expressions are executed again, and so on. Clearly, for the loop to stop eventually, logical_expression must eventually be FALSE. To achieve this logical_expression usually depends on a variable that is altered within the grouped expressions.\nExample:\ntime &lt;- 0\ndebt &lt;- debt_initial\nwhile (debt &gt; 0) {\n  time &lt;- time + period\n  debt &lt;- debt*(1 + r*period) - repayments\n}\n\n\n21.1.4 repeat Loops\nrepeat initiates an infinite loop right from the start. The only way to exit a repeat loop is to call break:\nrepeat { \n  if() { \n    break\n  }\n}\n\n\n21.1.5 next, break\nnext is used to skip an iteration of a loop:\nfor(i in 1:100) { \n  if(i &lt;= 20) { \n    next ## Skip the first 20 iterations \n  } \n  ## Do something here\n}\nbreak is used to exit a loop immediately, regardless of what iteration the loop may be on.\nfor(i in 1:100) { \n  print(i) \n  if(i &gt; 20) { \n    break ## Stop loop after 20 iterations \n  }\n}"
  },
  {
    "objectID": "program_iteration.html#loop-functions",
    "href": "program_iteration.html#loop-functions",
    "title": "21  Iteration",
    "section": "21.2 Loop Functions",
    "text": "21.2 Loop Functions\nMany R functions are vectorised, meaning that given vector input the function acts on each element separately, and a vector output is returned. This is a very powerful aspect of R that allows for compact, efficient, and readable code. Moreover, for many R functions, applying the function to a vector is much faster than if we were to write a loop to apply it to each element one at a time.\nBesides, writing for and while loops is useful when programming but not particularly easy when working interactively on the command line.\nR has some functions which implement looping in a compact form to make your life easier:\nlapply() ## Apply the function FUN to every element of vector X\nsapply() ## Same as lapply but try to simplify the result \napply() ## Apply a function that takes a vector argument to each of the rows (or columns) of a matrix, \ntapply() ## Apply a function over subsets of a vector\nmapply() ## Multivariate version of lapply (to vectorise over more than one argument)\nX can be a list or an atomic vector, which is a vector that comprises atomic objects (logical, integer, numeric, complex, character and raw). That is, sapply(X, FUN) returns a vector whose i -th element is the value of the expression FUN(X[i]). Note that R performs a loop over the elements of X, so execution of this code is not faster than execution of an equivalent loop.\nIf FUN has arguments other than X[i], then they can be included using the dots protocol as shown above. That is, sapply(X, FUN, ...) returns FUN(X[i], ...) as the i -th element.\n\n21.2.1 lapply\nThe lapply() function does the following simple series of operations:\n\nIt loops over a list, iterating over each element in that list\nIt applies a function to each element of the list (a function that you specify)\nReturns a list (the “l” is for “list”).\n\nlapply() always returns a list, regardless of the class of the input.\n\nstr(lapply)\n\nfunction (X, FUN, ...)  \n\n\nNote that the function() definition is right in the call to lapply():\nlapply(x, function(elt) { elt[,1] }\nExample:\n\nx &lt;- list(a = 1:5, b = rnorm(10)) \nlapply(x, mean)\n\n$a\n[1] 3\n\n$b\n[1] 0.1657981\n\n\nNote that when you pass a function to another function, you do not need to include the open and closed parentheses () like you do when you are calling a function.\nOnce the call to lapply() is finished, the function disappears and does not appear in the workspace.\n\n\n21.2.2 sapply\nThe sapply() function behaves similarly to lapply(), the only real difference being that the return value is a vector or a matrix.\n\nstr(sapply)\n\nfunction (X, FUN, ..., simplify = TRUE, USE.NAMES = TRUE)  \n\n\nsapply() will try to simplify the result of lapply() if possible.\nExample:\n\ns &lt;- split(airquality, airquality$Month)\nmode(s)\n\n[1] \"list\"\n\n(q &lt;- sapply(s, function(x) { \n              colMeans(x[, c(\"Ozone\", \"Solar.R\", \"Wind\")])\n          }\n))\n\n               5         6          7        8        9\nOzone         NA        NA         NA       NA       NA\nSolar.R       NA 190.16667 216.483871       NA 167.4333\nWind    11.62258  10.26667   8.941935 8.793548  10.1800\n\nmode(q)\n\n[1] \"numeric\"\n\n\n\n\n21.2.3 apply\nThe apply() function can take a list, matrix, or array. It is most often used to apply a function to the rows or columns of a matrix (which is just a 2-dimensional array).\n\nstr(apply)\n\nfunction (X, MARGIN, FUN, ..., simplify = TRUE)  \n\n\nUsing apply() is not really faster than writing a loop, but it works in one line and is highly compact.\n\nx &lt;- matrix(rnorm(200), 20, 10)\n# Take the mean of each column\n(k &lt;- apply(x, 2, mean))\n\n [1]  0.14727811  0.43325584 -0.26956194 -0.07370726 -0.07655632 -0.08571866\n [7]  0.29305626  0.18876939  0.51761735 -0.02386109\n\nmode(k)\n\n[1] \"numeric\"\n\n## Take the mean of each row\napply(x, 1, sum) \n\n [1] -1.5097227  2.9241030 -3.6808014  6.9609231 -2.5324817 -3.5862790\n [7]  4.1464080  2.6878499  0.2111298  0.7161461  0.7516207  8.1343997\n[13] -3.3015266  3.1270903 -1.9789594  1.2783737  4.0279224 -1.2387867\n[19]  2.8347659  1.0392588\n\n\nFor the special case of column/row sums and column/row means of matrices, there are some useful shortcuts:\nrowSums = apply(x, 1, sum) \nrowMeans = apply(x, 1, mean)\ncolSums = apply(x, 2, sum) \ncolMeans = apply(x, 2, mean)\n\n\n21.2.4 tapply\ntapply() is used to apply a function over subsets of a vector. It can be thought of as a combination of split() and sapply() for vectors only.\n\nstr(tapply)\n\nfunction (X, INDEX, FUN = NULL, ..., default = NA, simplify = TRUE)  \n\n\nExample:\n\n(x &lt;- c(rnorm(10), runif(10), rnorm(10, 1)))\n\n [1] -1.28447476 -0.87516348  0.38151289 -1.05675253 -0.49208629 -0.62921217\n [7] -0.79653296  1.02096588  0.70847441 -0.37224811  0.64748401  0.37824349\n[13]  0.83174195  0.33945354  0.90395266  0.20921515  0.92049471  0.82909468\n[19]  0.95179298  0.38200533  1.70245776  1.83811324  1.49084291 -0.57756468\n[25]  0.81538190 -0.48060434  1.04121825  0.07955710  1.93430660 -0.08805508\n\n# Define some groups with a factor variable\n(f &lt;- gl(3, 10))\n\n [1] 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3\nLevels: 1 2 3\n\ntapply(x, f, mean)\n\n         1          2          3 \n-0.3395517  0.6393478  0.7755654 \n\n# We can reduce the noise as follows:\nround(tapply(x, f, mean), digits=1)\n\n   1    2    3 \n-0.3  0.6  0.8 \n\n\n\n\n21.2.5 mapply\nThe mapply() function is a multivariate apply of sorts which applies a function in parallel over a set of arguments. Recall that lapply() and friends only iterate over a single R object. What if you want to iterate over multiple R objects in parallel? This is what mapply() is for.\n\nstr(mapply) \n\nfunction (FUN, ..., MoreArgs = NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE)  \n\n\nThe mapply() function has a different argument order from lapply() because the function to apply comes first rather than the object to iterate over. The R objects over which we apply the function are given in the ... argument because we can apply over an arbitrary number of R objects.\nExample 1:\n\nmapply(rep, 1:4, 4:1)\n\n[[1]]\n[1] 1 1 1 1\n\n[[2]]\n[1] 2 2 2\n\n[[3]]\n[1] 3 3\n\n[[4]]\n[1] 4\n\n\nExample 2:\n\nnoise &lt;- function(n, mean, sd) { \n            rnorm(n, mean, sd)\n          }\nmapply(noise, 1:5, 1:5, 2)\n\n[[1]]\n[1] -1.004983\n\n[[2]]\n[1] -0.2012958  5.8551029\n\n[[3]]\n[1] 4.475691 5.301174 4.193333\n\n[[4]]\n[1] 6.561300 8.247429 1.022848 6.973876\n\n[[5]]\n[1]  3.2210572  2.3405193 -0.3194301  7.4476711  3.8813917\n\n\nThe mapply() function can be used to automatically “vectorize” a function. What this means is that it can be used to take a function that typically only takes single arguments and create a new function that can take vector arguments. This is often needed when you want to plot functions.\nExample:\n## Generate some data\nx &lt;- rnorm(100)\n## This is not what we want\nsumsq(1:10, 1:10, x)  \n## Note that the call to sumsq() only produced one value instead of 10 values.\nmapply(sumsq, 1:10, 1:10, MoreArgs = list(x = x))\nThere’s even a function in R called vectorize() that automatically can create a vectorized version of your function. So, we could create a vsumsq() function that is fully vectorized as follows.\nExample:\nvsumsq &lt;- vectorize(sumsq, c(\"mu\", \"sigma\")) \nvsumsq(1:10, 1:10, x)\n\n\n21.2.6 split\nThe split() function takes a vector or other objects and splits it into groups determined by a factor or list of factors.\nThe form of the split() function is this:\n\nstr(split) \n\nfunction (x, f, drop = FALSE, ...)  \n\n\nThe combination of split() and a function like lapply() or sapply() is a common paradigm in R.\nExample 1:\n\nx &lt;- c(rnorm(10), runif(10), rnorm(10, 1))\nf &lt;- gl(3, 10) \n# split() returns a list\n(j &lt;- split(x, f))\n\n$`1`\n [1] -0.2351062  1.0849005 -0.2073284 -0.6839409  0.5267443  0.5026479\n [7] -0.1038182 -0.4041406 -0.4897551 -1.1820040\n\n$`2`\n [1] 0.9764283 0.9572011 0.4070798 0.6731056 0.9316433 0.7500879 0.5913454\n [8] 0.8603848 0.5486396 0.1240526\n\n$`3`\n [1] 0.8848965 0.7053862 1.6342864 0.9173015 0.4236826 0.5112164 1.1382702\n [8] 1.0402037 0.9944404 0.3444231\n\nmode(j)\n\n[1] \"list\"\n\n\nExample 2:\n# Splitting a Dataframe\ns &lt;- split(airquality, airquality$Month)"
  },
  {
    "objectID": "program_iteration.html#functional-programming-loops",
    "href": "program_iteration.html#functional-programming-loops",
    "title": "21  Iteration",
    "section": "21.3 Functional Programming Loops",
    "text": "21.3 Functional Programming Loops\nFunctional programming loops are built around functions that take other functions as inputs. In this chapter we’ll focus on three common tasks: modifying multiple columns, reading multiple files, and saving multiple objects. The tools that we need are provided by dplyr and purrr, both core members of the tidyverse.\n\nlibrary(tidyverse)\n\n\n21.3.1 Modifying multiple columns\nLet’s start creating a simple tibble and counting the number of observations and compute the median of every column.\nExample:\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\nYou could do it with copy-and-paste:\n\ndf |&gt; summarize(\n  n = n(),\n  a = median(a),\n  b = median(b),\n  c = median(c),\n  d = median(d),\n)\n\n# A tibble: 1 × 5\n      n     a      b       c      d\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1    10 0.805 -0.796 -0.0315 0.0922\n\n\nBut you can imagine that this will get very tedious if you have tens or even hundreds of columns. Instead, you can use across():\nExample:\n\ndf |&gt; summarize(\n  n = n(),\n  across(a:d, median),\n)\n\n# A tibble: 1 × 5\n      n     a      b       c      d\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1    10 0.805 -0.796 -0.0315 0.0922\n\n\nThere are two additional selection techniques that are particularly useful for across():\n\neverything(): selects every (non-grouping) column:\n\ndf |&gt; \n  group_by(grp) |&gt; \n  summarize(across(everything(), median))\n\nwhere(): allows you to select columns based on their type:\n-   `where(is.numeric)` selects all numeric columns.\n-   `where(is.character)` selects all string columns.\n-   `where(is.Date)` selects all date columns.\n-   `where(is.POSIXct)` selects all date-time columns.\n-   `where(is.logical)` selects all logical columns.\nJust like other selectors, you can combine these with Boolean algebra. For example, !where(is.numeric) selects all non-numeric columns."
  },
  {
    "objectID": "program_debugging.html#show-values",
    "href": "program_debugging.html#show-values",
    "title": "22  Debugging",
    "section": "22.1 Show values",
    "text": "22.1 Show values\nYou will spend a lot of time correcting errors in your programs. To find an error or bug, you need to be able to see how your variables change as you move through the branches and loops of your code.\nAn effective and simple way of doing this is to include statements like cat(\"var =\", var, \"\\n\") throughout the program, to display the values of variables such as var as the program executes. Once you have the program working you can delete these or just comment them so they are not executed.\nIt is also very helpful to make dry runs of your code, using simple starting conditions for which you know what the answer should be. These dry runs should ideally use short and simple versions of the final program, so that analysis of the output can be kept as simple as possible.\nGraphs and summary statistics of intermediate outcomes can be very revealing, and the code to create them is easily commented out for production runs."
  },
  {
    "objectID": "program_debugging.html#message",
    "href": "program_debugging.html#message",
    "title": "22  Debugging",
    "section": "22.2 Message",
    "text": "22.2 Message\nWe can fix this problem by anticipating the possibility of NA values and checking to see if the input is NA with the is.na() function:\nprintmessage3 &lt;- function(x) {\n    if(length(x) &gt; 1L)\n        stop(\"'x' has length &gt; 1\")\n        if(is.na(x))\n            print(\"x is a missing value!\")\n    else if(x &gt; 0) \n        print(\"x is greater than zero\")\n    else\n        print(\"x is less than or equal to zero\") \n    invisible(x)\n}"
  },
  {
    "objectID": "program_debugging.html#traceback",
    "href": "program_debugging.html#traceback",
    "title": "22  Debugging",
    "section": "22.3 traceback()",
    "text": "22.3 traceback()\nThe traceback() command prints out the function call stack after an error has occurred. The function call stack is the sequence of functions that was called before the error occurred.\nThe traceback() command must be called immediately after an error occurs. Once another function is called, you lose the traceback.\nlm(y ~ x) \nError in eval(expr, envir, enclos) : object 'y' not found\ntraceback() \n7: eval(expr, envir, enclos)\n6: eval(predvars, data, env)\n5: model.frame.default(formula = y \\~ x, drop.unused.levels = TRUE)\n4: model.frame(formula = y \\~ x, drop.unused.levels = TRUE)\n3: eval(expr, envir, enclos)\n2: eval(mf, parent.frame())\n1: lm(y \\~ x)\nLooking at the traceback is useful for figuring out roughly where an error occurred but it’s not useful for more detailed debugging. For that you might turn to the debug() function."
  },
  {
    "objectID": "program_debugging.html#debug",
    "href": "program_debugging.html#debug",
    "title": "22  Debugging",
    "section": "22.4 debug()",
    "text": "22.4 debug()\nThe debug() function takes a function as its first argument. Here is an example of debugging the lm() function.\ndebug(lm)   ## Flag the 'lm()' function for interactive debugging\nlm(y ~ x) debugging\nNow, every time you call the lm() function it will launch the interactive debugger. To turn this behavior off you need to call the undebug() function.\nThe debugger calls the browser at the very top level of the function body. From there you can step through each expression in the body. There are a few special commands you can call in the browser:\n\nn: executes the current expression and moves to the next expression.\nc: continues execution of the function and does not stop until either an error or the function exits.\nQ: quits the browser.\n\nYou can turn off interactive debugging with the undebug() function like this:\nundebug(lm)  ## Unflag the 'lm()' function for debugging"
  },
  {
    "objectID": "program_debugging.html#recover",
    "href": "program_debugging.html#recover",
    "title": "22  Debugging",
    "section": "22.5 recover()",
    "text": "22.5 recover()\nThe recover() function can be used to modify the error behavior of R when an error occurs. Normally, when an error occurs in a function, R will print out an error message, exit out of the function, and return you to your workspace to await further commands.\nWith recover() you can tell R that when an error occurs, it should halt execution at the exact point at which the error occurred.\noptions(error = recover)  ## Change default R error behavior\nread.csv(\"nosuchfile\")  ## This code doesn't work"
  },
  {
    "objectID": "program_profiling.html#system.time",
    "href": "program_profiling.html#system.time",
    "title": "23  Profiling R Code",
    "section": "23.1 system.time()",
    "text": "23.1 system.time()\nThe system.time() function computes the time (in seconds) needed to execute an expression and if there’s an error, gives the time until the error occurred. Using system.time() allows you to test certain functions or code blocks to see if they are taking excessive amounts of time.\nThe function returns an object of class proc_time which contains two useful bits of information:\n\nuser time: time charged to the CPU(s) for this expression.\nelapsed time: “wall clock” time, the amount of time that passes for you as you’re sitting there.\n\nThe elapsed time may be greater than the user time if the CPU spends a lot of time waiting around. The elapsed time may be smaller than the user time if your machine has multiple cores/processors (and is capable of using them).\nHere’s an example of where the elapsed time is greater than the user time:\n\n## Elapsed time &gt; user time \nsystem.time(readLines(\"http://www.amazon.com\")) \n\n   user  system elapsed \n   0.00    0.00    0.38 \n\n\nYou can time longer expressions by wrapping them in curly braces within the call to system.time().\nsystem.time({ \n  ## expression or loop or function\n  })"
  },
  {
    "objectID": "program_profiling.html#the-r-profiler",
    "href": "program_profiling.html#the-r-profiler",
    "title": "23  Profiling R Code",
    "section": "23.2 The R Profiler",
    "text": "23.2 The R Profiler\nsystem.time() assumes that you already know where the problem is and can call system.time() on that piece of code. What if you don’t know where to start? This is where the profiler comes in handy.\nIn conjunction with Rprof(), we will use the summaryRprof() function which summarizes the output from Rprof().\nRprof() keeps track of the function call stack at regularly sampled intervals and tabulates how much time is spent inside each function.\nThe profiler is started by calling the Rprof() function:\nRprof()  ## Turn on the profiler\nOnce you call the Rprof() function, everything that you do from then on will be measured by the profiler. Therefore, you usually only want to run a single R function or expression once you turn on the profiler and then immediately turn it off.\nThe profiler can be turned off by passing NULL to Rprof():\nRprof(NULL)  ## Turn off the profiler\nsummaryRprof()\nThe summaryRprof() function tabulates the R profiler output and calculates how much time is spend in which function. There are two methods for normalizing the data:\n\n“by.total”: divides the time spend in each function by the total run time.\n“by.self”: does the same as “by.total” but first subtracts out time spent in functions above the current function in the call stack.\n\nThe final bit of output that summaryRprof() provides is the sampling interval and the total runtime.\n$sample.interval \n[1] 0.02\n$sampling.time \n[1] 7.41"
  },
  {
    "objectID": "graphs.html#principles-of-graphics",
    "href": "graphs.html#principles-of-graphics",
    "title": "24  Graphs",
    "section": "24.1 Principles of Graphics",
    "text": "24.1 Principles of Graphics\n1. Show comparisons\nYou should always be comparing at least two things.\n2. Show causality, mechanism, explanation, systematic structure\nGenerally, it’s difficult to prove that one thing causes another thing even with the most carefully collected data. But it’s still often useful for your data graphics to indicate what you are thinking about in terms of cause.\n3. Show multivariate data\nThe point is that data graphics should attempt to show this information as much as possible, rather than reduce things down to one or two features that we can plot on a page… From the plot it seems that there is a slight negative relationship between the two variables….This example illustrates just one of many reasons why it can be useful to plot multivariate data and to show as many features as intelligently possible. In some cases, you may uncover unexpected relationships depending on how they are plotted or visualized.\n4. Integrate evidence\nData graphics should make use of many modes of data presentation simultaneously, not just the ones that are familiar to you or that the software can handle. One should never let the tools available drive the analysis; one should integrate as much evidence as possible on to a graphic as possible.\n5. Describe and document the evidence\nData graphics should be appropriately documented with labels, scales, and sources. A general rule for me is that a data graphic should tell a complete story all by itself. Is there enough information on that graphic for the person to get the story? While it is certainly possible to be too detailed, I tend to err on the side of more information rather than less.\n6. Content, Content, Content\nAnalytical presentations ultimately stand or fall depending on the quality, relevance, and integrity of their content. This includes the question being asked and the evidence presented in favor of certain hypotheses. Starting with a good question, developing a sound approach, and only presenting information that is necessary for answering that question, is essential to every data graphic."
  },
  {
    "objectID": "graphs.html#types-of-graphs",
    "href": "graphs.html#types-of-graphs",
    "title": "24  Graphs",
    "section": "24.2 Types of Graphs",
    "text": "24.2 Types of Graphs\nVisualizing the data via graphics can be important at the beginning stages of data analysis to understand basic properties of the data, to find simple patterns in data, and to suggest possible modeling strategies. In later stages of an analysis, graphics can be used to “debug” an analysis, if an unexpected (but not necessarily wrong) result occurs, or ultimately, to communicate your findings to others.\nWe will make a distinction between exploratory graphs and final graphs:\nExploratory graphs are usually made very quickly and a lot of them are made in the process of checking out the data, developing a personal understanding of the data and to prioritize tasks for follow up. Details like axis orientation or legends, while present, are generally cleaned up and prettified if the graph is going to be used for communication later.\n\n24.2.1 One-dimension graphs\n\n\n\n\n\n\n\n\nGraph\nCode\nUse\n\n\n\n\nFive-number summary\nfivenum(), summary()\n\n\n\nBoxplots\nboxplot(pollution$pm25, col = \"blue\")\nCommonly plot outliers that go beyond the bulk of the data\n\n\nBarplot\ntable(pollution$region) %&gt;% barplot(col = \"wheat\")\nFor visualizing categorical data, with the number of entries for each category being proportional to the height of the bar\n\n\nHistogram\nhist(pollution$pm25, col = \"green\", breaks = 100)\nrug(pollution$pm25)\nabline(v = 12, lwd = 2)\nabline(v = median(pollution$pm25), col = \"magenta\", lwd = 4)\nCheck skewness of the data, symmetry, multi-modality, and other features\n\n\nDensity plot\na &lt;- density(airquality$Ozone, na.rm = TRUE)\nplot(a)\nComputes a non-parametric estimate of the distribution of a variables\n\n\n\n\n\n24.2.2 Multi-dimension graphs\n\n\n\n\n\n\n\n\nGraph\nCode\nUse\n\n\n\n\nMultiple 1-D plots\nboxplot(pm25 ~ region, data = pollution, col = \"red\")\npar(mfrow = c(2, 1), mar = c(4, 4, 2, 1))\nhist(subset(pollution, region == \"east\")$pm25, col = \"green\")\nwith(subset(airquality, Month == 5), points(Wind, Ozone, col = \"blue\"))\nhist(subset(pollution, region == \"west\")$pm25, col = \"green\")&gt; boxplot(pm25 ~ region, data = pollution, col = \"red\")\npar(mfrow = c(2, 1), mar = c(4, 4, 2, 1))\nhist(subset(pollution, region == \"east\")$pm25, col = \"green\")\nwith(subset(airquality, Month == 5), points(Wind, Ozone, col = \"blue\"))\nhist(subset(pollution, region == \"west\")$pm25, col = \"green\")\nFor seeing the relationship between two variables, especially when one is naturally categorical\n\n\nScatterplots\nwith(pollution, plot(latitude, pm25, col = region)) &gt; abline(h = 12, lwd = 2, lty = 2)\nlevels(pollution$region)\npar(mfrow = c(2, 1), mar = c(4, 4, 2, 1))\nhist(subset(pollution, region == \"east\")$pm25, col = \"green\")\nhist(subset(pollution, region == \"west\")$pm25, col = \"green\")\nVisualizing two continuous variables\n\n\nScatter plot matrix\npairs(~var1+var2+var3+var4+var5, data=data, main=\"scatterplot matrix\")\nIt is used to find the correlation between a variable and other variables, and to select the important variables, which is also known as variable selection\n\n\nSmooth scatterplots\nwith(airquality, {plot(Temp, Ozone), lines(loess.smooth(Temp, Ozone))\nSimilar to scatterplots but rather plots a 2-D histogram. Can be useful for scatterplots with many many data points"
  },
  {
    "objectID": "graphs.html#the-base-plotting-system",
    "href": "graphs.html#the-base-plotting-system",
    "title": "24  Graphs",
    "section": "24.3 The Base Plotting System",
    "text": "24.3 The Base Plotting System\nThe base plotting system is the original plotting system for R. The basic model is sometimes referred to as the “artist’s palette” model. The idea is you start with blank canvas and build up from there.\nYou will typically start with a plot function (or similar plot creating function) to initiate a plot and then annotate the plot. If you don’t have a completely well-formed idea of how you want your data to look, you’ll often start by “throwing some data on the page” and then slowly add more information to it as our thought process evolves.\nThere are a few downsides though:\n\nYou can’t go backwards once the plot has started.\nWhile the base plotting system is nice in that it gives you the flexibility to specify these kinds of details to painstaking accuracy, sometimes it would be nice if the system could just figure it out for you.\nIt’s difficult to describe or translate a plot to others because there’s no clear graphical language or grammar that can be used to communicate what you’ve done.\n\n\n24.3.1 How to Create a Base Plot?\n\nFirst, you must read the data into R with read.csv(). For example, the avgpm25.csv dataset contains the annual mean PM2.5 averaged over the period 2008 through 2010\nclass &lt;- c(\"numeric\", \"character\", \"factor\", \"numeric\", \"numeric\")\npollution &lt;- read.csv(\"data/avgpm25.csv\", colClasses = class)  \nExplicitly launch a graphics device.\nCall a plotting function to make a plot (Note: if you are using a file device, no plot will appear on the screen).\nAnnotate the plot if necessary.\nExplicitly close graphics device with dev.off() (this is very important!).\n\nExample:\n# Open PDF device; create 'myplot.pdf' in my working directory\npdf(file = \"myplot.pdf\", width = 4, height = 3) # The height and width arguments are in units of inches.\n# Create plot and send to a file (no plot appears on screen) \nwith(faithful, plot(eruptions, waiting))\n# Annotate plot; still nothing on screen \ntitle(main = \"Old Faithful Geyser data\") \n# Close the PDF file device \ndev.off() \n# Now you can view the file 'myplot.pdf' on your computer\nLet’s further detail each of the steps involved in creating a base plot.\n\n\n24.3.2 Graphics Devices\nWe can think of a graphics device as being a platform upon which the plot is created. If we create a plot, then a default graphics device is automatically opened for the plot to appear upon. In other words, when you make a plot in R, it has to be “sent” to a specific graphics device.\nThe most common place for a plot to be “sent” is the screen device. Functions like plot() in base, xyplot() in lattice, or qplot in ggplot2 will default to sending a plot to the screen device. Therefore, when making a plot, you need to consider how the plot will be used to determine what device the plot should be sent to.\nThe list of devices supported by your installation of R is found in ?Devices.\n\nUsing the commands pdf, postscript, jpeg, png, or bmp, we can also produce graphics in the formats that correspond to these names.\nThe jpeg, png, and bmp graphics are all raster-style graphics, which may translate poorly when included in a document.\nIn contrast, the pdf, postscript, and windows metafile (win.metafile, available on Windows) formats allow for vector-style graphics, which are scaleable, and better suited to integration in documents.\n\n\n\n\n\n\n\n\nVector\nBitmap\n\n\n\n\nGood for line drawings and plots with solid colors using a modest number of points\ngood for plots with a large number of points, natural scenes or web-based plots\n\n\npdf (line-type graphics, resizes well, usually portable)\npng (good for line drawings or images with solid colors)\n\n\nsvg (XML-based scalable vector graphics; supports animation and interactivity)\njpeg (good for plotting many many many points, does not resize well)\n\n\nwin.metafile\ntiff\n\n\npostscript\nbmp\n\n\n\nIt is possible to open multiple graphics devices (screen, file, or both), for example when viewing multiple plots at once. Plotting can only occur on one graphics device at a time, though.\nOne way of having more than one plot visible is to open additional graphics devices. In a Windows environment this is done by using the command windows() before each additional plot.\nTo create a graphics device without a plot, we call the function that is specific to our operating system (that is, windows for Windows, quartz for Mac, and X11 for Unix).\nThe currently active graphics device can be found by calling dev.cur()\nEvery open graphics device is assigned an integer starting with 2 (there is no graphics device 1). You can change the active graphics device with dev.set(&lt;integer&gt;).\n\n\n24.3.3 Graphics Parameters\nGraphics parameters control how output appears on graphics devices. To get a complete list with their current values, type par(). Some of the parameters mentioned above, namely pch, lwd and col, are examples of graphics parameters. To get the value of a specific parameter, for example pch, type par(\"pch\").\nSome graphics parameters can apply to one or more plots, and others only make sense when applied to graphics devices. For example, to change the symbol for a single plot, we could include the argument pch = 2 in the call to the plot function. However, we could also make this change for all graphics that are produced on the device.\nTo change a graphics parameter for the graphics device, we use the par command. These are some examples:\n\npar(mfrow = c(a,b)) where a and b are integers, will create a matrix of plots on one page, with a rows and b columns. These will be filled by rows; use mfcol if you wish to fill them by columns.\nExample:\n\npar(mfrow = c(2, 2), mar=c(5, 4, 2, 1))\ncurve(x*sin(x), from = 0, to = 100, n = 1001)\ncurve(x*sin(x), from = 0, to = 10, n = 1001)\ncurve(x*sin(x), from = 0, to = 1, n = 1001)\ncurve(x*sin(x), from = 0, to = 0.1, n = 1001)\n\n\n\n\npar(mar = c(bottom, left, top, right)) will create space around each plot, in which to write axis labels and titles. Measurements are in units of character widths.\npar(oma = c(bottom, left, top, right)) will create space around the matrix of plots (an outer margin). Measurements are in units of character widths.\npar(las = 1) rotates labels on the y-axis to be horizontal rather than vertical.\npar(pty = \"s\") forces the plot shape to be square. The alternative is that the plot shape is mutable, which is the default, and corresponds to pty =\"m\".\n\n\n\n24.3.4 Make a Plot with plot(x,y)\nplot(x, y) is used to plot one vector against another, with the x values on the x-axis and the y values on the y-axis.\nThe plot command offers a wide variety of options for customizing the graphic. Each of the following arguments can be used within the plot statement, singly or together, separated by commas\n\ntype = \"p\": Determines the type of plot, with options:\n\n“p” for points (the default);\n“l” for lines;\n“b” for both, with gaps in the lines for the points; …\n\nExample:\n\nwith(airquality, plot(Wind, Ozone, \n    main = \"Ozone and Wind in New York City\",\n    type = \"p\")\n)\n\n\n\n\npoints(x, y): To add points (x[1], y[1]), (x[2], y[2]), … to the current plot.\nlines(x, y): To add lines.\nabline(v = xpos) and abline(h = ypos): to draw vertical or horizontal lines. - col: Both points and lines take the optional input from col (e.g. “red”, “blue”, etc.). The complete list of available colours can be obtained by the colours function (or colors).\nExample: # Plot with a Regression Line\n\nFirst make the plot (as above).\nFit a simple linear regression model using the lm() function.\nTake the output of lm() and pass it to the abline() function which automatically takes the information from the model object and calculates the corresponding regression line\n\ntext(x, y, labels): To add the text labels [i] at the point (x[i], y[i]). The optional input pos is used to indicate where to position the labels in relation to the points.\ntitle(text): To add a title, where text is a character string.\nmain = \"Plot title goes in here\": provides the plot title.\nxlab = \" \" / ylab = \" \": To add axis labels.\nNote: In a call to plot, the arguments for main, sub, and xlab and ylab can be character strings or expressions that contain a mathematical expression. For mathematical typesetting you can use the functions expression(require graphics) and bquote(base):\nExample:\n...\nxlab = expression(alpha),\nylab = expression(100 %*% (alpha^3 - alpha^2) + 15),\n# Mix of mathematical expressions and character strings into a single expression by using the paste function\nmain = expression(paste(\"Function: \",\n   f(alpha) == 100 %*% (alpha^3 - alpha^2) + 15)),\n...\nxlab = expression(alpha) tells R to interpret alpha in the context of the MML (mathematical markup language), producing an α as the label for the x-axis.\npch = k: Determines the shape of points, with k taking a value from 1 to 25.\nlwd = 1: line width, default 1.\nxlim = c(a,b)/ `ylim = c(a,b): Will set the lower and upper limits of the x-axis/y-axis to be a and b, respectively.\n\nAs an example we plot part of the parabola y2 = 4x, as well as its focus and directrix. We make use of the surprisingly useful input type = \"n\", which results in the graph dimensions being established, and the axes being drawn, but nothing else.\nExample:\n\nx &lt;- seq(0, 5, by = 0.01)\ny.upper &lt;- 2*sqrt(x)\ny.lower &lt;- -2*sqrt(x)\ny.max &lt;- max(y.upper)\ny.min &lt;- min(y.lower)\nplot(c(-2, 5), c(y.min, y.max), type = \"n\", xlab = \"x\", ylab = \"y\")\nlines(x, y.upper)\nlines(x, y.lower)\nabline(v=-1)\npoints(1, 0)\ntext(1, 0, \"focus (1, 0)\", pos=4)\ntext(-1, y.min, \"directrix x = -1\", pos = 4)\ntitle(\"The parabola y^2 = 4*x\")\n\n\n\n\n\n\n24.3.5 Augmenting a Plot\nA traditional plot can be augmented using any of a number of different tools after its creation. A number of these different steps are detailed below:\n\nStart by creating the plot object, which sets up the dimensions of the space, but omit any plot objects for the moment:\nExample:\nopar1 &lt;- par(las = 1, mar=c(4,4,3,2))   \nplot(ufc$dbh.cm, ufc$height.m, axes=FALSE, \n    xlab=\"\",\n    ylab=\"\",\n    type=\"n\"\n)\nNext, we add the points. Here we use different colours and symbols for different heights of trees: those that are realistic, and those that are not, which may reflect measurement errors. We use the vectorised ifelse() function:\nExample:\npoints(ufc$dbh.cm, ufc$height.m,     \n  col = ifelse(ufc$height.m &gt; 4.9, \"darkseagreen4\", \"red\"),\n  pch = ifelse(ufc$height.m &gt; 4.9, 1, 3)\n)\nThen we add axes. The following are the simplest possible calls. We can also control the locations of the tickmarks, and their labels; we can overlay different axes, change colour, and so on. ?axis provides the details:\nExample:\naxis(1)\naxis(2)\nWe can next add axis labels using margin text (switching back to vertical direction for the y-axis text):\nExample:\nopar2 &lt;- par(las=0)\nmtext(\"Diameter (cm)\", side=1, line=3)\nmtext(\"Height (m)\", side=2, line=3)\nWrap the plot in the traditional frame. As before, we can opt to use different line types and different colours:\nExample:\nbox()\nFinally, we add a legend:\nExample:\nlegend(x = 60, y = 15, c(\"Normal trees\", \"A weird tree\"),\n    col=c(\"darkseagreen3\", \"red\"),\n    pch=c(1, 3),\n    bty=\"n\"\n)\nNote the first two arguments: the location of the legend can also be expressed relative to the graph components, for example, by “bottomright”.\nIf we wish, we can return the graphics environment to a previous state:\npar(opar1)\n\nCheck out the playwith package, which provides interaction with graphical objects at a level unattainable in base R.\n\n\n24.3.6 Color in Plots\nTypically we add color to a plot, not to improve its artistic value, but to add another dimension to the visualization. It makes sense that the range and palette of colors you use will depend on the kind of data you are plotting. Careful choices of plotting color can have an impact on how people interpret your data and draw conclusions from them.\nThe function colors() lists the names of (657) colors you can use in any plotting function. Typically, you would specify the color in a (base) plotting function via the col argument.\nThe grDevices package has two functions, they differ only in the type of object that they return:\n\ncolorRamp: Take a palette of colors and return a function that takes values between 0 and 1.\npal &lt;- colorRamp(c(\"red\", \"blue\")) \npal(0)\nThe numbers in the matrix will range from 0 to 255 and indicate the quantities of red, green, and blue (RGB) in columns 1, 2, and 3 respectively. there are over 16 million colors that can be expressed in this way.\nThe idea here is that you do not have to provide just two colors in your initial color palette; you can start with multiple colors and colorRamp() will interpolate between all of them.\ncolorRampPalette: Takes a palette of colors and returns a function that takes integer arguments and returns a vector of colors interpolating the palette (like heat.colors() or topo.colors()).\npal &lt;- colorRampPalette(c(\"red\", \"yellow\"))\npal(3)\nReturns 3 colors in between red and yellow. Note that the colors are represented as hexadecimal strings.\nNote that the rgb() function can be used to produce any color via red, green, blue proportions and return a hexadecimal representation:\nrgb(0, 0, 234, maxColorValue = 255)\nPart of the art of creating good color schemes in data graphics is to start with an appropriate color palette that you can then interpolate with a function like colorRamp() or colorRampPalette().\n\nFor improved color palettes you can use the RColorBrewer package. Here is a display of all the color palettes available from this package:\n\nlibrary(RColorBrewer) \ndisplay.brewer.all()\n\n\n\n\n\nThe brewer.pal() function creates nice looking color palettes especially for thematic maps:\n\nlibrary(RColorBrewer)\n(cols &lt;- brewer.pal(3, \"BuGn\"))\n\n[1] \"#E5F5F9\" \"#99D8C9\" \"#2CA25F\"\n\n\nThese three colors make up your initial palette. Then you can pass them to colorRampPalette() to create my interpolating function.\n\npal &lt;- colorRampPalette(cols)\n\nNow you can plot your data using this newly created color (ramp) palette:\nimage(volcano, col = pal(20))\nThe smoothScatter() function is very useful for making scatterplots of very large datasets:\n\nset.seed(1)\nx &lt;- rnorm(10000)\ny &lt;- rnorm(10000) \nsmoothScatter(x, y)\n\n\n\n\nAdding transparency: Color transparency can be added via the alpha parameter to rgb() to produce color specifications with varying levels of transparency.\n\nrgb(1, 0, 0, 0.1)\n\n[1] \"#FF00001A\"\n\n\nTransparency can be useful when you have plots with a high density of points or lines. If you add some transparency to the black circles, you can get a better sense of the varying density of the points in the plot.\n\n# x, y from the previous example\nplot(x, y, pch = 19, col = rgb(0, 0, 0, 0.15)) \n\n\n\n\n\n\n\n24.3.7 Copying Plots\nNote that copying a plot is not an exact operation, so the result may not be identical to the original:\n## Copy my plot to a PNG file  (follows the previous code from pdf creation)\ndev.copy(png, file = \"geyserplot.png\") \n\n## Don't forget to close the PNG device! \ndev.off()\n\n\n24.3.8 Complementary Packages\n\ngraphics: contains plotting functions for the “base” graphing systems, including plot, hist, boxplot and many others\ngrdevices: contains all the code implementing the various graphics devices, including X11, PDF, PostScript, PNG, etc."
  },
  {
    "objectID": "graphs.html#the-lattice-system",
    "href": "graphs.html#the-lattice-system",
    "title": "24  Graphs",
    "section": "24.4 The Lattice System",
    "text": "24.4 The Lattice System\nTrellis graphics are a data visualisation framework developed at the Bell Labs, which have been implemented in R as the lattice package.\nTrellis graphics are a set of techniques for displaying multidimensional data. They allow great flexibility for producing conditioning plots; that is, plots obtained by conditioning on the value of one of the variables.\nTo use Trellis graphics you must first you must load the lattice package with the library function: library(lattice).\n\n24.4.1 When to Use Trellis Plots\n\nLattice plots tend to be most useful for conditioning types of plots, i.e. looking at how y changes with x across levels of z. These types of plots are useful for looking at multidimensional data and often allow you to squeeze a lot of information into a single window or page.\nWith the lattice system, plots are created with a single function call, such as xyplot() or bwplot().\nNote that there is no real distinction between the functions that create or initiate plots and the functions that annotate plots because it all happens at once.\nAnother difference from base plotting is that things like margins and spacing are set automatically. This is possible because entire plot is specified at once via a single function call.\nThe notion of panels comes up a lot with lattice plots because you typically have many panels in a lattice plot (each panel typically represents a condition, like “region”).\nOnce a plot is created, you cannot “add” to the plot (but of course you can just make it again with modifications).\n\n\n\n24.4.2 How to Use Lattice\nTo illustrate the use of the lattice package we use the ufc dataset, where dbh (diameter at breast height) and height vary by species. That is, the plots are conditioned on the value of the variable species.\nExample:\n# These plots display a distribution of values taken on by the variable dbh, divided up according to the value of the variable species\ndensityplot(~ dbh.cm | species, data = ufc) # A density plot\nbwplot(~ dbh.cm | species, data = ufc) # # A box and whiskers plot\nhistogram(~ dbh.cm | species, data = ufc) # A histogram\n# We plot height as a function of dbh.\nxyplot(height.m ~ dbh.cm | species, data = ufc) # A scatterplot\nAll four commands require a formula object, which is described using ~and |.\n\nIf a dataframe is passed to the function, using the argument data, then the column names of the dataframe can be used for describing the model.\nWe interpret y ~ x | a as saying we want y as a function of x, divided up by the different levels of a.\nIf `a is not a factor then a factor will be created by coercion.\nIf we are just interested in x we still include the ~ symbol, so that R knows that we are specifying a model.\nIf we wish to provide within-panel conditioning on a second variable, then we use the group argument.\n\nIn order to display numerous lattice objects on a graphics device, we call the print function with the split and more arguments.\nExample:\n# Place a lattice object (called my.lat) in the top-right corner of a 3-row, 2-column graphics device, and allow for more objects\nprint(my.lat, split = c(2,1,2,3), more = TRUE)\nSee ?print.trellis for further details.\nGraphics produced by lattice are highly customisable.\nExample:\nxyplot(height.m ~ dbh.cm | species,\n    data = ufc,\n    subset = species %in% list(\"WC\", \"GF\"),\n    panel = function(x, y, ...) {\n        panel.xyplot(x, y, ...),       \n        panel.abline(lm(y~x), ...)\n    },\n    xlab = \"Diameter (cm)\",\n    ylab = \"Height (m)\"\n    )\nNote that the subset argument is a logical expression or a vector of integers, and is used to restrict the data that are plotted. We can also change the order of the panels using the index.cond argument (see ?xyplot for more details). The panels are plotted bottom left to top right by default, or top left to bottom right if the argument as.table = TRUE is supplied.\nThe panel argument accepts a function, the purpose of which is to control the appearance of the plot in each panel. The panel function should have one input argument for each variable in the model, not including the conditioning variable."
  },
  {
    "objectID": "graphs.html#d-plots",
    "href": "graphs.html#d-plots",
    "title": "24  Graphs",
    "section": "24.5 3D Plots",
    "text": "24.5 3D Plots\nR provides considerable functionality for constructing 3D graphics, using either the base graphics engine or the lattice package. However, its is recommended to use the lattice package, because the data can be supplied to the lattice functions in a familiar structure: observations in rows and variables in columns, unlike that required by the base 3D graphics engine (observations in a grid).\nExample:\nufc.plots &lt;- read.csv(\"../data/ufc-plots.csv\")\nstr(ufc.plots)\nlibrary(lattice)\nwireframe(vol.m3.ha ~ east * north,\n    main = expression(paste(\"Volume (\", m^3, ha^{-1}, \")\", sep =        \"\")),\n    xlab = \"East (m)\", ylab = \"North (m)\",\n    data = ufc.plots\n    )\nTo learn more about base-graphics 3D plots, run the demonstrations demo(persp) and demo(image) and look at the examples presented."
  },
  {
    "objectID": "graphs.html#ggplot2",
    "href": "graphs.html#ggplot2",
    "title": "24  Graphs",
    "section": "24.6 ggplot2",
    "text": "24.6 ggplot2\nggplot2 is one of the most elegant and most versatile systems for making graphs. ggplot2 implements the grammar of graphics, a coherent system for describing and building graphs.\nggplot2 splits the difference between base and lattice in a number of ways. Taking cues from lattice, the ggplot2 system automatically deals with spacings, text, titles but also allows you to annotate by “adding” to a plot.\nYou can find more information in the Graphs section here.\n\n24.6.1 Installation\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\nA typical plot with the ggplot package looks as follows:\ndata(mpg)\nqplot(displ, hwy, data = mpg)\nIn ggplot2, aesthetics are the things we can see (e.g. position, color, fill, shape, line type, size). You can use aesthetics in ggplot2 via the aes() function:\nggplot(data, aes(x=var1, y=var2))\n\n\n24.6.2 Geometric objects\nGeometric objects are the plots or graphs you want to put in the chart.\nYou can use geom_point() to create a scatterplot, geom_line() to create a line plot, and geom_boxplot() to create a boxplot in the chart.\nhelp.search(\"geom_\", package=\"ggplot2\")\nIn ggplot2, geom is also the layers of the chart. You can add in one geom object after another, just like adding one layer after another layer.\nggplot(data, aes(x=var1, y=var2)) + \n  geom_point(aes(color=\"red\"))\n\n\n24.6.3 Plotly\nPlotly JS allows you to create interactive, publication-quality charts. You can create a Plotly chart using ggplot:\ninstall.packages(\"plotly\")\nset.seed(12)\nvar1 &lt;- rnorm(100, mean=1, sd=1)\nvar2 &lt;- rnorm(100, mean=2, sd=1)\ndata &lt;- data.frame(var1, var2)\ngg &lt;- ggplot(data) + geom_line(aes(x=var1, y=var2))\ng &lt;- ggplotly(gg)"
  },
  {
    "objectID": "graph_ggplot.html#the-layered-grammar-of-graphics",
    "href": "graph_ggplot.html#the-layered-grammar-of-graphics",
    "title": "25  Creating a ggplot",
    "section": "25.1 The Layered Grammar of Graphics",
    "text": "25.1 The Layered Grammar of Graphics\nThe grammar of graphics is a template made of seven parameters (the bracketed words that appear in the template) for building plots. The grammar of graphics is based on the insight that you can uniquely describe any plot as a combination of a dataset, a geom, a set of mappings, a stat, a position adjustment, a coordinate system, a faceting scheme, and a theme.\nggplot(data = &lt;DATA&gt;) + \n  &lt;GEOM_FUNCTION&gt;(\n     mapping = aes(&lt;MAPPINGS&gt;),\n     stat = &lt;STAT&gt;, \n     position = &lt;POSITION&gt;\n  ) +\n  &lt;COORDINATE_FUNCTION&gt; +\n  &lt;FACET_FUNCTION&gt;\nTo build a basic plot from scratch:\n\nYou could start with a dataset and then transform it into the information that you want to display (with a stat).\nNext, you could choose a geometric object to represent each observation in the transformed data.\nYou could then use the aesthetic properties of the geoms to represent variables in the data. You would map the values of each variable to the levels of an aesthetic.\nYou’d then select a coordinate system to place the geoms into, using the location of the objects (which is itself an aesthetic property) to display the values of the x and y variables.\nYou could further adjust the positions of the geoms within the coordinate system (a position adjustment) or split the graph into subplots (faceting).\nYou could also extend the plot by adding one or more additional layers, where each additional layer uses a data set, a geom, a set of mappings, a stat, and a position adjustment.\n\nTo learn more about the theoretical underpinnings that describes the theory of ggplot2 in detail, click here: The Layered Grammar of Graphics."
  },
  {
    "objectID": "graph_ggplot.html#define-a-plot-object",
    "href": "graph_ggplot.html#define-a-plot-object",
    "title": "25  Creating a ggplot",
    "section": "25.2 Define a Plot Object",
    "text": "25.2 Define a Plot Object\nWith ggplot2, you begin a plot with the function ggplot(), defining a plot object that you then add layers to.\nThe first argument of ggplot() is the dataset to use in the graph and so ggplot(data = penguins) creates an empty graph that is primed to display the penguins data, but since we haven’t told it how to visualize it yet, for now it’s empty (it’s like an empty canvas you’ll paint the remaining layers of your plot onto).\nExample:\n\nggplot(data = penguins)"
  },
  {
    "objectID": "graph_ggplot.html#map-the-data",
    "href": "graph_ggplot.html#map-the-data",
    "title": "25  Creating a ggplot",
    "section": "25.3 Map the Data",
    "text": "25.3 Map the Data\nNext, we need to tell ggplot() how the information from our data will be visually represented. The mapping argument of the ggplot() function defines how variables in your dataset are mapped to visual properties (aesthetics) of your plot.\nThe mapping argument is always defined in the aes() function, and the x and yarguments of aes() specify which variables to map to the x and y axes.\nExample:\n\nggplot(data = penguins,\n       mapping = aes(x = flipper_length_mm, \n                     y = body_mass_g))\n\n\n\n\nOur empty canvas now has more structure – it’s clear where flipper lengths will be displayed (on the x-axis) and where body masses will be displayed (on the y-axis). But the penguins themselves are not yet on the plot. This is because we have not yet determine how to represent the observations from our dataframe on our plot."
  },
  {
    "objectID": "graph_ggplot.html#represent-the-data-geoms",
    "href": "graph_ggplot.html#represent-the-data-geoms",
    "title": "25  Creating a ggplot",
    "section": "25.4 Represent the Data (geoms)",
    "text": "25.4 Represent the Data (geoms)\nTo do so, we need to define a geom: the geometrical object that a plot uses to represent data. These geometric objects are made available in ggplot2 with functions that start with geom_.\nFor example, bar charts use geom_bar(), line charts use geom_line(), boxplots use geom_boxplot(), scatterplots use geom_point(), and so on.\nExample:\n\nggplot(data = penguins,\n       mapping = aes(x = flipper_length_mm, \n                     y = body_mass_g)) +\n  geom_point()\n\n\n\n\nNote: One common problem when creating ggplot2 graphics is to put the + in the wrong place: it has to come at the end of the line, not the start.\nYou can rewrite the previous plot more concisely:\n\nggplot(penguins, aes(x = flipper_length_mm, \n                     y = body_mass_g)) + \n  geom_point()\n\n\n\n\nOr using the pipe, |&gt;:\n\npenguins |&gt; \n  ggplot(aes(x = flipper_length_mm, \n             y = body_mass_g)) + \n  geom_point()\n\n\n\n\nNow we have something that looks like what we might think of as a “scatterplot”.\nYou can also set the visual properties of your geom manually as an argument of your geom function (outside of aes()) instead of relying on a variable mapping to determine the appearance. For example, we can make all of the points in our plot blue:\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point(color = \"blue\")\n\n\n\n\n\n25.4.1 Help with geoms\nThe best place to get a comprehensive overview of all of the geoms ggplot2 offers, as well as all functions in the package, is this page: https://ggplot2.tidyverse.org/reference.\nTo learn more about any single geom, use the help (e.g., ?geom_smooth).\nSee also https://exts.ggplot2.tidyverse.org/gallery/ for a sampling of community made geoms."
  },
  {
    "objectID": "graph_ggplot.html#add-aesthetics-and-layers",
    "href": "graph_ggplot.html#add-aesthetics-and-layers",
    "title": "25  Creating a ggplot",
    "section": "25.5 Add Aesthetics and Layers",
    "text": "25.5 Add Aesthetics and Layers\nYou will we need to modify the aesthetic mapping, inside of aes().\nExample:\n\nggplot(data = penguins, \n       mapping = aes(x = flipper_length_mm, \n                     y = body_mass_g, \n                     color = species)) +\n  geom_point()\n\n\n\n\nOnce you map an aesthetic, ggplot2 takes care of the rest. It selects a reasonable scale to use with the aesthetic, and it constructs a legend that explains the mapping between levels and values. For x and y aesthetics, ggplot2 does not create a legend, but it creates an axis line with tick marks and a label. The axis line provides the same information as a legend; it explains the mapping between locations and values.\nWhen a categorical variable is mapped to an aesthetic, ggplot2 will automatically assign a unique value of the aesthetic (here a unique color) to each unique level of the variable (each of the three species), a process known as scaling. ggplot2 will also add a legend that explains which values correspond to which levels.\nNow let’s add one more layer: a smooth curve displaying the relationship between body mass and flipper length. Since this is a new geometric object representing our data, we will add a new geom as a layer on top of our point geom: geom_smooth(). And we will specify that we want to draw the line of best fit based on a linear model with method = \"lm\".\nExample:\n\nggplot(data = penguins,\n       mapping = aes(x = flipper_length_mm, \n                     y = body_mass_g, \n                     color = species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWhen aesthetic mappings are defined in ggplot(), at the global level, they’re passed down to each of the subsequent geom layers of the plot. However, each geom function in ggplot2 can also take a mapping argument, which allows for aesthetic mappings at the local level that are added to those inherited from the global level. In other words, if you place mappings in a geom function, ggplot2 will treat them as local mappings for the layer. It will use these mappings to extend or overwrite the global mappings for that layer only. This makes it possible to display different aesthetics in different layers.\nExample:\n\nggplot(data = penguins,\n       mapping = aes(x = flipper_length_mm, \n                     y = body_mass_g)) +\n  geom_point(mapping = aes(color = species)) +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nNote that since we want points to be colored based on species but don’t want the lines to be separated out for them, we should specify color = species for geom_point() only.\nYou can specify different data for each layer. Here, we use red points as well as open circles to highlight two-seater cars. The local data argument in geom_point() overrides the global data argument in ggplot() for that layer only.\nExample:\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  geom_point(\n    data = mpg |&gt; filter(class == \"2seater\"), \n    color = \"red\"\n  ) +\n  geom_point(\n    data = mpg |&gt; filter(class == \"2seater\"), \n    shape = \"circle open\", size = 3, color = \"red\"\n  )\n\n\n\n\nIt’s generally not a good idea to represent information using only colors on a plot, as people perceive colors differently due to color blindness or other color vision differences. Therefore, in addition to color, we can also map species to the shape aesthetic.\nExample:\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n25.5.1 Help with aes\nYou can learn more about all possible aesthetic mappings in the aesthetic specifications vignette at https://ggplot2.tidyverse.org/articles/ggplot2-specs.html. Remember that the specific aesthetics you can use for a plot depend on the geom you use to represent the data."
  },
  {
    "objectID": "graph_ggplot.html#statistical-transformation",
    "href": "graph_ggplot.html#statistical-transformation",
    "title": "25  Creating a ggplot",
    "section": "25.6 Statistical Transformation",
    "text": "25.6 Statistical Transformation\nMany graphs, like scatterplots, plot the raw values of a dataset. Other graphs, like bar charts, calculate new values to plot. The algorithm used to calculate new values for a graph is called a stat, short for statistical transformation.\nYou can figure out which stat a geom uses by inspecting the default value for the stat argument. For example, ?geom_bar shows that the default value for stat is “count”, which means that geom_bar() uses stat_count(). To find the possible variables that can be computed by the stat, look for the section titled “computed variables” in the help for geom_bar().\nEvery geom has a default stat; and every stat has a default geom. This means that you can typically use geoms without worrying about the underlying statistical transformation. However, there are three reasons why you might need to use a stat explicitly:\n\nYou might want to override the default stat. In the following example we change the stat of geom_bar() from count (the default) to identity. This lets us map the height of the bars to the raw values of a y variable.\nExample:\n\ndiamonds |&gt;\n  count(cut) |&gt;\n  ggplot(aes(x = cut, y = n)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\nYou might want to override the default mapping from transformed variables to aesthetics.\nExample:\n\n# Display a bar chart of proportions, rather than counts\nggplot(diamonds, aes(x = cut, y = after_stat(prop), group = 1)) + \n  geom_bar()\n\n\n\n\nYou might want to focus on the statistical transformation in your code. For example, you might use stat_summary(), which summarizes the y values for each unique x value, to draw attention to the summary that you’re computing:\nExample:\n\nggplot(diamonds) + \n  stat_summary(\n    aes(x = cut, y = depth),\n    fun.min = min,\n    fun.max = max,\n    fun = median\n  )"
  },
  {
    "objectID": "graph_ggplot.html#position-adjustments",
    "href": "graph_ggplot.html#position-adjustments",
    "title": "25  Creating a ggplot",
    "section": "25.7 Position Adjustments",
    "text": "25.7 Position Adjustments\nYou can color a bar chart using either the color aesthetic, or, more usefully, the fill aesthetic:\nExample:\n\nggplot(mpg, aes(x = drv, color = drv)) + \n  geom_bar()\n\n\n\n\nExample:\n\nggplot(mpg, aes(x = drv, fill = drv)) + \n  geom_bar()\n\n\n\n\nNote that if you map the fill aesthetic to another variable, like class, the bars are automatically stacked. In our example, each colored rectangle represents a combination of drv and class.\nExample:\n\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar()\n\n\n\n\nThe stacking is performed automatically using the position adjustment specified by the position argument. If you don’t want a stacked bar chart, you can use one of four other options:\n\nposition = \"identity\": will place each object exactly where it falls in the context of the graph. See how bars overlap and why the identity position adjustment is more useful for 2d geoms, like points, where it is the default.\n\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar(alpha = 1/5, position = \"identity\")\n\n\n\n\nposition = \"fill\": works like stacking, but makes each set of stacked bars the same height. This makes it easier to compare proportions across groups.\nExample:\n\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar(position = \"fill\")\n\n\n\n\nposition = \"dodge\": places overlapping objects directly beside one another. This makes it easier to compare individual values.\nExample:\n\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\nposition = \"jitter\": adds a small amount of random noise to each point in a scatterplot. This spreads the points out to solve the problem of overplotting, which makes it difficult to see the distribution of the data.\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point(position = \"jitter\")"
  },
  {
    "objectID": "graph_ggplot.html#coordinate-systems",
    "href": "graph_ggplot.html#coordinate-systems",
    "title": "25  Creating a ggplot",
    "section": "25.8 Coordinate Systems",
    "text": "25.8 Coordinate Systems\nThe default coordinate system is the Cartesian coordinate system where the x and y positions act independently to determine the location of each point.\nThere are two other coordinate systems that are occasionally helpful.\n\ncoord_quickmap(): sets the aspect ratio correctly for plotting spatial data with ggplot2. See more in the Maps chapter of the ggplot2 book.\nExample:\n\nlibrary(maps)\n\nnz &lt;- map_data(\"nz\")\n\nggplot(nz, aes(x = long, y = lat, group = group)) +\n  geom_polygon(fill = \"white\", color = \"black\")\n\n\n\nggplot(nz, aes(x = long, y = lat, group = group)) +\n  geom_polygon(fill = \"white\", color = \"black\") +\n  coord_quickmap()\n\n\n\n\ncoord_polar(): uses polar coordinates. Polar coordinates can reveal interesting connections between a bar chart and a Coxcomb chart.\nExample:\n\nbar &lt;- ggplot(data = diamonds) + \n  geom_bar(\n    mapping = aes(x = clarity, fill = clarity), \n    show.legend = FALSE,\n    width = 1\n  ) + \n  theme(aspect.ratio = 1)\n\nbar + coord_flip()\n\n\n\nbar + coord_polar()"
  },
  {
    "objectID": "graph_ggplot.html#facets",
    "href": "graph_ggplot.html#facets",
    "title": "25  Creating a ggplot",
    "section": "25.9 Facets",
    "text": "25.9 Facets\nFacets splits a plot into subplots that each display one subset of the data based on a categorical variable.\nExample:\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_wrap(~cyl)\n\n\n\n\nTo facet your plot with the combination of two variables, switch from facet_wrap() to facet_grid(). The first argument of facet_grid() is also a formula, but now it’s a double sided formula: rows ~ cols.\nExample:\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_grid(drv ~ cyl)\n\n\n\n\nBy default each of the facets share the same scale and range for x and y axes. This is useful when you want to compare data across facets but it can be limiting when you want to visualize the relationship within each facet better. Setting the scales argument in a faceting function to free_x will allow for different scales of x-axis across columns, free_y will allow for different scales on y-axis across rows, and free will allow both.\nExample:\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_grid(drv ~ cyl, scales = \"free\")"
  },
  {
    "objectID": "graph_ggplot.html#improve-labels",
    "href": "graph_ggplot.html#improve-labels",
    "title": "25  Creating a ggplot",
    "section": "25.10 Improve Labels",
    "text": "25.10 Improve Labels\nAnd finally, we can improve the labels of our plot using the labs() function in a new layer:\n\ntitle, adds a title.\nsubtitle, adds a subtitle to the plot.\nx, is the x-axis label.\ny, is the y-axis label.\ncolor and shape define the label for the legend.\n\nIn addition, we can improve the color palette to be colorblind safe with the scale_color_colorblind() function from the ggthemes package.\nExample:\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Body mass and flipper length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n    color = \"Species\", shape = \"Species\"\n  ) +\n  scale_color_colorblind()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe purpose of a plot title is to summarize the main finding. Avoid titles that just describe what the plot is (e.g. “Fuel efficcieny generally decreases with engine size”).\n\n25.10.1 Mathematical labels\nIt’s possible to use mathematical equations instead of text strings. Just switch ” ” out for quote() and read about the available options in ?plotmath:\nExample:\nggplot(df, aes(x, y)) +\n  geom_point() +\n  labs(\n    x = quote(x[i]),\n    y = quote(sum(x[i] ^ 2, i == 1, n))\n  )"
  },
  {
    "objectID": "graph_ggplot.html#themes",
    "href": "graph_ggplot.html#themes",
    "title": "25  Creating a ggplot",
    "section": "25.11 Themes",
    "text": "25.11 Themes\ngplot2 includes the eight themes, with theme_gray() as the default. Many more are included in add-on packages like ggthemes (https://jrnold.github.io/ggthemes). You can also create your own themes, if you are trying to match a particular corporate or journal style.\nExample:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(se = FALSE) +\n  theme_bw()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nIt’s also possible to control individual components of each theme, like the size and color of the font used for the y axis.\nExample:\n  ...\n  theme(\n    legend.position = c(0.6, 0.7),\n    legend.direction = \"horizontal\",\n    legend.box.background = element_rect(color = \"black\"),\n    plot.title = element_text(face = \"bold\"),\n    plot.title.position = \"plot\",\n    plot.caption.position = \"plot\",\n    plot.caption = element_text(hjust = 0)\n  )\nFor an overview of all theme() components, see help with ?theme."
  },
  {
    "objectID": "graph_ggplot.html#layout",
    "href": "graph_ggplot.html#layout",
    "title": "25  Creating a ggplot",
    "section": "25.12 Layout",
    "text": "25.12 Layout\nWhat if you have multiple plots you want to lay out in a certain way? The patchwork package allows you to combine separate plots into the same graphic.\nTo place two plots next to each other, you can simply add them to each other. Note that you first need to create the plots and save them as objects (in the following example they’re called p1 and p2). Then, you place them next to each other with +.\n\nlibrary(patchwork)\np1 &lt;- ggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  labs(title = \"Plot 1\")\np2 &lt;- ggplot(mpg, aes(x = drv, y = hwy)) + \n  geom_boxplot() + \n  labs(title = \"Plot 2\")\np1 + p2\n\n\n\n\nYou can also create complex plot layouts with patchwork. In the following, | places the p1 and p3 next to each other and / moves p2 to the next line.\n\np3 &lt;- ggplot(mpg, aes(x = cty, y = hwy)) + \n  geom_point() + \n  labs(title = \"Plot 3\")\n(p1 | p3) / p2"
  },
  {
    "objectID": "graph_ggplot.html#save-your-plot",
    "href": "graph_ggplot.html#save-your-plot",
    "title": "25  Creating a ggplot",
    "section": "25.13 Save Your Plot",
    "text": "25.13 Save Your Plot\nOnce you’ve made a plot, you might want to get it out of R by saving it as an image that you can use elsewhere. That’s the job of ggsave(), which will save the plot most recently created to disk:\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\n\n\n\nggsave(filename = \"penguin-plot.png\")\n\nSaving 7 x 5 in image\n\n\nThis will save your plot to your working directory."
  },
  {
    "objectID": "graph_types.html#visualizing-categorical-variables",
    "href": "graph_types.html#visualizing-categorical-variables",
    "title": "26  Types of Graphs",
    "section": "26.1 Visualizing Categorical Variables",
    "text": "26.1 Visualizing Categorical Variables\nA variable is categorical if it can only take one of a small set of values.\n\n26.1.1 Bar Chart\nTo examine the distribution of a categorical variable, you can use a bar chart. The height of the bars displays how many observations occurred with each x value.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(ggthemes)\n\nExample:\n\nggplot(penguins, aes(x = species)) +\n  geom_bar()\n\n\n\n\n\n\n26.1.2 Bar Plots\nIn bar plots of categorical variables with non-ordered levels, like the penguin species above, it’s often preferable to reorder the bars based on their frequencies. Doing so requires transforming the variable to a factor (how R handles categorical data) and then reordering the levels of that factor.\nExample:\n\nggplot(penguins, aes(x = fct_infreq(species))) +\n  geom_bar()"
  },
  {
    "objectID": "graph_types.html#visualizing-numerical-variables",
    "href": "graph_types.html#visualizing-numerical-variables",
    "title": "26  Types of Graphs",
    "section": "26.2 Visualizing Numerical Variables",
    "text": "26.2 Visualizing Numerical Variables\nA variable is numerical (or quantitative) if it can take on a wide range of numerical values, and it is sensible to add, subtract, or take averages with those values. Numerical variables can be continuous or discrete.\n\n26.2.1 Histogram\nOne commonly used visualization for distributions of continuous variables is a histogram.\nExample:\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 200)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nA histogram divides the x-axis into equally spaced bins and then uses the height of a bar to display the number of observations that fall in each bin. In the graph above, the tallest bar shows that 39 observations have a body_mass_g value between 3,500 and 3,700 grams, which are the left and right edges of the bar.\nYou can set the width of the intervals in a histogram with the binwidth argument, which is measured in the units of the x variable.\nExample:\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 20)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 2000)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nAn alternative visualization for distributions of numerical variables is a density plot. A density plot is a smoothed-out version of a histogram and a practical alternative, particularly for continuous data that comes from an underlying smooth distribution.\nExample:\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_density()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`)."
  },
  {
    "objectID": "graph_types.html#visualizing-relationships",
    "href": "graph_types.html#visualizing-relationships",
    "title": "26  Types of Graphs",
    "section": "26.3 Visualizing Relationships",
    "text": "26.3 Visualizing Relationships\nTo visualize a relationship we need to have at least two variables mapped to aesthetics of a plot.\n\n26.3.1 Numerical and Categorical Variable\nYou can use side-by-side box plots. A boxplot is a type of visual shorthand for measures of position (percentiles) that describe a distribution. It is also useful for identifying potential outliers.\nExample:\n\nggplot(penguins, aes(x = species, y = body_mass_g)) +\n  geom_boxplot()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nAlternatively, we can make density plots with geom_density().\nExample:\n\nggplot(penguins, aes(x = body_mass_g, color = species)) +\n  geom_density(linewidth = 0.75)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\nAdditionally, we can map species to both color and fill aesthetics and use the alpha aesthetic to add transparency to the filled density curves. This aesthetic takes values between 0 (completely transparent) and 1 (completely opaque). In the following plot it’s set to 0.5.\n\nggplot(penguins, aes(x = body_mass_g, color = species, fill = species)) +\n  geom_density(alpha = 0.5)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n26.3.2 Two Categorical Variables\nWe can use stacked bar plots to visualize the relationship between two categorical variables. For example, the following two stacked bar plots both display the relationship between island and species, or specifically, visualizing the distribution of species within each island.\nExample:\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar()\n\n\n\n\nThe following plot, a relative frequency plot created by setting position = \"fill\" in the geom, is more useful for comparing species distributions across islands since it’s not affected by the unequal numbers of penguins across the islands.\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar(position = \"fill\")\n\n\n\n\nIn creating these bar charts, we map the variable that will be separated into bars to the x aesthetic, and the variable that will change the colors inside the bars to the fill aesthetic.\n\n\n26.3.3 Two Numerical Variables\nA scatterplot is probably the most commonly used plot for visualizing the relationship between two numerical variables.\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n26.3.4 Three or More Variables\nWe can incorporate more variables into a plot by mapping them to additional aesthetics. For example, in the following scatterplot the colors of points represent species and the shapes of points represent islands.\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = island))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n26.3.5 Facets\nAdding too many aesthetic mappings to a plot makes it cluttered and difficult to make sense of. Another way, which is particularly useful for categorical variables, is to split your plot into facets, subplots that each display one subset of the data.\nTo facet your plot by a single variable, use facet_wrap(). The first argument of facet_wrap() is a formula3, which you create with ~ followed by a variable name. The variable that you pass to facet_wrap() should be categorical.\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = species)) +\n  facet_wrap(~island)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nIf you don’t specify the width and height they will be taken from the dimensions of the current plotting device. For reproducible code, you’ll want to specify them."
  },
  {
    "objectID": "graph_types.html#visualizing-maps",
    "href": "graph_types.html#visualizing-maps",
    "title": "26  Types of Graphs",
    "section": "26.4 Visualizing Maps",
    "text": "26.4 Visualizing Maps\n\n26.4.1 Coordinates on a Map\nExample:\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(mapview)\nlibrary(modeldata)\n\ndata(ames)\n\names |&gt;\n  select(Longitude, Latitude, Neighborhood) |&gt;\n  # glimpse() |&gt; \n  mapview(map.types = \"OpenStreetMap\",\n          xcol = \"Longitude\", ycol = \"Latitude\", \n          zcol = \"Neighborhood\", \n          crs = 4269, \n          grid = FALSE)\n\n\n\n\n\n\nYou can find more information here\n\n\n26.4.2 Maps silhouettes\nExample\n\nlibrary(maps)\n\nnz &lt;- map_data(\"nz\")\n\nggplot(nz, aes(x = long, y = lat, group = group)) +\n  geom_polygon(fill = \"white\", color = \"black\")\n\n\n\nggplot(nz, aes(x = long, y = lat, group = group)) +\n      geom_polygon(fill = \"white\", color = \"black\") +\n      coord_quickmap()"
  },
  {
    "objectID": "model_flow.html",
    "href": "model_flow.html",
    "title": "27  Modeling Workflow",
    "section": "",
    "text": "A model workflow is important in two ways. First, using a workflow encourages good methodology since it is a single point of entry to the estimation components of a data analysis. Second, it enables the user to better organize projects\nThe typical path to determining an appropriate model is an iterative process. The general phases are:\n\nExploratory data analysis (EDA): Initially there is a back and forth between numerical analysis and data visualization where different discoveries lead to more questions and data analysis side-quests to gain more understanding.\nFeature engineering: The understanding gained from EDA results in the creation of specific model terms that make it easier to accurately model the observed data. This can include complex methodologies (e.g., PCA) or simpler features (using the ratio of two predictors).\nModel tuning and selection: A variety of models are generated and their performance is compared. Some models require parameter tuning in which some structural parameters must be specified or optimized. This is the stage where repeated data splitting is used for resampling.\nModel evaluation: During this phase of model development, we assess the model’s performance metrics, examine residual plots, and conduct other EDA-like analyses to understand how well the models work. In some cases, formal between-model comparisons help you understand whether any differences in models are within the experimental noise.\n\nAfter an initial sequence of these tasks, more understanding is gained regarding which models are superior as well as which data subpopulations are not being effectively estimated. This leads to additional EDA and feature engineering, another round of modeling, and so on. Once the data analysis goals are achieved, typically the last steps are to finalize, document, and communicate the model. For predictive models, it is common at the end to validate the model on an additional set of data reserved for this specific purpose."
  },
  {
    "objectID": "model_spend.html#splitting-data",
    "href": "model_spend.html#splitting-data",
    "title": "28  Spending Your Data",
    "section": "28.1 Splitting Data",
    "text": "28.1 Splitting Data\nThe primary approach for empirical model validation is to split the existing pool of data into two distinct sets:\n\nThe training set. It is usually the majority of the data. These data are a sandbox for model building where different models can be fit, feature engineering strategies are investigated, and so on. As modeling practitioners, we spend the vast majority of the modeling process using the training set as the substrate to develop the model.\nThe test set. This is held in reserve until one or two models are chosen as the methods most likely to succeed. The test set is then used as the final arbiter to determine the efficacy of the model. More specifically, it is critical to:\n\nLook at the test set only once; otherwise, it becomes part of the modeling process.\nQuarantine the test set from any model building activities.\nMirror what the model would encounter in the wild. In other words, the test set should always resemble new data that will be given to the model.\n\n\nNote that the proportion of data that should be allocated for splitting is highly dependent on the context of the problem at hand. Too little data in the training set hampers the model’s ability to find appropriate parameter estimates. Conversely, too little data in the test set lowers the quality of the performance estimates. Bear in mind that keeping the training data in a separate data frame from the test set is one small check to make sure that information leakage does not occur by accident.\nSuppose we allocate 80% of the data to the training set and the remaining 20% for testing. The most common method is to use simple random sampling. The rsample package has tools for making data splits such as this; the function initial_split() was created for this purpose. It takes the data frame as an argument as well as the proportion to be placed into training.\nExample:\n\nlibrary(tidymodels)\ntidymodels_prefer()\n\n# Set the random number stream so that the results can be \n# reproduced later\nset.seed(501)\n\n# Save the split information for an 80/20 split of the data\names_split &lt;- initial_split(ames, prop = 0.80)\names_split\n\n&lt;Training/Testing/Total&gt;\n&lt;2344/586/2930&gt;\n\n\nThe printed information denotes the amount of data in the training set (n=2,344), the amount in the test set (n=586), and the size of the original pool of samples (n=2,930).\nThe object ames_split is an rsplit object and contains only the partitioning information; to get the resulting data sets, we apply two more functions:\n\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\ndim(ames_train)\n\n[1] 2344   74\n\n\nThese objects are data frames with the same columns as the original data but only the appropriate rows for each set.\n\n28.1.1 Stratified sampling\nSimple random sampling is appropriate in many cases but there are exceptions. When there is a dramatic class imbalance in classification problems, one class occurs much less frequently than another. Using a simple random sample may haphazardly allocate these infrequent samples disproportionately into the training or test set. To avoid this, stratified sampling can be used. The training/test split is conducted separately within each class and then these subsamples are combined into the overall training and test set.\nAs an example, a stratified random sample would conduct the 80/20 split within each of these data subsets and then pool the results. In rsample, this is achieved using the strata argument:\nExample:\n\nset.seed(502)\n\n# Only a single column can be used for stratification (e.g. Sale_Price)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\ndim(ames_train)\n\n[1] 2342   74\n\n\n\n\n28.1.2 Time series\nThere is one situation when random sampling is not the best choice: when the data have a significant time component, such as time series data. Here, it is more common to use the most recent data as the test set.\nThe rsample package contains a function called initial_time_split() that is very similar to initial_split(). Instead of using random sampling, the prop argument denotes what proportion of the first part of the data should be used as the training set; the function assumes that the data have been pre-sorted in an appropriate order.\n\n\n28.1.3 Validation set\nIn the neural network and deep learning literature it is common to hear about validation sets as an answer to the question about how we can tell what is the best test set that should be used to properly evaluate a model performance on the final model(s), if we don’t measure performance until the test set?\nDuring the early days of neural networks, researchers realized that measuring performance by re-predicting the training set samples led to results that were overly optimistic (significantly, unrealistically so). This led to models that overfit, meaning that they performed very well on the training set but poorly on the test set.12 To combat this issue, a small validation set of data were held back and used to measure performance as the network was trained. Once the validation set error rate began to rise, the training would be halted. In other words, the validation set was a means to get a rough sense of how well the model performed prior to the test set.\nValidation sets are a special case of resampling methods that are used on the training set.\nExample:\n\nset.seed(52)\n# To put 60% into training, 20% in validation, and 20% in testing:\names_val_split &lt;- initial_validation_split(ames, prop = c(0.6, 0.2))\names_val_split\n\n&lt;Training/Validation/Testing/Total&gt;\n&lt;1758/586/586/2930&gt;\n\n\nTo get the training, validation, and testing data, the same syntax is used:\n\names_train &lt;- training(ames_val_split)\names_test &lt;- testing(ames_val_split)\names_val &lt;- validation(ames_val_split)"
  },
  {
    "objectID": "model_fit.html#linear-regression-model",
    "href": "model_fit.html#linear-regression-model",
    "title": "29  Fitting Models",
    "section": "29.1 Linear Regression Model",
    "text": "29.1 Linear Regression Model\nSuppose that a linear regression model was our initial choice. This is equivalent to specifying that the outcome data is numeric and that the predictors are related to the outcome in terms of simple slopes and intercepts:\n\\[\n\\begin{aligned}\ny_i = β_0 + β_1 {x_1}_i +...+ β_p {x_p}_i\n\\end{aligned}\n\\]\nA variety of methods can be used to estimate the model parameters:\n\nOrdinary linear regression uses the traditional method of least squares to solve for the model parameters.\nIn R, the stats package can be used for the first case. The syntax for linear regression using the function lm() is:\nmodel &lt;- lm(formula, data, ...)\nRegularized linear regression adds a penalty to the least squares method to encourage simplicity by removing predictors and/or shrinking their coefficients towards zero. This can be executed using Bayesian or non-Bayesian techniques.\nA Bayesian model can be fit using the rstanarm package:\nmodel &lt;- stan_glm(formula, data, family = \"gaussian\", ...)\nA popular non-Bayesian approach to regularized regression is the glmnet model. Its syntax is:\nmodel &lt;- glmnet(x = matrix, y = vector, family = \"gaussian\", ...)\n\nNote that to fit models across different packages, the data must be formatted in different ways. lm() and stan_glm() only have formula interfaces while glmnet() does not. For a person trying to do data analysis, these differences require the memorization of each package’s syntax and can be very frustrating.\nFor tidymodels, the approach to specifying a model is intended to be more unified:\n\nSpecify the type of model based on its mathematical structure (e.g., linear regression, random forest, KNN, etc).\nSpecify the engine for fitting the model. Most often this reflects the software package that should be used, like Stan or glmnet. These are models in their own right, and parsnip provides consistent interfaces by using these as engines for modeling.\nWhen required, declare the mode of the model. The mode reflects the type of prediction outcome. For numeric outcomes, the mode is regression; for qualitative outcomes, it is classification. If a model algorithm can only address one type of prediction outcome, such as linear regression, the mode is already set. For example, for the three cases we outlined:\n\n\nlibrary(tidymodels)\ntidymodels_prefer()\n\nlinear_reg() |&gt; set_engine(\"lm\") |&gt; translate()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nModel fit template:\nstats::lm(formula = missing_arg(), data = missing_arg(), weights = missing_arg())\n\nlinear_reg(penalty = 1) |&gt; set_engine(\"glmnet\") |&gt; translate()\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1\n\nComputational engine: glmnet \n\nModel fit template:\nglmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    family = \"gaussian\")\n\nlinear_reg() |&gt; set_engine(\"stan\") |&gt; translate()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: stan \n\nModel fit template:\nrstanarm::stan_glm(formula = missing_arg(), data = missing_arg(), \n    weights = missing_arg(), family = stats::gaussian, refresh = 0)\n\n\nNote that missing_arg() is just a placeholder for the data that has yet to be provided.\n\n29.1.1 Example\nLet’s walk through how to predict the sale price of houses in the Ames data as a function of only longitude and latitude:\n\nlibrary(tidymodels)\nlibrary(modeldata)\ndata(ames)\ntidymodels_prefer()\n\n\n# Set the random number stream so that the results can be \n# reproduced later\nset.seed(501)\n\names_log10 &lt;- \n  ames |&gt;\n    mutate(Sale_Price = log10(Sale_Price))\n\n# Save the split information for an 80/20 split of the data\names_split &lt;- initial_split(ames_log10, prop = 0.80)\names_train &lt;- training(ames_split)\names_test  &lt;- testing(ames_split)\n\n\nlm_model &lt;- \n  linear_reg() %&gt;% \n  set_engine(\"lm\")\n\nlm_form_fit &lt;- \n  lm_model %&gt;% \n  # Recall that Sale_Price has been pre-logged\n  fit(Sale_Price ~ Longitude + Latitude, data = ames_train)\n\nlm_xy_fit &lt;- \n  lm_model %&gt;% \n  fit_xy(\n    x = ames_train %&gt;% select(Longitude, Latitude),\n    y = ames_train %&gt;% pull(Sale_Price)\n  )\n\nlm_form_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Sale_Price ~ Longitude + Latitude, data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n   -310.757       -2.055        2.938  \n\nlm_xy_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n   -310.757       -2.055        2.938"
  },
  {
    "objectID": "model_fit.html#using-the-model-results",
    "href": "model_fit.html#using-the-model-results",
    "title": "29  Fitting Models",
    "section": "29.2 Using the Model Results",
    "text": "29.2 Using the Model Results\nOnce the model is created and fit, we can use the results in a variety of ways; we might want to plot, print, or otherwise examine the model output. Several quantities are stored in a parsnip model object, including the fitted model. This can be found in an element called fit, which can be returned using the extract_fit_engine() function:\nExample:\n\nlm_form_fit %&gt;% extract_fit_engine()\n\n\nCall:\nstats::lm(formula = Sale_Price ~ Longitude + Latitude, data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n   -310.757       -2.055        2.938  \n\n\nNormal methods can be applied to this object, such as printing and plotting:\nExample:\n\nlm_form_fit %&gt;% extract_fit_engine() %&gt;% vcov()\n\n            (Intercept)     Longitude      Latitude\n(Intercept)  219.777464  1.6820856038 -1.4812181412\nLongitude      1.682086  0.0176858915 -0.0006168762\nLatitude      -1.481218 -0.0006168762  0.0338638634\n\n\nThe summary() method for lm objects can be used to print the results of the model fit, including a table with parameter values, their uncertainty estimates, and p-values.\n\nmodel_res &lt;- \n  lm_form_fit %&gt;% \n  extract_fit_engine() %&gt;% \n  summary()\n\nThe model coefficient table is accessible via the coef method.\nExample:\n\nparam_est &lt;- coef(model_res)\nparam_est\n\n               Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) -310.756920 14.8248934 -20.96183 1.492498e-89\nLongitude     -2.055314  0.1329883 -15.45485 2.211671e-51\nLatitude       2.938288  0.1840214  15.96710 1.479971e-54\n\nclass(param_est)\n\n[1] \"matrix\" \"array\" \n\n\nA next step might be to create a visualization of the parameter values. To do this, it would be sensible to convert the parameter matrix to a data frame. We could add the row names as a column so that they can be used in a plot. However, notice that several of the existing matrix column names would not be valid R column names for ordinary data frames (e.g., “Pr(&gt;|t|)”).\nAs a solution, the broom package can convert many types of model objects to a tidy structure. For example, using the tidy() method on the linear model produces:\n\ntidy(lm_form_fit)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  -311.      14.8       -21.0 1.49e-89\n2 Longitude      -2.06     0.133     -15.5 2.21e-51\n3 Latitude        2.94     0.184      16.0 1.48e-54\n\n\nThe column names are standardized across models and do not contain any additional data (such as the type of statistical test)."
  },
  {
    "objectID": "model_fit.html#make-predictions",
    "href": "model_fit.html#make-predictions",
    "title": "29  Fitting Models",
    "section": "29.3 Make Predictions",
    "text": "29.3 Make Predictions\nFor predictions, parsnip always conforms to the following rules:\n\nThe results are always a tibble.\nThe column names of the tibble are always predictable.\nThere are always as many rows in the tibble as there are in the input data set.\n\nFor example, when numeric data are predicted:\nExample:\n\names_test_small &lt;- ames_test %&gt;% slice(1:5)\npredict(lm_form_fit, new_data = ames_test_small)\n\n# A tibble: 5 × 1\n  .pred\n  &lt;dbl&gt;\n1  5.28\n2  5.28\n3  5.28\n4  5.24\n5  5.24\n\n\nNote that some tidyverse and tidymodels arguments and return values contain periods. This is to protect against merging data with duplicate names. There are some data sets that contain predictors named “pred!”.\nThese three rules make it easier to merge predictions with the original data:\n\names_test_small %&gt;% \n  select(Sale_Price) %&gt;% \n  bind_cols(predict(lm_form_fit, ames_test_small)) %&gt;% \n  # Add 95% prediction intervals to the results:\n  bind_cols(predict(lm_form_fit, ames_test_small, type = \"pred_int\")) \n\n# A tibble: 5 × 4\n  Sale_Price .pred .pred_lower .pred_upper\n       &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1       5.33  5.28        4.96        5.60\n2       5.37  5.28        4.96        5.60\n3       5.25  5.28        4.96        5.60\n4       5.18  5.24        4.92        5.56\n5       5.10  5.24        4.92        5.56"
  },
  {
    "objectID": "model_random.html#always-remember-to-set-your-seed",
    "href": "model_random.html#always-remember-to-set-your-seed",
    "title": "30  Random Numbers",
    "section": "30.1 Always Remember to Set Your Seed!",
    "text": "30.1 Always Remember to Set Your Seed!\nWhen simulating any random numbers it is essential to set the random number seed. Setting the random number seed with set.seed() ensures reproducibility of the sequence of random numbers.\nExample:\n\nset.seed(1)\nrnorm(5)\n\n[1] -0.6264538  0.1836433 -0.8356286  1.5952808  0.3295078\n\n\nExample: Simulating a Linear Model\nSuppose we want to simulate from the following linear model \\(y=β_0+β_1x+ε\\) where \\(ε ∼ N(0, 22)\\)\nAssume \\(x ∼ N(0, 12)\\), \\(β_0 = 0.5\\) and \\(β_1 = 2\\). The variable x might represent an important predictor of the outcome y.\nHere’s how you could do that in R:\n\n## Always set your seed! \nset.seed(20) \n## Simulate predictor variable \nx &lt;- rnorm(100) \n## Simulate the error term \ne &lt;- rnorm(100, 0, 2) \n## Compute the outcome via the model \ny &lt;- 0.5 + 2 * x + e \nsummary(y)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-6.4084 -1.5402  0.6789  0.6893  2.9303  6.5052 \n\nplot(x, y)"
  },
  {
    "objectID": "model_random.html#random-sampling",
    "href": "model_random.html#random-sampling",
    "title": "30  Random Numbers",
    "section": "30.2 Random Sampling",
    "text": "30.2 Random Sampling\nThe sample() function draws randomly from a specified set of (scalar) objects allowing you to sample from arbitrary distributions of numbers.\nExample:\n\nset.seed(1)\nsample(1:10,4)\n\n[1] 9 4 7 1\n\n# Doesn't have to be numbers\nsample(letters, 5)\n\n[1] \"b\" \"w\" \"k\" \"n\" \"r\"\n\n\nTo sample more complicated things, such as rows from a dataframe or a list, you can sample the indices into an object rather than the elements of the object itself:\n\nset.seed(20)\nidx &lt;- seq_len(nrow(airquality))## Create index vector \nsamp &lt;- sample(idx, 6) ##Sample from the index vector \nairquality[samp, ]\n\n    Ozone Solar.R Wind Temp Month Day\n107    NA      64 11.5   79     8  15\n120    76     203  9.7   97     8  28\n130    20     252 10.9   80     9   7\n98     66      NA  4.6   87     8   6\n29     45     252 14.9   81     5  29\n45     NA     332 13.8   80     6  14"
  },
  {
    "objectID": "time_analysis.html#ts-analysis-workflow",
    "href": "time_analysis.html#ts-analysis-workflow",
    "title": "31  TS Analysis",
    "section": "31.1 TS Analysis Workflow",
    "text": "31.1 TS Analysis Workflow\nTime series analysis is the process of extracting meaningful insights from time series data with the use of data visualization tools, statistical applications, and mathematical models. Those insights can be used to learn and explore past events and to forecast future events.\nThe analysis process can be divided into the following steps:\n\nData collection: This step includes extracting data from different data sources, such as flat files (such as CSV, TXT, and XLMS), databases (for example, SQL Server, and Teradata), or other internet sources (such as academic resources and the Bureau of Statistics datasets).\nData preparation: In most cases, raw data is unstructured and may require cleaning, transformation, aggregation, and reformatting.\nDescriptive analysis: This is used in summary statistics and data visualization tools to extract insights from the data, such as patterns, distributions, cycles, and relationships with other drivers to learn more about past events.\nPredictive analysis: We use this to apply statistical methods in order to forecast future events, including forecasting strategies, forecasting with linear regression, forecasting with exponential smoothing models, forecasting with ARIMA models, and forecasting with machine learning models."
  },
  {
    "objectID": "time_analysis.html#ts-checklist",
    "href": "time_analysis.html#ts-checklist",
    "title": "31  TS Analysis",
    "section": "31.2 TS Checklist",
    "text": "31.2 TS Checklist\nThe following is a checklist of possible actions that the analyst needs to consider when facing a TS analysis. The checklist needs to be adapted to the particular problem under consideration and is not exhaustive:\n\nDo you understand the context? Have the ‘right’ variables been measured?\nHave all the time series been plotted?\nAre there any missing values? If so, what should be done about them?\nAre there any outliers? If so, what should be done about them?\nAre there any obvious discontinuities in the data? If so, what does this mean?\nDoes it make sense to transform any of the variables?\nIs trend present? If so, what should be done about it?\nIs seasonality present? If so, what should be done about it?"
  },
  {
    "objectID": "time_objects.html#the-ts-class",
    "href": "time_objects.html#the-ts-class",
    "title": "32  TS Objects",
    "section": "32.1 The ts class",
    "text": "32.1 The ts class\nThe ts class is R’s built-in format for a regular univariate time series object.\nA regular time series is defined as an ordered sequence of observations over time, which is captured at equally spaced time intervals (that is, every day, week, month, and so on). Whenever this condition ceases to exist, the series becomes an irregular time series. We are specifically going to focus on the analysis and the forecasting of regular time series data.\nThe main characteristics of regular time series data is as follows:\n\nCycle/period: A regular and repeating unit of time that split the series into consecutive and equally long subsets (for example, for monthly series, a full cycle would be a year).\nFrequency: Defines the length or the number of units of the cycle (for example, for quarterly series, the frequency is four).\nTimestamp: Provides the time each observation in the series was captured, and can be used as the series index. By definition, in a regular time series the time difference (or delta) between two consecutive observations must be equal.\n\nA ts object is composed of two elements:\n\nthe series values and,\nits corresponding timestamp.\n\nIn addition, it also has several attributes, such as the series, cycle, frequency, and the time interval between each consecutive observation.\nAfter loading new data into the environment, it is always recommended that you check and verify whether the data structure is aligned with your expectations. A fast and recommended check to start with would be to verify the data class and length, which can be done with the is.ts and length functions:\nExample:\n\nlibrary(TSstudio)\n\n# Test if the object is a \"ts\" class\nis.ts(USgas) \n\n[1] TRUE\n\n# Get the number of observations\nlength(USgas) \n\n[1] 238\n\n\nThe structure of the ts object is a bit different from most data structures in R (matrix, data.frame, data.table, tibble, and so on). While the ts object is a two-dimensional dataset (time/index and the series observations), it doesn’t share the common attributes of most of the regular tables in R (such as matrix or data.frame) since the series index is embedded within the object itself. Therefore, some of the common functions for R’s tables won’t work with a ts object (such as the dim function).\nA practical and concise method to get the characteristics of the series is with the frequency and deltat functions from the stats package, which provide the series frequency and the time interval between the observations:\nExample:\n\nfrequency(USgas)\n\n[1] 12\n\ndeltat(USgas)\n\n[1] 0.08333333\n\n\nOther useful utility functions are the start and end functions, which, as their names imply, return the series timestamp’s starting and ending point, respectively:\nExample:\n\nstart(USgas)\n\n[1] 2000    1\n\nend(USgas)\n\n[1] 2019   10\n\n\nThe ts_info function from the TSstudio package provides a concise summary of most of the preceding functions, including the object class, the number of observations, the frequency, and the starting and ending of the series.\nExample:\n\nts_info(USgas)\n\n The USgas series is a ts object with 1 variable and 238 observations\n Frequency: 12 \n Start time: 2000 1 \n End time: 2019 10"
  },
  {
    "objectID": "time_objects.html#creating-a-ts-object",
    "href": "time_objects.html#creating-a-ts-object",
    "title": "32  TS Objects",
    "section": "32.2 Creating a ts Object",
    "text": "32.2 Creating a ts Object\nThe ts function from the stats package allows you to create a ts object by assigning sequential observations and mapping their attributes.\nExample:\n\nlibrary(TSstudio)\nmy_ts1 &lt;- ts(data = 1:60, # The series values\n            start = c(2010, 1), # The time of the first observation\n            end = c(2014, 12), # The time of the last observation\n            frequency = 12) # The series frequency\n\n# Review the attributes of the new object\nts_info(my_ts1)\n\n The my_ts1 series is a ts object with 1 variable and 60 observations\n Frequency: 12 \n Start time: 2010 1 \n End time: 2014 12 \n\nmy_ts1\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n2010   1   2   3   4   5   6   7   8   9  10  11  12\n2011  13  14  15  16  17  18  19  20  21  22  23  24\n2012  25  26  27  28  29  30  31  32  33  34  35  36\n2013  37  38  39  40  41  42  43  44  45  46  47  48\n2014  49  50  51  52  53  54  55  56  57  58  59  60\n\n\nNote that when creating a new ts object it is sufficient to define either the start or end arguments, as the end or start value of the series can be derived from the length of the series and its frequency.\n\n32.2.1 Preprocessing steps (ts)\nIt is most likely that your raw data won’t come in ts format and some preprocessing steps may be required before transforming the input to a ts object. Those preprocessing steps may include the following:\n\nExporting the data from an external source, for example, an Excel or CSV file, SQL database, GitHub, and so on.\nReformatting the timestamp of the series or defining the series index.\nReordering the series according to its chronological order.\n\nExample:\n\ndata(US_indicators)\nstr(US_indicators)\n\n'data.frame':   528 obs. of  3 variables:\n $ Date             : Date, format: \"1976-01-31\" \"1976-02-29\" ...\n $ Vehicle Sales    : num  885 995 1244 1191 1203 ...\n $ Unemployment Rate: num  8.8 8.7 8.1 7.4 6.8 8 7.8 7.6 7.4 7.2 ...\n\n\n\nExtract the data. We are going to convert the vehicle sales series into a ts object, so we will extract the first two columns of the series and assign them to a new data frame named tvs, which denotes the total vehicle sales:\n\ntvs &lt;- US_indicators[, 1:2]\nstr(tvs)\n\n'data.frame':   528 obs. of  2 variables:\n $ Date         : Date, format: \"1976-01-31\" \"1976-02-29\" ...\n $ Vehicle Sales: num  885 995 1244 1191 1203 ...\n\n\nArrange the data. Before converting the data frame, or generally any other R table format (such as tibble and data.table) to a ts object, you will need to arrange the data in chronological order.\n\nlibrary(dplyr)\ntvs &lt;- tvs |&gt;\n        arrange(Date)\nhead(tvs)\n\n        Date Vehicle Sales\n1 1976-01-31         885.2\n2 1976-02-29         994.7\n3 1976-03-31        1243.6\n4 1976-04-30        1191.2\n5 1976-05-31        1203.2\n6 1976-06-30        1254.7\n\n\nSet the start (or end) of the series. Since the input series is monthly, we need to define the cycle (year) and frequency (month) units of the first observation of the series. This can be done manually, by observing the first date of the series and set the start point accordingly. In this case, the series started in January 1976, and therefore we can define it as start = c(1976, 1), or we can write a code to capture the starting point of the series.\n\nlibrary(lubridate)\nstart_point &lt;- c(year(min(tvs$Date)),\n                  month(min(tvs$Date)))\nstart_point\n\n[1] 1976    1\n\n\nDefine the series with the ts function. Utilize the start_point variable to define the beginning of the series:\n\ntvs_ts &lt;- ts(data = tvs$`Vehicle Sales`,\n              start = start_point,\n              frequency = 12)\nhead(tvs_ts)\n\n[1]  885.2  994.7 1243.6 1191.2 1203.2 1254.7"
  },
  {
    "objectID": "time_objects.html#multivariate-time-series-objects",
    "href": "time_objects.html#multivariate-time-series-objects",
    "title": "32  TS Objects",
    "section": "32.3 Multivariate time series objects",
    "text": "32.3 Multivariate time series objects\nIn some instances, such as correlation analysis, you may need to work with multivariate time series objects. In this case you can use the mts (multiple time series) class, an extension of the ts class.\nThe mts class is a combination of the ts and matrix classes, and its advantage is that it shares the attributes of both those classes.\nExample:\n\ndata(Coffee_Prices)\nts_info(Coffee_Prices)\n\n The Coffee_Prices series is a mts object with 2 variables and 701 observations\n Frequency: 12 \n Start time: 1960 1 \n End time: 2018 5 \n\nclass(Coffee_Prices)\n\n[1] \"mts\"    \"ts\"     \"matrix\"\n\nhead(Coffee_Prices)\n\n       Robusta Arabica\n[1,] 0.6968643  0.9409\n[2,] 0.6887074  0.9469\n[3,] 0.6887074  0.9281\n[4,] 0.6845187  0.9303\n[5,] 0.6906915  0.9200\n[6,] 0.6968643  0.9123\n\n\nYou can utilize and apply any designated function for a matrix object (when applicable) on an mts object. Similarly, the mts object also supports the ts objects functions, such as the frequency, time, start, or end functions.\nNote that two or more series can be merged to an mts object only if all the following prerequisites are met:\n\nAll inputs are regular time series.\nAll inputs have exactly the same frequency or time interval between the observations.\nAll inputs start at the same cycle number and cycle unit (unless missing values are filled with NAs).\n\n\n32.3.1 Creating a mts Object\nThe process of converting a mts series object from a data frame to ts format is fairly similar to that of the ts object.\nIn the following example we are going to use the US_indicators series:\nExample:\n\ndata(\"US_indicators\")\n\n\nOrder the series by chronological order. Make sure that the series is sorted in chronological order with the arrange function:\n\nUS_indicators &lt;- US_indicators |&gt;\n                    arrange(Date)\n\nCreate the ts object. Use the ts function to create the ts objects, where the input data includes two columns (as opposed to a single one for the ts object):\n\nUS_indicators_ts &lt;- ts(data = US_indicators[, c(\"Vehicle Sales\", \"Unemployment Rate\")],\n                        start = c(year(min(tvs$Date)), month(min(tvs$Date))),\n                        frequency = 12)\n\nReview the transformed object. Use the ts_info and head functions:\n\nts_info(US_indicators_ts)\n\n The US_indicators_ts series is a mts object with 2 variables and 528 observations\n Frequency: 12 \n Start time: 1976 1 \n End time: 2019 12 \n\nhead(US_indicators_ts)\n\n     Vehicle Sales Unemployment Rate\n[1,]         885.2               8.8\n[2,]         994.7               8.7\n[3,]        1243.6               8.1\n[4,]        1191.2               7.4\n[5,]        1203.2               6.8\n[6,]        1254.7               8.0\n\n\n\n\n\n32.3.2 Setting the frequency\nNote that setting the frequency impacts the structure of the ts object output.\nAs an example we will use the seq.Date method to create a sequence of 3,650 days (or 365 times 10, which is 10 years excluding the extra day during leap years) starting on January 1, 2010. We will then use the rnorm function to generate the corresponding values of the series following a normal distribution with a mean of 15 and a standard deviation (sd) of 2:\n\ndaily_df &lt;- data.frame(date = seq.Date(from = as.Date(\"2010-01-01\"),\n                                       length.out = 365 * 10, \n                                       by = \"day\"),\n                       y = rnorm(365 * 10, mean = 15, sd = 2))\n\nstr(daily_df)\n\n'data.frame':   3650 obs. of  2 variables:\n $ date: Date, format: \"2010-01-01\" \"2010-01-02\" ...\n $ y   : num  13.1 13.3 16.9 13.6 15.9 ...\n\n\nWe will assign the first date of the series to a variable (start_date) and use it to set the start point of the series:\n\nstart_date &lt;- min(daily_df$date)\n\nFor a daily series with weekly cycles, the frequency of the series should be set to seven (for example, Monday to Sunday) as can be seen in the following example:\n\ndays_week_ts &lt;- ts(daily_df$y, \n                   start = c(1, wday(start_date)),\n                   frequency = 7)\nts_info(days_week_ts)\n\n The days_week_ts series is a ts object with 1 variable and 3650 observations\n Frequency: 7 \n Start time: 1 6 \n End time: 523 1"
  },
  {
    "objectID": "time_objects.html#data-manipulation-of-ts-objects",
    "href": "time_objects.html#data-manipulation-of-ts-objects",
    "title": "32  TS Objects",
    "section": "32.4 Data Manipulation of ts Objects",
    "text": "32.4 Data Manipulation of ts Objects\nOnce the series is transformed into a ts object, it might still be necessary to continue applying some additional transformation or preprocessing steps. These can include steps such as extracting or subsetting a specific element of the series or aggregating the series to a different frequency (for example, from monthly to quarterly).\nNote that due to the unique structure of the ts object, in most cases, the common extraction methods for data.frame do not apply to ts objects.\n\n32.4.1 The window function (stats)\nThe main purpose of the window function is to subset a ts object based on a time range.\nExample:\n\n# Extract all the observations of the year 2005\nwindow(USgas, start = c(2005,1), end = c(2005, 12))\n\n        Jan    Feb    Mar    Apr    May    Jun    Jul    Aug    Sep    Oct\n2005 2561.9 2243.0 2205.8 1724.9 1522.6 1534.1 1686.6 1695.1 1422.5 1428.2\n        Nov    Dec\n2005 1663.4 2326.4\n\n\nSimilarly, we can extract a specific frequency unit from the series. For example, let’s assume that we are interested in extracting all the observations of the series that occurred in the third quarter of the year.\nExample:\n\n# Extract observations from the quarters of the year 2005\nwindow(USgas, start = c(2005,1), end = c(2005, 12), frequency = 3)\n\nTime Series:\nStart = c(2005, 1) \nEnd = c(2005, 3) \nFrequency = 3 \n[1] 2561.9 1522.6 1422.5\n\n\n\n\n32.4.2 Aggregate ts objects (stats)\nThe aggregate function is a generic function for aggregating ts and data.frame objects. This splits the data into subsets, computes specific summary statistics (based on the user’s choice), and then aggregates the results to a ts or data.frame object (depending on the input type).\nExample:\n\n# Transform the USVSales series from a monthly frequency to quarterly\nUSVSales_q &lt;- aggregate(USVSales, nfrequency = 3, FUN = \"sum\")\nts_info(USVSales_q)\n\n The USVSales_q series is a ts object with 1 variable and 132 observations\n Frequency: 3 \n Start time: 1976 1 \n End time: 2019 3 \n\n\nHandling missing values, if these exist, can be done by using the na.action argument of the aggregate function, which, by default, ignores missing values.\n\n\n32.4.3 Creating lags and leads for ts objects (stats)\nThe use of lags in time series analysis is widespread because, typically, a time series is correlated with some of its previous lags.\nWe can generally distinguish between two types of lags:\n\nPast lags, or simply lags, represent a shift in the series by \\(n\\) steps back, with respect to the original series. For a series with \\(t\\) observations, the \\(n\\) lag of the series begins at time \\(n+1\\) and ends at time \\(t+n\\) (where the first \\(n\\) observations are missing).\nExample:\n\n# k Sets the number of lags (or leads) to be created\nUSVSales_lag4 &lt;- stats::lag(USVSales, k = -4) \n\nLead (or negative lags) represent a shift in the series by \\(n\\) steps forward, with respect to the original series. In this case, for a series with \\(t\\) observations, the lead \\(n\\) of the series begins at the time \\(n\\) and end at time \\(t-n\\) (where the last \\(n\\) observations are missing).\n\nUSVSales_lead1 &lt;- stats::lag(USVSales, k = 1)"
  },
  {
    "objectID": "time_objects.html#visualizing-time-series-objects",
    "href": "time_objects.html#visualizing-time-series-objects",
    "title": "32  TS Objects",
    "section": "32.5 Visualizing Time Series Objects",
    "text": "32.5 Visualizing Time Series Objects\nThere are two approaches for visualizing a time series object:\n\nDirect: This approach uses a visualization function to plot the object without any data transformation or conversion to another class. There are few packages that provide direct tools for visualizing time series objects:\n\nstats: In addition to the ts objects, this provides the plot.ts function for visualizing time series objects. This function is an extension of the plot function, which is an R built-in visualization function.\ndygraphs: This R package is an interface for the dygraphs JavaScript visualization library. It provides an interactive application for visualizing ts objects.\nTSstudio: This is a designated package for descriptive and predictive analysis of time series data. This includes rich and interactive visualization applications for time series objects, such as ts, mts, zoo, xts, and other table-based formats (such as data.frame and tibble). Like the previous package, the TSstudio package uses the plotly package engine.\n\nIn-direct: This approach involves applying some data transformation steps to restructure the data as a numeric two-dimensional structure (values over time). This includes the use of packages such as ggplot2, plotly, highcharter, and rbokeh.\n\nThe main difference between the direct and indirect approaches is that the former was designed to work with the ts objects and it therefore automatically transfers the value of the series and timestamp onto a y-axis and x-axis respectively. On the other hand, in the indirect approach, you will have to define those two dimensions (values vs. time) manually.\n\n32.5.1 The plot.ts function\nThe plot.ts function from the stats package is built-in R function for visualizing of a ts object. Therefore, most of the arguments of the plot function (such as title and labeling options) can be used with the plot.ts function.\nExample:\n\nplot.ts(tvs_ts,\n        main = \"US Monthly Total Vehicle Sales\",\n        ylab = \"Thousands of Vehicle\",\n        xlab = \"Time\")\n\n\n\n\nSimilarly, we can use the function to plot an mts object:\nExample:\n\nplot.ts(US_indicators_ts,\n        plot.type = \"multiple\",\n        main = \"US Monthly Vehicle Sales vs. Unemployment Rate\",\n        xlab = \"Time\")\n\n\n\n\n\n\n32.5.2 The dygraphs package\nThe dygraphs package is an R interface to the dygraphs JavaScript charting library, and it is completely dedicated to visualizing time series objects, including the ts class. It is highly customized, interactive, and supports HTML implementation (for example, the rmarkdown and Shiny packages).\nExxample:\n\nlibrary(dygraphs)\ndygraph(tvs_ts,\n        main = \"US Monthly Total Vehicle Sales\",\n        ylab = \"Thousands of Vehicle\") %&gt;%\n  dyRangeSelector()\n\n\n\n\n\nFor multiple objects, we will use the US_indicators_ts series, adding a second y-axis, which allows us to plot and compare the two series that are not on the same scale (for example, thousands of units versus percentage):\nExample:\n\ndygraph(US_indicators_ts,\n        main = \"US Monthly Vehicle Sales vs. Unemployment Rate\") %&gt;%\n  dyAxis(\"y\", label = \"Vehicle Sales\") %&gt;%\n  dyAxis(\"y2\", label = \"Unemployment Rate\") %&gt;%\n  dySeries(\"Vehicle Sales\", axis = 'y', color = \"green\") %&gt;%\n  dySeries(\"Unemployment Rate\", axis = 'y2', color = \"red\") %&gt;%\n  dyLegend(width = 400)\n\n\n\n\n\nLooking at the plot of the US_indicator_ts series when using two y-axes, you can see that it is easier to identify the relationship between the two series as opposed to plotting them on a separate plot (as we did with the plot.ts function earlier).\nThe dygraph uses an object-oriented approach, which allows you to add a different component to the plot with the pipe operator (%&gt;%). This allows the user to add additional arguments for the plot, as opposed to a large amount of arguments in one function.\n\n\n32.5.3 TSstudio package\nThe TSstudio package provides the ts_plot function for visualizing time series objects using the plotly package visualization engine. In addition, this function supports both time series objects, such as ts, mts, zoo, and xts, and also data frame types such as data.frame, data.table, and tibble.\nExample:\n\nlibrary(TSstudio)\nts_plot(tvs_ts,\n        title = \"US Monthly Total Vehicle Sales\",\n        Ytitle = \"Thousands of Vehicle\",\n        slider = TRUE # Add an interactive slider for the x-axis\n)\n\n\n\n\n\nSimilarly, the ts_plot function supports mts objects:\nExample:\n\nts_plot(US_indicators_ts,\n        title = \"US Monthly Vehicle Sales vs. Unemployment Rate\",\n        type = \"multiple\")"
  },
  {
    "objectID": "time_objects.html#transformations",
    "href": "time_objects.html#transformations",
    "title": "32  TS Objects",
    "section": "32.6 Transformations",
    "text": "32.6 Transformations\nPlotting the data may suggest that it is sensible to consider transforming them, for example, by taking logarithms or square roots. The three main reasons for making a transformation are as follows:\n\nTo stabilize the variance. If there is a trend in the series and the variance appears to increase with the mean, then it may be advisable to transform the data. In particular, if the standard deviation is directly proportional to the mean, a logarithmic transformation is indicated. On the other hand, if the variance changes through time without a trend being present, then a transformation will not help. Instead, a model that allows for changing variance should be considered.\nTo make the seasonal effect additive. If there is a trend in the series and the size of the seasonal effect appears to increase with the mean, then it may be advisable to transform the data so as to make the seasonal effect constant from year to year. The seasonal effect is then said to be additive. In particular, if the size of the seasonal effect is directly proportional to the mean, then the seasonal effect is said to be multiplicative and a logarithmic transformation is appropriate to make the effect additive.\nTo make the data normally distributed. Model building and forecasting are usually carried out on the assumption that the data are normally distributed. In practice this is not necessarily the case. This effect can be difficult to eliminate with a transformation and it may be necessary to model the data using a different ‘error’ distribution."
  },
  {
    "objectID": "time_components.html#the-cycle-component",
    "href": "time_components.html#the-cycle-component",
    "title": "33  TS Components",
    "section": "33.1 The Cycle Component",
    "text": "33.1 The Cycle Component\nA cycle can be described as a sequence of repeatable events over time, where the starting point of a cycle is at a local minimum of the series and the ending point is at the next one, and the ending point of one cycle is the starting point of the following cycle. Unlike the seasonal pattern, cycles do not necessarily occur at equally spaced time intervals, and their length could change from cycle to cycle.\nExample:\n\nlibrary(TSstudio)\ndata(USUnRate)\nts_info(USUnRate)\n\n The USUnRate series is a ts object with 1 variable and 864 observations\n Frequency: 12 \n Start time: 1948 1 \n End time: 2019 12 \n\nunemployment &lt;- window(USUnRate, start = c(1990,1))\nts_plot(unemployment,\n    title = \"US Monthly Unemployment Rate\",\n    Ytitle = \"Unemployment Rate (%)\",\n    Xtitle = \"Year\",\n    Xgrid = TRUE,\n    Ygrid = TRUE)\n\n\n\n\n\nLooking at the preceding series plot, you can observe that the series has had three cycles since 1990 of different length:\n\nThe first cycle occurred between 1990 and 2000, which was close to an 11-year cycle.\nThe second cycle started in 2000 and ended in 2007, which was a 7-year cycle.\nA third cycle, which began in 2007 and as of May 2019 has not been completed yet, which means that this has continued for more than 12 years."
  },
  {
    "objectID": "time_components.html#the-trend-component",
    "href": "time_components.html#the-trend-component",
    "title": "33  TS Components",
    "section": "33.2 The Trend Component",
    "text": "33.2 The Trend Component\nA trend, if it exists in time series data, represents the general direction of the series, either up or down, over time. Furthermore, a trend could have either linear or exponential growth (or close to either one), depending on the series characteristics.\nExample:\n\n\n\n\n\n\nThese examples represent time series data with a clear trend component, and it is therefore easy to identify the trend and classify its growth type."
  },
  {
    "objectID": "time_components.html#the-seasonal-component",
    "href": "time_components.html#the-seasonal-component",
    "title": "33  TS Components",
    "section": "33.3 The Seasonal Component",
    "text": "33.3 The Seasonal Component\nThe seasonal component (or seasonality) is another common pattern in time series data. If it exists, it represents a repeated variation in the series, which is related to the frequency units of the series (for example, the months of the year for a monthly series).\nA common examples for a series with a strong seasonality pattern is the demand for electricity or natural gas. In those cases, the seasonal pattern is derived from a variety of seasonal events, such as weather patterns, the season of the year, and sunlight hours.\nExample:\n\n\n\n\n\n\nThe seasonal and cycle components both describe cyclic events over time, where the length of their cycle distinguish the two. The seasonal component has a constant cycle, which is derived and tied to the series frequency. On the other hand, the cycle length of the cycle component is not necessarily constant and can typically vary from one cycle to the next one.\nA common way to identify whether a cycle pattern exists in a series is with the use of a heatmap for the time series data.\nExample:\n\nts_heatmap(USgas,\n    title = \"Heatmap - the US Natural Gas Consumption\")"
  },
  {
    "objectID": "time_components.html#white-noise",
    "href": "time_components.html#white-noise",
    "title": "33  TS Components",
    "section": "33.4 White Noise",
    "text": "33.4 White Noise\nA series is defined as white noise when there is no correlation between the series observations or patterns. In other words, the relationship between different observations is random. Typically, unless mentioned otherwise, we assume that white noise is an independent and identically distributed random.\nExample:\n\n\n\n\n\n\nThere are a few methods for testing whether a time series is white noise:\n\nThe basic method is carried out by plotting and eyeballing the series to identify whether the variation of the series appears to be random or not.\nWe can measure the correlation between the series and its lags with the autocorrelation function (ACF). A series is considered to be white noise whenever there is no correlation between the series and its lag. The acf function from the stats package calculates the level of correlation between a series and its lags.\nThe Ljung-Box test is another statistical test to evaluate whether the series is correlated with its lags. In this case, the null hypothesis assumes that the lags are not correlated. Therefore, lags with lower p-values (with respect to the level of significance of the test) would be considered as being correlated with the series. The Box.testfunction, another stats package function, performs a Ljung-Box test on a series and a specific lag.\nExample:\n\nlibrary(dplyr)\nx &lt;- lapply(1:24, function(i){\n          p &lt;- Box.test(white_noise, lag = i, type = \"Ljung-Box\")\n          output &lt;- data.frame(lag = i, p_value = p$p.value)\n          return(output) }) %&gt;% bind_rows\nplot(x = x$lag,\n    y = x$p_value, ylim = c(0,1),\n    main = \"Series white_noise - Ljung-Box Test\",\n    xlab = \"Lag\", ylab = \"P-Value\")\nabline(h = 0.05, col=\"red\", lwd=3, lty=2)\n\n\n\n\nYou can see in the preceding Ljung-Box test summary that the p-value of all the lags is above the red dotted line, which indicates that we failed to reject the null hypothesis for a level of significance of 0.05. This indicates that the series is not correlated with its first 24 lags and is, therefore, a white noise series."
  },
  {
    "objectID": "time_components.html#the-irregular-component",
    "href": "time_components.html#the-irregular-component",
    "title": "33  TS Components",
    "section": "33.5 The Irregular Component",
    "text": "33.5 The Irregular Component\nThis component is the remainder between the series and structural components, and provides an indication of irregular events in the series. This includes non-systematic patterns or events in the data, which cause irregular fluctuation. In addition, the irregular component could provide some indication of the appropriate fit of the other components when using a decomposing method.\nA high correlation in this component is an indication that some patterns related to one of the other components were leftover due to an inaccurate estimate. On the other hand, if the irregular component is not correlated with its lags (that is, a white noise), we can assume (depending on the series structure) that the estimation of the trend and seasonal components captured the majority of the information about the series structure."
  },
  {
    "objectID": "time_components.html#additive",
    "href": "time_components.html#additive",
    "title": "33  TS Components",
    "section": "33.6 The Additive versus the Multiplicative Model",
    "text": "33.6 The Additive versus the Multiplicative Model\nThese terms describe the model structure. A model is defined as additive whenever we add together its components, namely, whenever there is a growth in the trend (with respect to the previous period), or if the amplitude of the seasonal component roughly remains the same over time:\n\\[\\begin{align*}\nY_{t}=T_{t}+S_{t}+C_{t}+I_{t}\n\\end{align*}\\]\nExample:\nThe US monthly natural gas consumption series is an example of an additive series. You can easily notice that the amplitude of the seasonal component remains the same (or close to the same) over time:\n\n\n\n\n\n\nAs you can see, the amplitude of the USgas series seasonal component over the past 20 years did not change by much (apart from some years, which may be related to some unusual weather patterns). In addition, the series trend seems to be linear, with some structural breaks during 2010.\nSimilarly, a model is defined as multiplicative whenever we multiply its components, namely, whenever the growth of the trend or the magnitude of the seasonal component increases or decreases by some multiplicity from period to period over time:\n\\[\\begin{align*}\nY_{t}=T_{t} \\times S_{t} \\times C_{t} \\times I_{t}\n\\end{align*}\\]\nExample:\nThe AirPassengers dataset (available in the dataset package), which describes the total monthly international airline passengers between the years 1949 and 1960, is an example for multiplicative series. As you can see in the following data, the amplitude of the seasonal component increases from year to year:\n\n\n\n\n\n\nThe typical approach for handling a series with a multiplicative structure is by applying a data transformation on the input series. The most common data transformation approaches for time series data are the following:\n\nLog transformation: This applies a log on both sides of the series equation. The new structure of the series allows us to treat it as a normal additive series:\n\n\\[\\begin{align*}\nlog(Y_{t})=log(T_{t}) + log(S_{t}) + log(C_{t}) + log(I_{t})\n\\end{align*}\\]\n\nBox-Cox transformation: This is based on applying power on the input series using the following formula:\n\n\\[\\begin{align*}\nY^{'}_{t}|\\lambda =\n  \\begin{cases}\n      \\frac{Y^{\\lambda}_{t} -1}{\\lambda} & \\lambda \\neq0 \\\\\n      log(Y_{t})& \\lambda=0\n  \\end{cases}\n\\end{align*}\\]\nAs you can see from the preceding Box-Cox equation, for \\(\\lambda=0\\), the transformation is a log transformation.\nThe forecast package provides several tools for applying a Box-Cox transformation on time series data. The BoxCox.lambda function estimates the value of \\(\\lambda\\), which minimizes the coefficient variation of the input series.\nExample:\nWe will use BoxCox.lambda to identify the \\(\\lambda\\) value for the AirPassenger series:\n\nlibrary(forecast)\nAirPassenger_lambda &lt;- BoxCox.lambda(AirPassengers)\nAirPassenger_lambda\n\n[1] -0.2947156\n\n\nWe can then use this \\(\\lambda\\) value to transform the input series with the BoxCox function and lot it with the ts_plot function:\nExample:\n\nAirPassenger_transform &lt;- BoxCox(AirPassengers, lambda =\n                      AirPassenger_lambda)\nts_plot(AirPassenger_transform,\n      title = \"Monthly Airline Passenger Numbers 1949-1960 with Box-Cox Transformation\",\n      Ytitle = \"Number of Passengers - Scaled\",\n      Xtitle = \"Years\",\n      Xgrid = TRUE,\n      Ygrid = TRUE)\n\n\n\n\n\nAs you can see from the transformation plot of the AirPassenger series, the values of the series are scaled. Note that most of the forecasting models in the forecast package automatically transform the series before applying the model, and then re-transform the forecast output back to the original scale."
  },
  {
    "objectID": "time_decomposition.html#the-moving-average-function",
    "href": "time_decomposition.html#the-moving-average-function",
    "title": "34  Decomposition of TS",
    "section": "34.1 The Moving Average Function",
    "text": "34.1 The Moving Average Function\nThe moving average (MA) is a simple function for smoothing time series data. This function is based on averaging each observation of a series, when applicable, with its surrounding observations, that is, with a past, future, or a combination of both past and future observations, in chronological order.\nThe output of this transformation process is a smoothed version of the original series.\nThe main components of the MA function are as follows:\n\nThe rolling window: This is a generic function that slides across the series in chronological order to extract sequential subsets\nAverage function: This is either a simple or weighted average, which is applied on each subset of the rolling window function.\n\n\n34.1.1 The rolling window function\nThe structure of the rolling window function defines the sub-setting method of series observations, thus playing a pivotal role in the smoothing of time series data process.\nThe most common types of window structures are as follows:\n\nThe one-sided window: This is a sliding window with a width of \\(n\\), which groups each observation of the series (when applicable) with its past consecutive \\(n-1\\) observations.\n\n\n\n\nFigure 11. Smoothing time series with moving average with one-sided rolling window\n\n\nThe output of the MA function (marked in green) is missing the first two observations (marked in red), which are considered to be the cost of the transformation process when using a one-sided rolling window.\n\nThe two-sided window: This is a rolling window with a width of \\(n\\), which groups each observation of the series (when applicable) with its past \\(n_1\\) and future \\(n_2\\) observations. If n is an odd number and \\(n_1\\) is equal to \\(n_2\\), we can define the two-sided rolling window as centered.\n\n\n\n\nFigure 12. Smoothing time series with moving average with two-sided rolling window\n\n\nThe window groups each observation with its preceding and leading observations. In this case, you cannot transform the first and last observations of the series (marked in red).\n\n\n34.1.2 The average function\nThere are two types of averaging methods:\n\nThe arithmetic (or simple) average: This is the most common and basic method for averaging a sequence of observations. It is based on summing all the observations and dividing them by the number of observations.\n\\[\\overline{Y} = \\frac{\\sum_{i=1}^{n}Y_i}{n}\\]\nThe weighted average: This method is based on applying weight to each observation of the series.\n\\[\\overline{Y_t} = {\\sum_{i=1}^{n}w_i Y_i}\\]\n\\(w_i\\) represents the corresponding weight of the \\(i\\) observation of \\(Y\\). A weighted average should be used with time series data when there is a systematic relationship between some observations in the series based on their timestamp distance.\n\n\n\n34.1.3 Main applications of the MA function\nThe MA function has a variety of applications, such as data smoothing, noise reduction, and trend estimation. Also, with some small modifications, this function can be used as a forecasting model.\n\nNoise reduction: The use of the MA method creates a smoothing effect that reduces the series variation, smoothing the random noise and outliers.\nDe-seasonalize: In addition to the noise reduction, MAs can be used to remove the seasonal component (if any). This process has a pivotal role in the classical decomposing process, as it provides an estimation for the trend component, which is then utilized to estimate the seasonal component.\nForecasting: With some small modifications, the MA function can be used to forecast the future observations of the series by averaging the past observations to estimate future values of the series. The averaging method varies from a simple average to more advanced averaging methods.\n\n\n\n34.1.4 Calculating MAs\nTo calculate different types of MAs we will use the ts_ma function from the TSstudio package, but you can be sure that there is more than one package in R that provides an MA transformation function.\nThe ts_ma function from the TSstudio package allows us to generate and plot multiple MA outputs simultaneously, using different orders and methods (SMA, two-sided MA, and so on). The main parameters of this function are as follows:\n\nn: This sets the length of the past and future observations to be used in a two-sided MA function. For example, if n is set to three, the window function will group each observation with its past and future three consecutive observations, which will yield a 7 order MA. In addition, it is possible to set this parameter with multiple values in order to generate multiple two-sided MA functions simultaneously using a different order.\nn_left/n_right: These are used to customize the MA function by setting the length of the past (n_left) manually and/or the future (n_right) side of the window function. If both parameters are defined, the output is a two-sided MA function, either centered or uncentered. If only one of those parameters is set, the function output is a one-sided MA.\n\nExample:\n\nlibrary(TSstudio)\ndata(USVSales)\nts_info(USVSales)\n\n The USVSales series is a ts object with 1 variable and 528 observations\n Frequency: 12 \n Start time: 1976 1 \n End time: 2019 12 \n\n\n\nts_plot(USVSales,\n        title = \"US MOnthly Total Vehicle Sales\",\n        Ytitle = \"Thousands of Units\",\n        Xtitle = \"Years\",\n        Xgrid = TRUE,\n        Ygrid = TRUE)\n\n\n\n\n\n\ntwo_sided_ma &lt;- ts_ma(ts.obj = USVSales,\n                      n = c(2,5), # Sets an order 5 and 11 moving average\n                      n_left = 6, n_right = 5, # Sets an order 12 moving average\n                      plot = TRUE,\n                      multiple = TRUE,\n                      margin = 0.04)\n\n\n\n\nFigure 12. Three versions of two-sided MA outputs\n\n\nNote that the higher the order of the function, the smoother the output. Of course, the smoothing effect comes at the cost of losing observations from both the start and end of the series. The loss of observations is a function of the order of the function, since the higher the order of the MA function, the higher the loss of observations.\n\n\n34.1.5 One-Sided vs Two-Sided MAs\nThe selection of a specific type of MA method depends on the aim of the transformation. Moreover, the use of a different range of order may change the outcome significantly. Generally, when applying both simple and two-sided MAs with arithmetic average using the same order, the output of both methods will be identical but lagged.\nThe simple moving average (SMA) (one-sided MA) is one of the common MA functions, and, as the name implies, it is also one of the simplest MA functions. This transformation method is based on applying an arithmetic average on a one-sided rolling window. Hence, the rolling window groups each observation in the series (when applicable) with its previous \\(n\\) consecutive observations in order to calculate their arithmetic average.\nThe two-sided MA method is based on a two-sided rolling window function, which groups each observation of the series with the past \\(n_1\\) and future \\(n_2\\) consecutive observations (as opposed to the SMA or one-sided MA, which use only past \\(n-1\\) observations). The term two-sided MA refers to the use of the two-sided window function with an arithmetic average.\nThe output of the two-sided MA function could be either of the following:\n\nCentered: This is when \\(n_1\\) and \\(n_2\\) are equal, which ensures that the function output at time t is centered around the t observation of the original y series.\nUn-centered: This is when the length of \\(n_1\\) is different from the length of \\(n_2\\), or the order of the MA is an even number.\n\nExample:\n\n# Create one-sided and two-sided MA with an order of 12\none_sided_12 &lt;- ts_ma(USVSales, \n                      n = NULL, \n                      n_left = 11, \n                      plot = FALSE)\ntwo_sided_12 &lt;- ts_ma(USVSales, \n                      n = NULL, \n                      n_left = 6, \n                      n_right = 5,\n                      plot = FALSE)\none_sided &lt;- one_sided_12$unbalanced_ma_12\ntwo_sided &lt;- two_sided_12$unbalanced_ma_12\n\nWe will now bind the output of the one-sided and two-sided MA functions with the USVSales series and plot it with the ts_plot function:\n\nma &lt;- cbind(USVSales, one_sided, two_sided)\np &lt;- ts_plot(ma,\n          Xgrid = TRUE,\n          Ygrid = TRUE,\n          type = \"single\",\n          title = \"One-Sided vs. Two-Sided Moving Average - Order 12\")\n\nNext, we will use the layout function from the plotly package to set the plot legend and labels:\n\nlibrary(plotly)\np %&gt;% layout(legend = list(x = 0.05, y = 0.95),\n            yaxis = list(title = \"Thousands of Units\"),\n            xaxis = list(title = \"Year\"))\n\n\n\n\n\nConclusion:\n\nIt would make sense to use the one-sided MA function when you need to have the most recent observations (as the loss of observations, in this case, is from the beginning of the series).\nThe use of a centered two-sided MA function (or close to centered when the order of the function is even) is generally more appropriate to apply as a smoother or data filter method. As you can see, the output of the two-sided MA function fits better with the overall changes in the series trend with respect to the one-sided function output. This is mainly due to the fact that there is a delay of five periods between the two methods. This comes at the cost of losing the series’ last 5 observations in addition to the first 6 (as opposed to the last of the first 11 observations of the one-sided function). This makes the use of the two-sided function more expensive when you care about the most recent observations of the series."
  },
  {
    "objectID": "time_decomposition.html#classical-seasonal-decomposition",
    "href": "time_decomposition.html#classical-seasonal-decomposition",
    "title": "34  Decomposition of TS",
    "section": "34.2 Classical Seasonal Decomposition",
    "text": "34.2 Classical Seasonal Decomposition\nClassical decomposition (or classical seasonal decomposition by MA) is one of the most common methods of decomposing a time series down to its components. This method for estimating the three components is based on the use of an MA function followed by simple arithmetic calculations.\nThis is a three-step process, where each step is dedicated to the estimation of one of the components in sequential order (hence, the calculation of each component is derived from the estimate of the previous component):\n\nTrend estimation: This is the first step of the decomposing process, by using the MA function to remove the seasonal component from the series. The order of the MA function is defined by the frequency of the series. For instance, if the frequency of the input series is monthly (or 12), then the order of the MA should be set to 12. Since we are using a two-sided MA, some of the first and last observations of the trend estimation will be missing.\nSeasonal component estimation: A two-step process, starting with de-trending the series by subtracting the trend estimation from the previous step from the series, where:\n\nYou can use \\(Y_t - {\\hat T_t}\\) when using the additive model, and \\(\\frac {Y_t}{\\hat T_t}\\) when the model is multiplicative. Here, \\(Y_t\\) is the original series observation at time \\(t\\) and \\(\\hat {T_t}\\) is the corresponding trend estimation.\nAfter the series is detrended, the next step is to estimate the corresponding seasonal component for each frequency unit (for example, for a monthly series, the seasonal component for January, February, and so on). This calculation is done by grouping the observations by their frequency unit and then averaging each group. The output of this process is a new series with a length that is equal to the series frequency and is ordered accordingly. This series represents the seasonal component of each frequency unit, so this estimation is one-to-many (one estimation for multiple observations).\n\nIrregular component estimation: This is about subtracting the estimation of the trend and seasonal components from the original series. This is \\(\\hat{I_t} = {Y_t} - \\hat{T_t} - \\hat{S_t}\\) for an additive series, and \\(\\hat{I_t} = \\frac {Y_t}{({\\hat T_t} \\times {\\hat S_t})}\\) for a multiplicative series. Here, \\(Y_t\\) represents the original series at time \\(t\\), and \\(\\hat{T_t}\\), \\(\\hat{S_t}\\), and \\(\\hat{I_t}\\) represent the corresponding trend and seasonal and irregular components estimate.\n\nThe decompose function from the stats package implements this method. By default, this function is set to an additive model.\nExample:\n\ndata(USVSales)\nusv_decompose &lt;- decompose(USVSales)\nstr(usv_decompose)\n\nList of 6\n $ x       : Time-Series [1:528] from 1976 to 2020: 885 995 1244 1191 1203 ...\n $ seasonal: Time-Series [1:528] from 1976 to 2020: -225.2 -102.4 143 34.5 147.9 ...\n $ trend   : Time-Series [1:528] from 1976 to 2020: NA NA NA NA NA ...\n $ random  : Time-Series [1:528] from 1976 to 2020: NA NA NA NA NA ...\n $ figure  : num [1:12] -225.2 -102.4 143 34.5 147.9 ...\n $ type    : chr \"additive\"\n - attr(*, \"class\")= chr \"decomposed.ts\"\n\nclass(usv_decompose)\n\n[1] \"decomposed.ts\"\n\n\nFrom the preceding output you can see that the function returns a list of six objects:\n\nx: This is the original series, a ts object.\nseasonal: This is the estimate of the seasonal component, a ts object.\ntrend: This is the estimate of the series trend, a ts object. You can see that the first (and also the last) observations are missing due to the use of the two-sided MA function. The number of missing values is defined by the order of the MA function.\nrandom: This is the estimate of the irregular component, a ts object. This output is nothing but the remainder of the series and the preceding two components. The random object is missing whenever the trend estimation is missing.\nfigure: This is the estimated seasonal figure only.\ntype: This is the type of decomposition, either an additive (the default), or multiplicative model.\n\nNow we can plot the output of the decompose function as plotsupports the decomposed.ts class.\nExample:\n\nplot(usv_decompose)\n\n\n\n\nSimilarly, if the series has multiplicative growth, like the AirPassengers series, you can set the decomposition process with a multiplicative model:\nExample:\n\nair_decompose &lt;- decompose(AirPassengers, type = \"multiplicative\")\nplot(air_decompose)\n\n\n\n\nOne of the downsides of the classical decomposition method is that seasonal component estimation is based on the arithmetic average, which results in a one-to-many estimation, so there is a single seasonal component estimation for each cycle unit (for example, all observations of the series that occurred in January will have the same seasonal component estimation if the series is monthly). This is not problematic when applying this method to an additive series, such as the US vehicle sales or the monthly natural gas consumption datasets, as the magnitude of the seasonal oscillation remains the same (or close to the same) over time. However, this is not the case for a multiplicative series, as the magnitude of the seasonal oscillation grows over time."
  },
  {
    "objectID": "time_seasonality.html#type-of-seasonality",
    "href": "time_seasonality.html#type-of-seasonality",
    "title": "35  Seasonality Analysis",
    "section": "35.1 Type of Seasonality",
    "text": "35.1 Type of Seasonality\nWhen seasonality exists in the time series data, we can classify it into one of the following categories:\n\nSingle seasonal pattern: Whenever there is only one dominant seasonal pattern in the series. As the frequency of the series is lower (for example, monthly, quarterly, and so on), it is more likely to have only one dominant seasonal pattern as opposed to a high-frequency series, as there are fewer aggregation options for another type of frequencies.\nMultiple seasonal patterns: If more than one dominant seasonal pattern exists in the series. This type of patterns are more likely to occur whenever the series has a high frequency (for example, daily, hourly, half-hourly, and so on), as there are more options to aggregate the series to a lower frequency. A typical example of multiple seasonality is the hourly demand for electricity, which could have multiple seasonal patterns, as the demand is derived from the hour of the day, the day of the week, or the yearly patterns, such as weather or the amount of daylight throughout the day.\n\nExample:\n\n# USgas dataset represents the total monthly consumptions of naturalgas in the US since # January 2000\nlibrary(TSstudio)\ndata(USgas)\nts_info(USgas)\n\n The USgas series is a ts object with 1 variable and 238 observations\n Frequency: 12 \n Start time: 2000 1 \n End time: 2019 10 \n\nts_plot(USgas,\n            title = \"US Monthly Natural Gas consumption\",\n            Ytitle = \"Billion Cubic Feet\",\n            Xtitle = \"Year\",\n            Xgrid = TRUE,\n            Ygrid = TRUE)"
  },
  {
    "objectID": "time_seasonality.html#seasonal-analysis-with-the-tsstudio-package",
    "href": "time_seasonality.html#seasonal-analysis-with-the-tsstudio-package",
    "title": "35  Seasonality Analysis",
    "section": "35.2 Seasonal Analysis with the TSstudio Package",
    "text": "35.2 Seasonal Analysis with the TSstudio Package\nThe TSstudio package provides a set of interactive data visualization functions based on the plotly package engine for seasonal analysis. It supports multiple time series objects, such as ts, xts, zoo, and data frame objects (data.frame, data.table, and tbl).\nThe ts_seasonal function provides several types of seasonal plots. The type argument sets the type of plot. The default option of the type argument is normal, which splits the series by full cycle units and provides a similar seasonal plot as the ggseasonplot function.\nExample:\n\nts_seasonal(USgas, type =\"normal\")\n\n\n\n\n\nThe cycle option group plots the series frequency units over time in chronological order; for example, all the observations that occurred during January over a certain amount of time for a monthly series. This allows us to identify seasonal pattern without de-trending the series. For instance, in the following plot of the USgas series, you can see that, despite the growth from year to year, in most of the cases the order of the months (from high to low) remains the same:\nExample:\n\nts_seasonal(USgas, type = \"cycle\")\n\n\n\n\n\nThe box option provides a box plot representation for each frequency unit. The box plot can be very informative plot, as it provides the range and quartile representation of the observations of each frequency unit:\nExample:\n\nts_seasonal(USgas,type =\"box\")\n\n\n\n\n\nThe all option returns a plot of all of the three plots (normal, cycle, and box) side by side. This allows you to connect the information from the three different representations of the series to get a better understanding of the patterns (if any exist) and characteristics of the series:\nExample:\n\nts_seasonal(USgas,type =\"all\")\n\n\n\n\n\nThe ts_heatmap function returns a heatmap plot of the time series object where the y-axis represents the frequency units (the months, for example, in the case of a monthly series) and the x-axis represents the cycle units of the series (the years, for example, in the case of a monthly series). The magnitude of each observation is represented by the color scale of the map, so the darker it is, the higher the value of the observation with respect to the overall values of the series:\nExample:\n\nts_heatmap(USgas, color = \"Reds\")\n\n\n\n\n\nLast but not least, the ts_quantile function provides a visual of the quantile plot of time series data. It’s mainly useful on high-frequency time series data, such as hourly or daily. By default, the function returns a quantile plot of the series frequency units, where the middle line represents the median and the lower and upper lines represent the 25th and 75th percentiles.\nExample:\n\n# Visualize the 24 cycles of the UKgrid dataset\nlibrary(UKgrid)\nts_quantile(UKgrid)"
  },
  {
    "objectID": "time_seasonality.html#seasonal-analysis-with-the-forecast-package",
    "href": "time_seasonality.html#seasonal-analysis-with-the-forecast-package",
    "title": "35  Seasonality Analysis",
    "section": "35.3 Seasonal Analysis with the Forecast Package",
    "text": "35.3 Seasonal Analysis with the Forecast Package\nThe forecast package provides several functions for seasonal analysis based on the ggplot2 package graphics engine (and the base plot function) and it supports ts objects. The ggseasonplot function creates a seasonal plot of the series by splitting and plotting each year as a separate line. This allows the user to obtain the changes in the series from year to year.\nExample:\n\n# Create the seasonal plot of the USgas series\nlibrary(forecast)\nggseasonplot(USgas, year.labels=TRUE, continuous=TRUE)\n\n\n\nggseasonplot(USgas, polar = TRUE)\n\n\n\n\nWe can learn from this representations of the USgas series that the series has a strong repeated pattern, which indicates the existence of the monthly seasonal pattern. Furthermore, as you can see from the color scale and the years labels, the series is growing from year to year. In the polar representation you can see that the series has repeated seasonal patterns along with a year-to-year growth or trend."
  },
  {
    "objectID": "time_seasonality.html#seasonal-analysis-with-descriptive-statistics",
    "href": "time_seasonality.html#seasonal-analysis-with-descriptive-statistics",
    "title": "35  Seasonality Analysis",
    "section": "35.4 Seasonal Analysis with Descriptive Statistics",
    "text": "35.4 Seasonal Analysis with Descriptive Statistics\nDescriptive statistics are a simple yet powerful method to describe the key statistical characteristics of the data. This method is based on the use of summary statistics tables and is a summary of the key statistical indicators, such as the mean, median, quantile, and standard deviation, and data visualization tools, such as box plots and bar charts.\nDescriptive statistics can be used to describe the characteristics of the frequency units of a series. This allows us to identify whether we can segment each period of the series by some statistical criteria, for example, the mean, the quantile range, and so on.\nExample:\n\n# We group the USgas series by its frequency units and then summarize the mean and \n# standard deviation of each frequency unit\n\n# Transform the ts object to data.frame object\nUSgas_df &lt;- data.frame(year = floor(time(USgas)), month = cycle(USgas),\n                      USgas = as.numeric(USgas))\n\n# Set the month abbreviation and transforming it to a factor\nUSgas_df$month &lt;- factor(month.abb[USgas_df$month], levels = month.abb)\nhead(USgas_df)\n\n  year month  USgas\n1 2000   Jan 2510.5\n2 2000   Feb 2330.7\n3 2000   Mar 2050.6\n4 2000   Apr 1783.3\n5 2000   May 1632.9\n6 2000   Jun 1513.1\n\n# Summarize the series by its frequency units\nlibrary(dplyr)\nUSgas_summary &lt;- USgas_df %&gt;%\n                  group_by(month) %&gt;%\n                  summarise(mean = mean(USgas), sd = sd(USgas))\nUSgas_summary\n\n# A tibble: 12 × 3\n   month  mean    sd\n   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Jan   2806.  309.\n 2 Feb   2502.  223.\n 3 Mar   2325.  241.\n 4 Apr   1886.  175.\n 5 May   1708.  194.\n 6 Jun   1691.  216.\n 7 Jul   1864.  261.\n 8 Aug   1889.  237.\n 9 Sep   1687.  242.\n10 Oct   1788.  260.\n11 Nov   2015.  284.\n12 Dec   2543.  278.\n\n# Now we plot the summary\nlibrary(plotly)\nplot_ly (data = USgas_summary, x = ~ month, y = ~ mean, type = \"bar\", name = \"Mean\") %&gt;%\n            layout (title = \"USgas - Monthly Average\", yaxis = list(title = \"Mean\",\n            range = c(1500, 2700)))\n\n\n\n\n\nFrom the summary statistic table of the series you can see that, on average, each month is different from the next consecutive month by its standard deviation with the exception of the two pairs May/June and July/August, which are relatively close to each other. This allows us to characterize some months with a distinct behavior from the rest, such as January, February, March, and November.\n\n35.4.1 Multiple seasonal patterns\nThe first indication for the potential existence of multiple seasonal patterns in the series is a high frequency, such as daily, hourly, and minutely. In those cases, there is more than one way to set the frequency of the series.\nExample:\n\n# UKgrid is an hourly time series, which marks it automatically as a suspect of\n# having multiple seasonal patterns\nlibrary(xts)\nlibrary(UKgrid)\n\n# Transform the series from a half-hourly frequency to hourly and will use an xts \n# format\nUKgrid_xts &lt;- extract_grid(type = \"xts\", \n                           columns = \"ND\", \n                           aggregate = \"hourly\", \n                           na.rm = TRUE)\n\nThe national demand of the UK high-voltage electric power transmission network (or the UKgrid series) has a strong seasonal pattern.\n\n# Transform the UKgrid series into data.frame format\nUKgrid_df &lt;- data.frame(time = index(UKgrid_xts), UKgrid = as.numeric(UKgrid_xts))\nstr(UKgrid_df)\n\n'data.frame':   127296 obs. of  2 variables:\n $ time  : POSIXct, format: \"2005-04-01 00:00:00\" \"2005-04-01 01:00:00\" ...\n $ UKgrid: num  65080 68207 69172 66769 64660 ...\n\n# Create seasonal features based on the periods we wish to check\nlibrary(lubridate)\nUKgrid_df$hour &lt;- hour(UKgrid_df$time)\nUKgrid_df$weekday &lt;- wday(UKgrid_df$time, label = TRUE, abbr = TRUE)\nUKgrid_df$month &lt;- factor(month.abb[month(UKgrid_df$time)], levels = month.abb)\nhead(UKgrid_df)\n\n                 time UKgrid hour weekday month\n1 2005-04-01 00:00:00  65080    0   vi\\\\.   Apr\n2 2005-04-01 01:00:00  68207    1   vi\\\\.   Apr\n3 2005-04-01 02:00:00  69172    2   vi\\\\.   Apr\n4 2005-04-01 03:00:00  66769    3   vi\\\\.   Apr\n5 2005-04-01 04:00:00  64660    4   vi\\\\.   Apr\n6 2005-04-01 05:00:00  65209    5   vi\\\\.   Apr\n\n# Summarize the series by its hourly cycle\nUKgrid_hourly &lt;- UKgrid_df %&gt;%\n                dplyr::group_by(hour) %&gt;%\n                dplyr::summarise(mean = mean(UKgrid, na.rm = TRUE), \n                                 sd = sd(UKgrid, na.rm = TRUE))\n\n# Use the summary statistics table to plot both the hourly mean and its standard\n# deviation\nplot_ly(UKgrid_hourly) %&gt;%\n      add_lines(x = ~ hour, y = ~ mean, name = \"Mean\") %&gt;%\n      add_lines(x = ~ hour, y = ~ sd, name = \"Standard Deviation\", yaxis = \"y2\",\n          line = list(color = \"red\", dash = \"dash\", width = 3)) %&gt;%\n      layout(title = \"The UK Grid National Demand - Hourly Average vs. Standard\nDeviation\",\n      yaxis = list(title = \"Mean\"),\n      yaxis2 = list(overlaying = \"y\",\n      side = \"right\",\n      title = \"Standard Deviation\"),\n      xaxis = list(title=\"Hour of the day\"),\n      legend = list(x = 0.05, y = 0.9),\n      margin = list(l = 50, r = 50)\n      )\n\n\n\n\n\nLooking at the plot of the preceding summary statistic table, we can see the following behavior of the series:\n\nThere is low demand during the nighttime (between midnight and 6 a.m.) and high demand between the morning hours and early evening.\nThere is a strong correlation between the average demand and its standard deviation.\nThe relatively low standard deviation of the demand average during the nighttime could indicate that there is strong sub-seasonal effect during those hours beside the hourly seasonality. This should make sense, as those are normal sleep hours, and therefore, on average, the demand is reasonably the same throughout the weekdays.\nOn the other hand, the high standard deviation throughout the high-demand hours could indicate that the demand is distributed differently on different periodicity views (such as weekday or month of the year).\n\nTo examine the last point, we will subset the series into two groups representing the demand in the middle of the night and the demand throughout the day (3 a.m. and 9 a.m., respectively), and then we will group them by the weekday:\nExample:\n\nUKgrid_weekday &lt;- UKgrid_df %&gt;%\n                    dplyr::filter(hour == 3 | hour == 9) %&gt;%\n                    dplyr::group_by(hour, weekday) %&gt;%\n                    dplyr::summarise(mean = mean(UKgrid, na.rm = TRUE), sd = sd(UKgrid, na.rm = TRUE))\nUKgrid_weekday$hour &lt;- factor(UKgrid_weekday$hour)\nplot_ly(data = UKgrid_weekday, \n        x = ~ weekday, \n        y = ~ mean, \n        type = \"bar\",\n        color = ~ hour) %&gt;%\n            layout(title = \"The Hourly Average Demand by Weekday\", \n                   yaxis = list(title = \"Mean\", range = c(30000, 75000)), \n                   xaxis = list(title = \"Weekday\"))\n\n\n\n\n\nYou will see in the preceding bar chart that the demand for electricity at 3 a.m. is relatively stable throughout all the days of the week, with a slight difference between the average during the weekdays and the days in the weekend (about 2% different). On the other hand, there is a significant difference between the weekday and weekend demand at 9 a.m. (that is, the demand on Monday is higher on average by 28% from the one on Sunday)."
  },
  {
    "objectID": "time_seasonality.html#seasonal-analysis-with-density-plots",
    "href": "time_seasonality.html#seasonal-analysis-with-density-plots",
    "title": "35  Seasonality Analysis",
    "section": "35.5 Seasonal Analysis with Density Plots",
    "text": "35.5 Seasonal Analysis with Density Plots\nAnother approach for analyzing seasonal patterns in time series data is by plotting the distribution of the frequency units by using histogram or density plots. This will allow us to examine whether each frequency unit has a unique distribution that can distinguish it from the rest of the units.\nExample:\n\n# Plot the density of each of the USgas months \nlibrary(ggplot2)\nggplot(USgas_df, aes(x = USgas)) +\n  geom_density(aes(fill = month)) +\n  ggtitle(\"USgas - Kernel Density Estimates by Month\") +\n  facet_grid(rows = vars(as.factor(month)))\n\n\n\n\nThe shape of the density plot of each month provides us with insights about the characteristics of each month (or frequency unit). We can see some indication of a seasonal pattern series, as the density plots are not overlapping on each other (with the exception of some consecutive months, such as May and June).\nNow let’s subset one of the hours during the day in the UKgrid dataset and plot its distribution by the day of the week. We should expect an overlapping during the nighttime and be able to distinguish between the distribution during the weekdays and weekend, as opposed to just the weekday.\nExample:\n\n# This plot represents the distribution of the demand at 9 a.m. throughout the days of the \n# week\nUKgrid_df$weekday &lt;- as.factor(UKgrid_df$weekday)\nUKgrid_df %&gt;% \n  dplyr::filter(hour == 0) %&gt;%\n  ggplot(aes(x = UKgrid)) +\n    geom_density(aes(fill = as.factor(weekday))) +\n    ggtitle(\"UKgrid - Kernel Density Estimates by Hour of the day\") +\n    facet_grid(rows = vars(as.factor(weekday)))\n\n\n\n\nYou can see that the distribution during the weekdays is distinguished from the one at the weekend."
  },
  {
    "objectID": "time_correlation.html#correlation-between-two-variables",
    "href": "time_correlation.html#correlation-between-two-variables",
    "title": "36  Correlation Analysis",
    "section": "36.1 Correlation Between Two Variables",
    "text": "36.1 Correlation Between Two Variables\nOne of the main goals of correlation analysis is to identify and quantify the relationship between two variables. This relationship could vary from having a full dependency or linear relationship between the two, to complete independence.\nOne of the most popular methods for measuring the level of correlation between two variables is the Pearson correlation coefficient. Although this method is not necessarily the most appropriate one for time series data, it is a simple and intuitive method for measuring correlation. This method, also known as the population correlation coefficient, is a ratio between the covariance of two variables and the multiplication of their standard deviation:\n\\[\\begin{align*}\n\\rho_{x,y}=\\frac {COV(X,Y)}{\\sigma_x \\sigma_y} {, where -1\\leq\\rho_{x,y}\\leq 1}\n\\end{align*}\\]\nThe values of the correlation coefficient segment the level of correlation into three main groups:\n\nPositively correlated: This is where the value of the coefficient is greater than 0. This indicates some degree of a positive linear relationship between the variables, depending on the value of the coefficient. As the value of \\(\\rho_{x,y}\\) (correlation coefficient) grows closer to 1, the linear relationship between the two variables grows stronger; 1 indicates a perfect linear dependency.\nNegatively correlated: This is where the value of the coefficient is lower than 0. This is the reflection of the positive correlation, and it is an indication for an inverse linear relationship. In this case, as the value of \\(\\rho_{x,y}\\) is closer to -1, the negative linear relationship of the two variables (for example, when one variable goes up the other goes down); -1 represents a perfect inverse linear dependency between the two variables.\nNot correlated: This is where the value of the coefficient is equal to 0, which indicates that the two variables are independent.\n\nTypically, we would consider the correlation between two variables to be strong if the value of the correlation coefficient is higher than 0.75 or lower than -0.75 (but this, of course, could change according to the field of research).\nMeasuring and analyzing the correlation between two variables, in the context of time series analysis, can be categorized in the following two categories:\n\nAnalyzing the correlation between a series and its lags, as some of the past lags may contain predictive information, which can be utilized to forecast future events of the series. One of the most popular methods for measuring the level of correlation between a series and its lags is the autocorrelation function.\nAnalyzing the correlation between two series (or causality analysis) in order to identify exogenous factors or predictors, which can explain the variation of the series over time (for example, the effect of weather patterns such as rainfall or temperature on taxi rides in New York City). In this case, the measurement of correlation is typically done with the cross-correlation function.\n\nNote that while in lags analysis we extract the required data (the series lags) from the series itself, causality analysis requires additional effort, such as identifying and extracting external variables, which may not always available."
  },
  {
    "objectID": "time_correlation.html#lags-analysis",
    "href": "time_correlation.html#lags-analysis",
    "title": "36  Correlation Analysis",
    "section": "36.2 Lags Analysis",
    "text": "36.2 Lags Analysis\nThe goal of lags analysis is to identify and quantify the relationship between a series and its lags. This relationship is typically measured by calculating the correlation between the two and with the use of data visualization tools.\nExample 1:\n\nlibrary(TSstudio)\ndata(USgas)\nts_plot(USgas,\n        title = \"US Monthly Natural Gas consumption\",\n        Ytitle = \"Billion Cubic Feet\",\n        Xtitle = \"Year\")\n\n\n\n\n\nNote that one of the main characteristic of the USgas series is the strong seasonal pattern.\nExample 2:\n\ndata(EURO_Brent)\nts_plot(EURO_Brent,\n        title = \"Brent Crude Oil Prices\",\n        Ytitle = \"US Dollars per Barrel\",\n        Xtitle = \"Year\")\n\n\n\n\n\nAs you can see the EURO_Brent series plot, which represents the monthly prices of the Brent crude oil in USD, the price of the oil does not have seasonal patterns or general trends.\nExample 3:\n\ndata(USVSales)\nts_plot(USVSales,\n        title = \"US Monthly Total Vehicle Sales\",\n        Ytitle = \"Thousands of units\",\n        Xtitle = \"Year\")\n\n\n\n\n\nThe plot of USVSales, the US monthly total vehicle sales, is an example of a series with both seasonal and cycle patterns.\n\n36.2.1 The autocorrelation function\nThe autocorrelation function (ACF) is the main method in time series analysis for quantifying the level of correlation between a series and its lags. The acf function from the stats package is R’s built-in ACF, which, by default, visualizes the results using a bar plot.\nExample 1:\nUse the acf function to plot the correlation of the USgas series and its first 60 lags (by setting the lag.max argument to 60)\n\nacf(USgas, lag.max = 60)\n\n\n\n\nEach bar in the ACF plot represents the level of correlation between the series and its lags in chronological order. Note that according to the x-axis notation lags 1 and 2 represent the 12 and 24 lags. The blue dotted lines indicate whether the level of correlation between the series and each lag is significant or not. By testing the null hypothesis that the correlation of the lag with the series is equal to zero, we can reject it whenever the level of correlation is either above or below the upper and lower dotted lines, respectively, with a level of significance of 5%. Otherwise, whenever the correlation is between the upper and lower dotted lines, we fail to reject the null hypothesis, and we can therefore ignore those lags (or assume that there is no significant correlation between the two).\nAs you can see from the USgas ACF plot, the series has a strong positive correlation with the seasonal lags (which decay over time) along with negative correlation with the mid-seasonal lags (for example, lags 6, 18, and 30).\nExample 2:\n\nacf(EURO_Brent, lag.max = 60)\n\n\n\n\nYou can see that the correlation of the series with its lags is decaying over time, whereas the closer the lag is, chronologically to the series, the stronger the relationship with the series. This type of correlation is also an indication that the series is not stationary and a differencing of the series is required.\nExample 3:\n\nacf(USVSales, lag.max = 60)\n\n\n\n\nThe correlation plot has a cyclic shape as a result of the seasonal pattern of the series. On the other hand, the decay rate of USVSales is faster compared to the rate of USgas due to the cycle pattern of the series, which shifts the series direction over time. As a result, the series is mainly correlated with the first seasonal lag. That being said, if we remove the series cycle (or detrend it) we will probably have a similar correlation pattern as USgas.\n\n\n36.2.2 Lag plots\nA lag plot is a non-statistical approach for analyzing the relationship between a series and its lags. This method is based on data visualization tools, with the use of two-dimensional scatter plots for visualizing the series (typically on the y-axis) against the \\(k\\) lag of the series. Hence, each pair of points represents a combination of the series observations and their corresponding lagged values. As more points on the lag plot are closer to the 45 degree line, the higher the correlation will be between the series and the corresponding lag.\nExample 1:\n\nts_lags(USgas)\n\n\n\n\n\nYou can see that, moving along from the first lag up to the sixth lag, the relationship between the series and its lags become less linear. This process starts to reverse from the seventh lag as the relationship gradually becomes more linear, where the seasonal lag (or lag 12) has the strongest relationship with the series. Those results are aligned with the ones we saw earlier with the ACF plot.\n\n# Plot the most recent seasonal lags (that is, lags 12, 24, 36, and 48)\nts_lags(USgas, lags = c(12, 24, 36, 48))\n\n\n\n\n\nExample 2:\n\nts_lags(EURO_Brent)\n\n\n\n\n\nAs you can see, the results are aligned with the ones we received before with the ACF. The EURO_Brent series has a strong linear relationship with the first lag, where the strength of that relationship decays as the distance of the lag from the series is higher.\nExample 3:\n\nts_lags(USVSales)\n\n\n\n\n\nIn the case of USVSales, the 12 lag has the closest linear relationship with the series (as we observed before with the ACF plot)."
  },
  {
    "objectID": "time_correlation.html#causality-analysis",
    "href": "time_correlation.html#causality-analysis",
    "title": "36  Correlation Analysis",
    "section": "36.3 Causality Analysis",
    "text": "36.3 Causality Analysis\nThe goal of causality analysis, in the context of time series analysis, is to identify whether a causality relationship exists between the series we wish to forecast and other potential exogenous factors. The use of those external factors as drivers of the forecasting model (whenever exists) could potentially provide accurate and robust forecast (as oppose of using only the past observation of the series).\n\n36.3.1 Causality vs correlation\nTwo variables will have a causality relationship whenever the change of one variable triggers a direct change of the second variable. This is also known as a cause-and-effect. For instance, the temperatures in Chicago have a direct impact on the consumption of natural gas throughout the year (as most of the heating systems operate with natural gas) and we can therefore assume that there is a causality relationship between the two. One of the main characteristics of causality is a high correlation between the two variables.\nHowever, this could be misleading in some cases, as high correlation by itself between two variables should not instantly imply the existence of a causality relationship, as the two may have a high dependency on a third variable. For example, you should expect to have a high correlation between the sales of ice cream and bathing suits (high demand during the summer and low throughout the winter), but there is no causal relationship between the two besides the fact that both are highly correlated with the same factor —the season of the year (or weather patterns).\nThe following diagram describes this type of relationship:\n\n\n\n\nFigure 13. Relationships of causality and correlation\n\n\n\nIn the context of time series analysis, the causality between two time series can be categorized into the following two types:\n\nDirect causality: This is where series B reacts immediately to the changes of series A at time t. For example, the speed of the wind will have a direct impact on the level of electricity production of a wind turbine.\nIn-direct causality: This is where the change of series A at time t triggers a change of series B at time t + n (where n &gt; 0). This lag effect is common in economic indicators, such as Gross Domestic Product (GDP) and consumption or the unemployment rate, where the change in the first triggers a gradual change in the other over time (for example, a drop in the GDP this quarter will impact the employment rate in the next quarter).\n\nTypically, a series from the first type will have a stronger dependency and a higher level of correlation compared to the second type. Yet, it is harder (or even not practical in some cases) to utilize series A as a predictor of series B, as the future values of series A are unknown and therefore need to be forecast as well (unless the future values are deterministic).\nThis could potentially increase the level of uncertainty of the model output, due to the fact that the input values are forecasted and will therefore come with some degree of uncertainty (as opposed to the actual input values of series A that were used in the training process). For example, if you wish to forecast the level of electricity production of a wind turbine during the next 10 days by using the wind speed as a predictor, you will have to forecast the wind speed in the next 10 days as well.\nIn conclusion, analyzing and identifying causality between two series is based on the use of correlation measurement and lag plots, along with some intuition and common sense (as mentioned previously, high correlation by itself is not necessarily an indication for causality).\n\n\n36.3.2 The cross-correlation function\nThe cross-correlation function (CCF) is the sister function of the ACF and it measures the level of correlation between two series and their lags in a fairly similar way.\nExample:\nAnalyze the relationship between the total vehicle sales (USVSales) and the unemployment rate (USUnRate) in the US to understand whether there is a cause and effect relationship between the two.\n\ndata(USUnRate)\nts_plot(USUnRate,\n        title = \"US Monthly Civilian Unemployment Rate\",\n        Ytitle = \"Unemployment Rate (%)\",\n        Xtitle = \"Year\")\n\n\n\n\n\nHowever, as the USUnRate series starts during the 1950s, as opposed to the USVSales series, which began in 1976, we need before starting to align the two series to the same time frame using the window function:\n\nus_vsales &lt;- window(USVSales, start = c(1976,1), end = c(2018,6))\nus_unrate &lt;- window(USUnRate, start = c(1976,1), end = c(2018,6)) \n\nNext, we will plot the two series on a two-y-axis plot (as the units of the two series are different) using the plotly package:\n\nlibrary(plotly)\nplot_ly(x = time(us_vsales),\n        y = us_vsales,\n        type = \"scatter\",\n        mode = \"line\",\n        name = \"Total Vehicle Sales\") %&gt;%\n  add_lines(x = time(us_unrate),\n            y = us_unrate,\n            name = \"Unemployment Rate\",\n            yaxis = \"y2\") %&gt;%\n  layout(title = \"Total Monthly Vehicle Sales vs Unemployment Rate in the US\",\n         yaxis2 = list(overlaying = \"y\",\n                       side = \"right\",\n                       title = \"Percentage\",\n                       showgrid = FALSE),\n         yaxis = list(title = \"Thousands of Units\",\n                      showgrid = FALSE),\n         legend = list(orientation = 'h'),\n         margin = list(l = 50, r = 50, b = 50, t = 50, pad = 2))\n\n\n\n\n\nFrom the preceding plot one can see that the two series move to the opposite direction, so when the vehicle sales increase, the unemployment rate decreases and the other way around. You can also see that, in most of the cases, the changes in the vehicle sales series are leading to the changes in the unemployment rate.\nTo explore this assumption further, we can measure the level of correlation between the unemployment rate and the vehicle sales and its lags using the ccf function from the stats package as well:\nExample:\n\nccf(x = us_vsales, y = us_unrate, lag.max = 36)\n\n\n\n\nSimilarly to the ACF output, each bar in the CCF plot represents the level of correlation between the main series and the lags of the secondary. Lag 0 represents the direct correlation between the two (original) series, where the negative and positive lags represent the correlation between the unemployment rate and the past and leading lags of the vehicle sales series, respectively. The main thing to note from the preceding plot is that the unemployment rate is correlated more to the past lags as opposed to the leading lags of the vehicle sales.\nIt is difficult (and probably even wrong) to conclude from the results that the vehicle sales explicitly drive the changes in the unemployment rate (and it is definitely not in the scope of this book). However, there is some indication of a causality relationship, which can be derived from the level of correlation along with common sense, given the size of the vehicle industry in the US and its historical impact on the economy.\nAlternatively, you can plot the relationship between US vehicle sales and the lags of the unemployment rate with the ccf_plot function from the TSstudio package. This function works similarly to the ts_lags function we previously introduced:\n\nccf_plot(x = USVSales, y = USUnRate, lags = 0:12)\n\n\n\n\n\nThe main advantage of the ccf_plot function over the ccf function is that the first automatically aligns the two series according to their chronological order, whereas the ccf function does not have this automatic functionality, and therefore requires a pre-processing step (as we used previously with the window function)."
  },
  {
    "objectID": "time_forecast_basics.html#forecasting-workflow",
    "href": "time_forecast_basics.html#forecasting-workflow",
    "title": "37  Forecasting Basics",
    "section": "37.1 Forecasting Workflow",
    "text": "37.1 Forecasting Workflow\nForecasting is the third and last component of the time series analysis. Traditional time series forecasting follows the same workflow as most of the fields of predictive analysis, such as regression or classification, and typically includes the following steps:\n\nGoal Definition. Determining and clearly defining the forecasting goal is essential for arriving at useful results. One must first determine the purpose of generating forecasts, the type of forecasts that are needed, how the forecasts will be used by the organization, what are the costs associated with forecast errors, what data will be available in the future, and more. These issues affect every step in the forecasting process, from data collection through data exploration, preprocessing, modeling and performance evaluation.\n\nDescriptive vs. Predictive Goals. Time series data is done for either descriptive or predictive purposes. In descriptive modeling, or time series analysis, a time series is modeled to determine its components in terms of seasonal patterns, trends, relation to external factors, and the like. These can then be used for decision making and policy formulation. In contrast, time series forecasting uses the information in a time series (perhaps with additional information) to forecast future values of that series. The difference between descriptive and predictive goals leads to differences in the types of methods used and in the modeling process itself. A predictive model is judged by its predictive accuracy rather than by its ability to provide correct causal explanations.\nForecast Horizon and Forecast Updating. How far into the future should we forecast? Must we generate all forecasts at a single time point, or can forecasts be generated on an ongoing basis?. Both questions depend on how the forecasts will be used in practice and therefore require close collaboration with the forecast stakeholders in the organization. While long-term forecasting is often a necessity, it is important to have realistic expectations regarding forecast accuracy: the further into the future, the more likely that the forecasting context will change and therefore uncertainty increases. The model should be examined periodically to assure its suitability for the changed context and if possible, updated. Therefore, it is sometimes useful to provide periodic updated forecasts by incorporating new accumulated information. Such refreshing of the forecasts based on new data is called roll-forward forecasting. All these aspects of the forecast horizon have implications on the required length of the series for building the forecast model, on frequency and timeliness of collection, on the forecasting methods used, on performance evaluation, and on the uncertainty levels of the forecasts.\nForecast Use. How will the forecasts be used? Understanding how the forecasts will be used, perhaps by different stakeholders, is critical for generating forecasts of the right type and with a useful accuracy level. Will the forecasts and forecasting method to be presented to management or to the technical department? Answers to such questions are necessary for choosing appropriate data, methods, and evaluation schemes.\nLevel of Automation. The level of required automation depends on the nature of the forecasting task and on how forecasts will be used in practice. In scenarios where many series are to be forecasted on an ongoing basis, and not much forecasting expertise can be allocated to the process, an automated solution can be advantageous.\n\nData preparation: Here, we prepare the data for the training and testing process of the model. This step includes splitting the series into training (in-sample) and testing (out-sample) partitions, creating new features (when applicable), and applying a transformation if needed (for example, log transformation, scaling, and so on).\n\nData Quality. The quality of our data in terms of measurement accuracy, missing values, corrupted data, and data entry errors can greatly affect the forecasting results. Data quality is especially important in time series forecasting, where the sample size is small (typically not more than a few hundred values in a series).\nTemporal Frequency. With today’s technology, many time series are recorded on very frequent time scales. However, although data might be available at a very frequent scale, for the purpose of forecasting it is not always preferable to use this frequency. In considering the choice of temporal frequency, one must consider the frequency of the required forecasts (the goal) and the level of noise1 in the data. For example, if the goal is to forecast next-day sales at a grocery store, minute-by-minute sales data is likely less useful than daily aggregates, and the minute-by-minute series will contain many sources of noise that degrade its daily-level forecasting power. Even when forecasts are needed on a particular frequency (such as daily) it is sometimes advantageous to aggregate the series to a lower frequency (such as weekly), and model the aggregated series to produce forecasts.\nSeries Granularity. “Granularity” refers to the coverage of the data. This can be in terms of geographical area, population, time of operation, etc. As with temporal frequency, the level of granularity must be aligned with the forecasting goal, while considering the levels of noise. Exploring different aggregation and slicing levels is often needed for obtaining adequate series. The level of granularity will eventually affect the choice of preprocessing, forecasting method(s), and evaluation metrics.\nDomain Expertise. Domain expertise is needed for determining which data to use (e.g., daily vs. hourly, how far back, and from which source), describing and interpreting patterns that appear in the data, from seasonal patterns to extreme values and drastic changes (e.g., clarifying what are “after hours”, interpreting massive absences due to the organization’s annual picnic, and explaining the drastic change in trend due to a new company policy). Domain expertise is also used for helping evaluate the practical implications of the forecasting performance.\nTime Series Components. For the purpose of choosing adequate forecasting methods, it is useful to dissect a time series into a systematic part and a non-systematic part. The systematic part is typically divided into three components: level, trend, and seasonality. The non-systematic part is called noise.\nLevel describes the average value of the series, trend is the change in the series from one period to the next, and seasonality describes a short-term cyclical behavior that can be observed several times within the given series. While some series do not contain trend or seasonality, all series have a level. Lastly, noise is the random variation that results from measurement error or other causes that are not accounted for. It is always present in a time series to some degree, although we cannot observe it directly.\nThe different components are commonly considered to be either additive or multiplicative. Forecasting methods attempt to isolate the systematic part and quantify the noise level. The systematic part is used for generating point forecasts and the level of noise helps assess the uncertainty associated with the point forecasts.\n\n\n\nVisualizing Time Series. An effective initial step for characterizing the nature of a time series and for detecting potential problems is to use data visualization. By visualizing the series we can detect initial patterns, identify its components and spot potential problems such as extreme values, unequal spacing, and missing values.\nMissing Values. Missing values in a time series create “holes” in the series. The presence of missing values has different implications and requires different actions depending on the forecasting method. Forecasting methods such as ARIMA models and smoothing methods cannot be directly applied to time series with missing values, because the relationship between consecutive periods is modeled directly. In such cases, a solution is to impute, or “fill in”, the missing values. Imputation approaches range from simple solutions, such as averaging neighboring values, to creating forecasts of missing values using earlier values or external data. In contrast, forecasting methods such as linear and logistic regression models and neural networks can be fit to a series with “holes”, and no imputation is required. The implication of missing values in such cases is that the model/method is fitted to less data points. In short, since some forecasting methods cannot tolerate missing values in a series and others can, it is important to discover any missing values before the modeling stage.\nUnequally Spaced Series. Equal spacing means that the time interval between two consecutive periods is equal (e.g., daily, monthly, quarterly data). However, some series are naturally unequally spaced. These include series that measure some quantity during events where event occurrences are random (such as bus arrival times), naturally unequally spaced (such as holidays or music concerts), or determined by someone other than the data collector (e.g., bid timings in an online auction). As with missing values, some forecasting methods can be applied directly to unequally spaced series, while others cannot. Converting an unequally spaced series into an equally spaced series typically involves interpolation using approaches similar to those for handling missing values.\nExtreme values. These are values that are unusually large or small compared to other values in the series. Extreme values can affect different forecasting methods to various degrees. The decision whether to remove an extreme value or not must rely on information beyond the data. Is the extreme value the result of a data entry error? Was it due to an unusual event (such as an earthquake) that is unlikely to occur again in the forecast horizon? If there is no grounded justification to remove or replace the extreme value, then the best practice is to generate two sets of forecasts: those based on the series with the extreme values and those based on the series excluding the extreme values.\nChoice of Time Span. Another pre-processing operation that is sometimes required is determining how far back into the past we should be consider. In other words, what is the time span of the data to be used. While a very short (and recent) series might be insufficiently informative for forecasting purposes, beyond a certain length the additional information will likely be useless at best, and harmful at worst. Considering a very long past of the series might deteriorate the accuracy of future forecasts because of the changing context and environment occurring during the data period.\n\nTrain the model: When building predictive models we typically create two or three data partitions: a training set, a validation set, and sometimes an additional test set. The training partition, typically the largest partition, contains the data used to train, tune, and estimate the model coefficients that minimize the selected error criteria. The same training partition is generally used to develop multiple models. The fitted values and the model estimation of the training partition observations will be used later on to evaluate the overall performance of the model. The validation partition is used to assess the performance of each model so that we can compare models and pick the best one. The test partition (sometimes called the hold- out or evaluation partition) is used to assess the performance of the chosen model with new data. Before attempting to forecast future values of the series, the training and validation periods must be recombined into one long series, and the chosen method/model is rerun on the complete data. This final model is then used to forecast future values.\nTest the model: Here, we utilize the trained model to forecast the corresponding observations of the testing partition. The main idea here is to evaluate the performance of the model with a new dataset (that the model did not see during the training process).\nModel evaluation: Last but not least, after the model was trained and tested, it is time to evaluate the overall performance of the model on both the training and testing partitions. Based on the evaluation process of the model, if the model meets a certain threshold or criteria, then we either retain the model using the full series in order to generate the final forecast or select a new training parameter/different model and repeat the training process.\n\nNote that the process does not end once forecasts are generated, because forecasting is typically an ongoing goal. Hence, forecast accuracy is monitored and sometimes the forecasting method is adapted or changed to accommodate changes in the goal or the data over time.\nIn time series, this process has its own unique characteristics, which distinguish it from other predictive fields:\n\nThe training and testing partitions must be ordered in chronological order, as opposed to random sampling.\nOnce we have trained and tested the model using the training and testing partitions, we will retrain the model with all of the data (or at least the most recent observation in chronological order).\n\n\n\n\n\nFigure 14. Time series forecasting workflow"
  },
  {
    "objectID": "time_forecast_basics.html#training-approaches",
    "href": "time_forecast_basics.html#training-approaches",
    "title": "37  Forecasting Basics",
    "section": "37.2 Training Approaches",
    "text": "37.2 Training Approaches\nOne of the core elements of the forecasting workflow is the model training process. The quality of the model’s training will have a direct impact on the forecast output. The main goals of this process are as follows:\n\nFormalize the relationship of the series with other factors, such as seasonal and trend patterns, correlation with past lags, and external variables in a predictive manner.\nTune the model parameters (when applicable).\nCheck if the model is scalable on new data, or in other words, avoids overfitting.\n\nPrior to the training process, the series is split into training and testing partitions, where the model is being trained on the training partition and tested on the testing partition. These partitions must be in chronological order, regardless of the training approach that has been used. The main reason for this is that most of the time series models establish a mathematical relationship between the series in terms of its past lags and error terms.\n\n37.2.1 Training with single training and testing partitions\nOne of the most common training approaches is using single training and testing (or single out-of-sample) partitions. This approach is based on splitting the series into training and testing partitions (or in-sample and out-sample partitions, respectively), training the model on the training partition, and testing its performance on the testing set.\nThis approach has a single parameter —the length of the out-of-sample (or the length of the testing partition). Typically, the length of the testing partition is derived from the following rules of thumb: up to 30% of the total length of the series in order to have enough observation data for the training process.\nThere are two methods to split a time series into a training and testing partitions:\n\nThe window method.\nExample:\nSplit the USgas series into partitions, leaving the last 12 observations of the series as the testing partition and the rest as training.\n\nlibrary(TSstudio)\ndata(USgas)\nts_info(USgas)\n\n The USgas series is a ts object with 1 variable and 238 observations\n Frequency: 12 \n Start time: 2000 1 \n End time: 2019 10 \n\n# Use the window function to split the series into training and testing\n# partitions\ntrain &lt;- window(USgas, \n                start = time(USgas)[1], \n                end = time(USgas)[length(USgas) - 12])\ntest &lt;- window(USgas,\n               start = time(USgas)[length(USgas) - 12 + 1],\n               end = time(USgas)[length(USgas)])\nts_info(train)\n\n The train series is a ts object with 1 variable and 226 observations\n Frequency: 12 \n Start time: 2000 1 \n End time: 2018 10 \n\nts_info(test)\n\n The test series is a ts object with 1 variable and 12 observations\n Frequency: 12 \n Start time: 2018 11 \n End time: 2019 10 \n\n\nThe ts_split function from the TSstudio package, which provides a customized way for creating training and testing partitions for time series data:\nExample:\n\nUSgas_partitions &lt;- ts_split(USgas, sample.out = 12)\ntrain &lt;- USgas_partitions$train\ntest &lt;- USgas_partitions$test\nts_info(train)\n\n The train series is a ts object with 1 variable and 226 observations\n Frequency: 12 \n Start time: 2000 1 \n End time: 2018 10 \n\nts_info(test)\n\n The test series is a ts object with 1 variable and 12 observations\n Frequency: 12 \n Start time: 2018 11 \n End time: 2019 10 \n\n\nThe main advantage of this method is its simplicity, as it is fairly fast to train and test a model while using (relatively) cheap compute power. On the other hand, it isn’t possible to come to a conclusion about the stability and scalability of the model’s performance based on a single test unit. One way to mitigate that risk is with the backtesting approach, which is based on training a model with multiple training and testing partitions.\n\n\n\n37.2.2 Training with backtesting\nThe backtesting approach for training a forecasting model is an advanced version of the single out-of-sample approach we saw previously. It is based on the use of a rolling window to split the series into multiple pairs of training and testing partitions.\nThe following diagram demonstrates the structure of the backtesting with an expanding window:\n\n\n\n\nFigure 15. The backtesting training process\n\n\n\nAs you can see in the preceding diagram, all of the training partitions of the expanding window method start at the same index point, \\(T_0\\), where each training partition is a combination of the previous training partition with the following \\(n\\) observations (where \\(n\\) is a constant that represents the expanded rate of the window). It would make sense to use this method when the series has a strong seasonal pattern and stable trend. In this case, the first observations of the series could, potentially, have predictive information that can be utilized by the model.\nThe downside of this method is training the model with different length partitions, as typically a model tends to learn more and therefore perform better when more data is available. Therefore, we may observe that the performance of models that are trained on the latest partitions perform better than the ones that are trained with the first partitions."
  },
  {
    "objectID": "time_forecast_basics.html#forecast-methods",
    "href": "time_forecast_basics.html#forecast-methods",
    "title": "37  Forecasting Basics",
    "section": "37.3 Forecast Methods",
    "text": "37.3 Forecast Methods\nThere is a big variety of forecasting methods for time series, the reason being is that different methods perform differently depending on the nature of the data and the forecasting requirements. Forecasting methods can be roughly divided into the following:\n\nModel-based methods. This methods use a statistical, mathematical, or other scientific model to approximate a data series. The training data are used to estimate the parameters of the model, and then the model with estimated parameters is used to generate forecasts. Model-based forecasting methods include multiple linear regression and autoregressive models, where the user specifies a certain linear model and then estimates it from the time series. Logistic regression models are also model-based. Model-based methods are especially advantageous when the series at hand is very short.\nData-driven methods. These methods include data-driven approach of smoothing, where algorithms “learn” patterns from the data. Data-driven methods are advantageous when model assumptions are likely to be violated, or when the structure of the time series changes over time. Naive forecasts are also data-driven, in that they simply use the last data point in the series for generating a forecast. Another advantage of many data-driven forecasting methods is that they require less user input, however, this means that more data (that is, a longer series) is required for adequate learning. Data mining methods such as neural networks, regression trees and other algorithms for predicting cross-sectional data are sometimes used for time series forecasting, especially for incorporating external information into the forecasts.\n\nModel-based methods are generally preferable for forecasting series with global patterns that extend throughout the data period, as they use all the data to estimate the global pattern. For a local pattern, a model would require specifying how and when the patterns change, which is usually impractical and often unknown. Therefore, data-driven methods are preferable for forecasting series with local patterns, as they “learn” patterns from the data, and their memory length can be set to best adapt to the rate of change in the series.\nNote that one serious danger with data-driven methods is overfitting the training period, and hence performance should be carefully evaluated and constantly monitored to avoid this pitfall.\nThe methods presented here create forecasts for a time series based on its history. Such methods are termed extrapolation methods. The advantage of this practice is simplicity. The disadvantage is that forecasts do not take into account possible relationships between the series. An alternative to extrapolation methods, when the purpose is forecasting, is to capture external information that correlates with a series more heuristically. For example, consider a model for forecasting the average weekly airfare on a certain route that adds to the model the weekly gas prices and past weekly airfare rates as inputs.\n\n37.3.1 Forecasting automation\nIn terms of automation, some forecasting methods are easier to automate than others. Typically, data-driven methods are easier to automate, because they “learn” patterns from the data. However, even data-driven methods can perform poorly if the characteristics of the series (seasonality, trend) are not correctly specified, or if there is insufficient data.\nSmoothing methods are preferable for automated forecasting, because they require less tweaking, are suitable for a range of trends and seasonal patterns, are computationally fast, require very little data storage, and adjust when the behavior of a series changes over time.\nNeural networks are similarly useful for automation, although they are computationally more demanding and require a lot of past data.\nEven with an automated system in place, it is advisable to monitor the forecasts and forecast errors produced by an automated system and occasionally re-examine their suitability and update the system accordingly.\n\n\n37.3.2 Combining methods\nWhile it might appear that we should choose one method for forecasting among the various options, it turns out that combining multiple forecasting methods can lead to improved predictive performance.\nCombining methods can be done via two-level (or multilevel) methods, where the first method uses the original time series to generate forecasts of future values, and the second method uses the forecast errors from the first layer to generate forecasts of future forecast errors, thereby “correcting” the first level forecasts.\nAnother combination approach is via ensembles, where multiple methods are applied to the time series, each generating separate forecasts. The resulting forecasts are then averaged in some way to produce the final forecast. Averaging across multiple methods can lead to forecasts that are more robust and are of higher precision.\nAnother combination approach is to use different series measuring the same phenomenon of interest (e.g., temperature) and take an average of the multiple resulting forecasts. For example, when a series of interest is available from multiple sources, each with slightly different numbers or precision (e.g., from manual and electronic collection systems or from different departments or measuring devices), we can create ensembles of forecasts, each based on a different series."
  },
  {
    "objectID": "time_forecast_basics.html#forecast-evaluation",
    "href": "time_forecast_basics.html#forecast-evaluation",
    "title": "37  Forecasting Basics",
    "section": "37.4 Forecast Evaluation",
    "text": "37.4 Forecast Evaluation\nThe primary goal of the evaluation step is to assess the ability of the trained model to forecast (or based on other criteria) the future observations of the series accurately. This process includes doing the following:\n\nResidual analysis: This focuses on the quality of the model, with fitted values in the training partition.\nScoring the forecast: This is based on the ability of the model to forecast the actual values of the testing set.\n\n\n37.4.1 Residual analysis\nResidual analysis tests how well the model captured and identified the series patterns. In addition, it provides information about the residuals distributions, which are required to build confidence intervals for the forecast.\nThe mathematical definition of a residual is the difference between the actual observation and the corresponding fitted value of the model, which was observed in the training process, or as the following equation:\n\\[\\epsilon_t = Y_t - \\hat{Y_t}\\]\nWhere, \\(\\epsilon_t\\), \\(Y_t\\), and \\(\\hat{Y_t}\\) represent the residual, actual, and fitted values, respectively, at time \\(t\\).\nTo demonstrate the residual analysis process, we will train an ARIMA model on the training partition we created earlier for the USgas series.\nExample:\n\nlibrary(forecast)\nmd &lt;- auto.arima(train)\n\nThe checkresiduals function from the forecast package returns the following four outputs:\n\nTime series plot of the residuals.\nACF plot of the residuals.\nDistribution plot of the residuals.\nThe output of the Ljung-Box test. Recall that the Ljung-Box test is a statistical method for testing whether the autocorrelation of a series (which, in this case, is the residuals) is different from zero.\n\n\ncheckresiduals(md)\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(2,1,1)(2,1,1)[12]\nQ* = 24.95, df = 18, p-value = 0.1263\n\nModel df: 6.   Total lags used: 24\n\n\nStarting with the output of the Ljung-Box test output, you will notice that, based on the P-value results, we can reject the null hypothesis with a level of significance of 0.01. Hence, there is an indication that the correlation between the residual series and its lags are different from zero.\nThe ACP plot provides additional support for that as well. This indicates that the model did not fully capture all of the series patterns, and you may want to modify the model tuning parameters.\nThe residual time series plot oscillates around the x-axis, with the exception of a few residuals, which cross the value of +/- 200. This could indicate that some outliers occur during these periods, and you should check those data points in the series.\nLast but not least is the distribution plot of the residuals, which seem to be a fairly good representation of a normal distribution.\n\n\n37.4.2 Scoring the forecast\nOnce you finalize the model tuning, it is time to test the ability of the model to predict observations that the model didn’t see before (as opposed to the fitted values that the model saw throughout the training process). The most common method for evaluating the forecast’s success is to predict the actual values with the use of an error metric to quantify the forecast’s overall accuracy.\nThe selection of a specific error metric depends on the forecast accuracy’s goals. Common examples of common error metrics are: the Mean Squared Error (MSE), the Root Mean Squared Error (RMSE), the Mean Absolute Error (MAE), the Mean Absolute Percentage Error (MAPE).\nWe will use the accuracy function to obtain several error metrics for both the fitted values (the training set row) and the actual forecast (the test set row).\nExample:\nUse the model we trained earlier (md) to forecast the 12 observations we left for testing and score its performance.\n\nfc &lt;- forecast(md, h = 12)\n# Score the model's performance with respect to the actual values\n# in the testing partition:\naccuracy(fc, test)\n\n                    ME      RMSE      MAE       MPE     MAPE      MASE\nTraining set  5.844136  97.81626 73.42657 0.1170672 3.522348 0.6376860\nTest set     37.847885 103.22848 81.46603 1.3107987 3.261643 0.7075062\n                     ACF1 Theil's U\nTraining set -0.004183172        NA\nTest set     -0.046708926 0.3404092\n\n\nYou will notice that the model MAPE results are 3.52% and 7.84% on the training and testing partitions, respectively. A higher error rate on the testing partition compared to the training partition should not come as a surprise, as typically the model saw the training partition data throughout the training process. A fairly low error rate in the training set, along with the high error rate in the testing set, is a clear indication of overfitting in the model.\nAn alternative approach to evaluating the fit of the model on both the training and testing is with the test_forecast function from the TSstudio package. This function visualizes the actual series, the fitted values on the training partition, and the forecasted values on the testing set. Hovering over the fitted or forecasted values makes a textbox pop up with the RMSE and MAPE results on both the training and testing partitions.\nExample:\n\ntest_forecast(actual = USgas,\n              forecast.obj = fc,\n              test = test)\n\n\n\n\n\nWith this approach it is easier and faster to identify insights about the goodness of the fit of both the fitted and forecasted values when plotting those values against the actual values of the series. For instance, you will immediately notice that the residual peak during 2006 is caused by outliers (or lower consumption than the normal pattern of the series). In addition, the actual forecast missed the 2018 yearly peak. Those insights cannot be observed with error metrics.\n\n\n37.4.3 Forecast benchmark\nAccording to the error metrics, the trained model scored a MAPE of 3.26% or RMSE of 103.221. How can we assess whether these results are too high or low? The most common method is to benchmark the model’s performance to some baseline forecast or to some legacy method that we wish to replace.\nA popular benchmark approach would be to use a simplistic forecasting approach as a baseline. For instance, let’s forecast the series with a naive approach and use it as a benchmark for the previous forecast we created with the ARIMA model.\nExample:\n\nlibrary(forecast)\nnaive_model &lt;- naive(train, h = 12)\n\nA naive approach typically assumes that the most recently observed value is the true representative of the future. Therefore, it will continue with the last value to infinity (or as the horizon of the forecast). In the naive model, there is no training process, and the fitted values are set as the actual values.\n\n# Review the performance of the model on the training and testing partitions\ntest_forecast(actual = USgas,\n              forecast.obj = naive_model,\n              test = test)\n\n\n\n\n# Evaluate the model's performance on both the training and testing partitions\naccuracy(naive_model, test)\n\n                     ME     RMSE      MAE        MPE     MAPE     MASE\nTraining set  -1.028444 285.6607 228.5084 -0.9218463 10.97123 1.984522\nTest set     301.891667 499.6914 379.1417  9.6798015 13.28187 3.292723\n                  ACF1 Theil's U\nTraining set 0.3761105        NA\nTest set     0.7002486  1.499679\n\n\nHowever, since USgas has a strong seasonal pattern, it would make sense to use a seasonal naive model that takes into account seasonal variation. The snaive_model function from the forecast package uses the last seasonal point as a forecast of all of the corresponding seasonal observations. For example, if we are using monthly series, the value of the most recent January in the series will be used as the point forecast for all future January months:\n\nsnaive_model &lt;- snaive(train, h = 12)\ntest_forecast(actual = USgas,\n              forecast.obj = snaive_model,\n              test = test)\n\n\n\n\n# Evaluate the model's performance on both the training and testing partitions\naccuracy(snaive_model, test)\n\n                   ME     RMSE      MAE      MPE     MAPE     MASE       ACF1\nTraining set 33.99953 148.7049 115.1453 1.379869 5.494048 1.000000  0.4859501\nTest set     96.45000 164.6967 135.8833 3.612060 5.220458 1.180103 -0.2120929\n             Theil's U\nTraining set        NA\nTest set     0.4289964\n\n\nIt seems that the seasonal naive model has a better fit for the type of series we are forecasting, that is, USgas, due to its strong seasonal pattern (compared to the naive model). Therefore, we will use it as a benchmark for the ARIMA model.\nBy comparing both the MAPE and RMSE of the two models in the testing partition, it is clear that the ARIMA model provides a lift (in terms of accuracy) with respect to the benchmark model:\n\n\n\nModel\nMAPE\nRMSE\n\n\n\n\nARIMA\n3.26%\n103.22\n\n\nsnaive\n5.22%\n164.69"
  },
  {
    "objectID": "time_forecast_basics.html#finalizing-the-forecast",
    "href": "time_forecast_basics.html#finalizing-the-forecast",
    "title": "37  Forecasting Basics",
    "section": "37.5 Finalizing the Forecast",
    "text": "37.5 Finalizing the Forecast\nNow that the model has been trained, tested, tuned (if required), and evaluated successfully, we can move forward to the last step and finalize the forecast. This step is based on recalibrating the model’s weights or coefficients with the full series.\nThere are two approaches to using the model parameter setting:\n\nIf the model was tuned manually, you should use the exact tuning parameters that were used on the trained model.\nIf the model was tuned automatically by an algorithm (such as the auto.arima), you can do either of the following:\n\nExtract the parameter setting that was used by with the training partition.\nLet the algorithm retun the model parameters using the full series, under the assumption that the algorithm has the ability to adjust the model parameters correctly when training the model with new data. The use of algorithms to automate the model tuning process is recommended when the model’s ability to tune the model is tested with backtesting.\n\n\nExample:\n\nmd_final &lt;- auto.arima(USgas)\nfc_final &lt;- forecast(md_final, h = 12)\nplot_forecast(fc_final,\n              title = \"The US Natural Gas Consumption Forecast\",\n              Xtitle = \"Year\",\n              Ytitle = \"Billion Cubic Feet\")"
  },
  {
    "objectID": "time_forecast_basics.html#handling-forecast-uncertainty",
    "href": "time_forecast_basics.html#handling-forecast-uncertainty",
    "title": "37  Forecasting Basics",
    "section": "37.6 Handling Forecast Uncertainty",
    "text": "37.6 Handling Forecast Uncertainty\nThe main goal of the forecasting process is to minimize the level of uncertainty around the future values of the series. Although we cannot completely eliminate this uncertainty, we can quantify it and provide some range around the point estimate of the forecast (which is nothing but the model’s expected value of each point in the future). This can be done by using either the confidence interval (or a credible interval, when using the Bayesian model) or by using simulation.\n\n37.6.1 Confidence intervals\nThe confidence interval is a statistical approximation method that’s used to express the range of possible values that contain the true value with some degree of confidence (or probability).\nThere are two parameters that determine the range of confidence interval:\n\nThe level of confidence or the probability that the true value will be in that range. The higher the level of confidence is, the wider the interval range. By default, the forecast function generates a prediction interval with a level of confidence of 80% and 95%, but you can modify it using the level argument.\nThe estimated standard deviation of the forecast at time \\(T+i\\), where \\(T\\) represents the length of the series and \\(i\\) represents the \\(i\\) forecasted value. The lower the error rate, the shorter the range of the prediction interval.\n\nExample:\n\nfc_final2 &lt;- forecast(md_final, \n                      h = 60, \n                      level = c(80, 90))\nplot_forecast(fc_final2,\n              title = \"The US Natural Gas Consumption Forecast\",\n              Xtitle = \"Year\",\n              Ytitle = \"Billion Cubic Feet\")\n\n\n\n\n\n\n\n37.6.2 Simulation\nAn alternative approach is to use the model distribution to simulate possible paths for the forecast. This method can only be used when the model distribution is available.\nThe forecast_sim function from the TSstudio package provides a built-in function for simulating possible forecasting paths. This estimate can be used to calculate the forecast point estimate (for example, using the mean or median of all of the paths), or to calculate probabilities of getting different values.\nExample:\n\n# Feed the same model to the function and run 500 iterations:\nfc_final3 &lt;- forecast_sim(model = md_final,\n                          h = 60,\n                          n = 500)\n\nfc_final3 contains all of the calculate simulations and the simulated paths. Let’s extract the simulation plot (and use the plotly package to add titles for the plot):\n\nlibrary(plotly)\nfc_final3$plot %&gt;%\n      layout(title = \"US Natural Gas Consumption - Forecasting Simulation\",\n             yaxis = list(title = \"Billion Cubic Feet\"),\n             xaxis = list(title = \"Year\"))\n\n\n\n\n\n\n\n37.6.3 Horse race approach\nThe horse race approach is a robust forecasting approach based on training, testing, and evaluating multiple forecasting models and selecting the model that performs the best on the testing partitions.\nIn the following example, we will apply horse racing between seven different models using backtesting. The train_model function from the TSstudio package conducts the full process of training, testing, evaluating, and then forecasting, using the model that performed the best on the backtesting testing partitions. By default, the model will test the following models:\n\narima - model from the stats package\nauto.arima - Automated ARIMA model. from the forecast package\nets - Exponential smoothing state space model from the forecast package\nHoltWinters - Holt-Winters filtering from the stats package\nnnetar - Neural network time series model from the forecast package\ntslm - model from the forecast package (note that the ‘tslm’ model must have the formula argument in the ‘method_arg’ argument)\n\nExample:\n\nlibrary(TSstudio)\nset.seed(1234)\nmethods &lt;- list(ets1 = list(method = \"ets\",\n                            method_arg = list(opt.crit = \"lik\"),\n                            notes = \"ETS model with opt.crit = lik\"),\n                ets2 = list(method = \"ets\",\n                            method_arg = list(opt.crit = \"amse\"),\n                            notes = \"ETS model with opt.crit = amse\"),\n                arima1 = list(method = \"arima\",\n                              method_arg = list(order = c(2,1,0)),\n                              notes = \"ARIMA(2,1,0)\"),\n                arima2 = list(method = \"arima\",\n                              method_arg = list(order = c(2,1,2),\n                                                seasonal = list(order = c(1,1,1))), \n                              notes = \"SARIMA(2,1,2)(1,1,1)\"),\n                hw = list(method = \"HoltWinters\",\n                          method_arg = NULL,\n                          notes = \"HoltWinters Model\"),\n                tslm = list(method = \"tslm\",\n                            method_arg = list(formula = input ~ trend + season),\n                            notes = \"tslm model with trend and seasonal components\"))\nUSgas_forecast &lt;- train_model(USgas, \n                              methods = methods,\n                              train_method= list(partitions = 4, \n                                                 sample.out = 12, \n                                                 space = 3),\n                              error = \"MAPE\",\n                              horizon = 60)\n\n# A tibble: 6 × 7\n  model_id model   notes avg_mape avg_rmse `avg_coverage_80%` `avg_coverage_95%`\n  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;              &lt;dbl&gt;              &lt;dbl&gt;\n1 hw       HoltWi… Holt…   0.0355     111.              0.938              1    \n2 ets1     ets     ETS …   0.0468     149.              0.833              0.958\n3 arima2   arima   SARI…   0.0475     145.              0.646              0.917\n4 ets2     ets     ETS …   0.0555     165.              0.625              0.896\n5 tslm     tslm    tslm…   0.0892     253.              0.25               0.625\n6 arima1   arima   ARIM…   0.175      554.              0.854              0.979\n\n\nThe model provides a leaderboard that’s ordered based on the error criteria that’s set. In this case, the hw model had the lowest error rate, and therefore the function recommended that we use it (although all of the models and their forecasts are available for extraction from the function output)."
  },
  {
    "objectID": "time_forecast_ma.html#the-simple-moving-average",
    "href": "time_forecast_ma.html#the-simple-moving-average",
    "title": "38  Forecasting with Smoothing Methods (MA Models)",
    "section": "38.1 The Simple Moving Average",
    "text": "38.1 The Simple Moving Average\nThe simple moving average (\\(SMA\\)) function for smooth time series data can be utilized, with some simple steps, as a forecasting model. This method is recommended when the input series has no structural patterns, such as trend and seasonal components. In this case, it is reasonable to assume that the forecasted values are relatively close to the last observations of the series.\nHowever, there are a few popular methods for removing trends (de-trending) and removing seasonality (de-seasonalizing) from a series, such as regression models, advanced exponential smoothing methods, and the operation of differencing. The moving average can then be used to forecast such de-trended and de-seasonalized series, and then the trend and seasonality can be added back to the forecast.\nExample:\nIn the following example, we will create a customized \\(SMA\\) function and use it to forecast the monthly prices of the Robusta coffee prices in the next 12 months. The Robusta coffee prices (USD per kg) are an example of time series data that has no specific trend or seasonal patterns. Rather, this series has a cycle component, where the magnitude and length of the cycle keep changing from cycle to cycle.\n\nlibrary(TSstudio)\ndata(Coffee_Prices)\nts_info(Coffee_Prices)\n\n The Coffee_Prices series is a mts object with 2 variables and 701 observations\n Frequency: 12 \n Start time: 1960 1 \n End time: 2018 5 \n\n# Extract the monthly prices of the Robusta coffee\nrobusta &lt;- Coffee_Prices[,1]\n# Review the series\nts_plot(robusta,\n        title = \"The Robusta Coffee Monthly Prices\",\n        Ytitle = \"Price in USD\",\n        Xtitle = \"Year\")\n\n\n\n\n\nNext, we will create a basic \\(SMA\\) function using some of the functionality of the tidyr package:\n\nlibrary(tidyr)\nsma_forecast &lt;- function(df, h, m, w = NULL){\n  # Error handling\n  if(h &gt; nrow(df)){\n    stop(\"The length of the forecast horizon must be shorter than the \n         length of the series\")}\n  if(m &gt; nrow(df)){\n    stop(\"The length of the rolling window must be shorter than the length\n         of the series\")}\n  if(!is.null(w)){\n    if(length(w) != m){\n      stop(\"The weight argument is not aligned with the length of the\n           rolling window\")\n      } else if(sum(w) !=1){\n        stop(\"The sum of the average weight is different than 1\")\n  }}\n  \n# Setting the average weigths\n  if(is.null(w)){\n    w &lt;- rep(1/m, m)\n  }\n### Setting the data frame ###\n# Changing the Date object column name\n  names(df)[1] &lt;- \"date\"\n\n# Setting the training and testing partition\n# according to the forecast horizon\n  df$type &lt;- c(rep(\"train\", nrow(df) - h), rep(\"test\", h))\n# Spreading the table by the partition type\n  df1 &lt;- df %&gt;% spread(key = type, value = y)\n# Create the target variable\n  df1$yhat &lt;- df1$train\n# Simple moving average function\n  for(i in (nrow(df1) - h + 1):nrow(df1)){\n    r &lt;- (i-m):(i-1)\n  df1$yhat[i] &lt;- sum(df1$yhat[r] * w)\n  }\n# dropping from the yhat variable the actual values\n# that were used for the rolling window\n  df1$yhat &lt;- ifelse(is.na(df1$test), NA, df1$yhat)\n  df1$y &lt;- ifelse(is.na(df1$test), df1$train, df1$test)\n  return(df1)\n}\n\nThe function arguments are as follows:\n\ndf: The input series in a two-column data frame format, where the first column is a Date object and the second one is the actual values of the series.\nh: The horizon of the forecast. For the purpose of the following example, the function set the last \\(h\\) observations as a testing set. This allows us to compare model performance.\nm: The length of the rolling window.\nw: The weights of the average, by default, using equal weights (or arithmetic average).\n\nThe sma_forecast function has the following components:\n\nError handling: Test and verify whether the input arguments of the function are valid. If one of the defined tests isn’t true, it will stop the function from running and trigger an error message.\nData preparation: This defines the data.frame object based on the window length and the forecast horizon.\nData calculation: Calculates the simple moving average and return the results.\n\nNow, let’s utilize the sma_forecast function to demonstrate the performance of the \\(SMA\\) function. We will forecast the last 24 months of the Robusta series using a rolling window of 3, 6, 12, 24, and 36 months:\n\nrobusta_df &lt;- ts_to_prophet(robusta)\nrobusta_fc_m1 &lt;- sma_forecast(robusta_df, h = 24, m = 1)\nrobusta_fc_m6 &lt;- sma_forecast(robusta_df, h = 24, m = 6)\nrobusta_fc_m12 &lt;- sma_forecast(robusta_df, h = 24, m = 12)\nrobusta_fc_m24 &lt;- sma_forecast(robusta_df, h = 24, m = 24)\nrobusta_fc_m36 &lt;- sma_forecast(robusta_df, h = 24, m = 36)\n\nWe will use the plotly package to plot the results of the different moving average functions:\n\nlibrary(plotly)\nplot_ly(data = robusta_df[650:nrow(robusta_df),], x = ~ ds, y = ~ y,\n        type = \"scatter\", mode = \"lines\",\n        name = \"Actual\") %&gt;%\n  add_lines(x = robusta_fc_m1$date, y = robusta_fc_m1$yhat,\n            name = \"SMA - 1\", line = list(dash = \"dash\")) %&gt;%\n  add_lines(x = robusta_fc_m6$date, y = robusta_fc_m6$yhat,\n            name = \"SMA - 6\", line = list(dash = \"dash\")) %&gt;%\n  add_lines(x = robusta_fc_m12$date, y = robusta_fc_m12$yhat,\n            name = \"SMA - 12\", line = list(dash = \"dash\")) %&gt;%\n  add_lines(x = robusta_fc_m24$date, y = robusta_fc_m24$yhat,\n            name = \"SMA - 24\", line = list(dash = \"dash\")) %&gt;%\n  add_lines(x = robusta_fc_m36$date, y = robusta_fc_m36$yhat,\n            name = \"SMA - 36\", line = list(dash = \"dash\")) %&gt;%\n  layout(title = \"Forecasting the Robusta Coffee Monthly Prices\",\n         xaxis = list(title = \"\"),\n         yaxis = list(title = \"USD per Kg.\"))\n\n\n\n\n\nThe main observations from the preceding plot are as follows:\n\nIf the length of the rolling window is shorter:\n\nThe range of the forecast is fairly close to the most recent observations of the series.\nThe faster the forecast converges to some constant value.\n\nIf the window length is longer:\n\nThe longer it takes until the forecast converges to some constant value.\nIt can handle better shocks and outliers.\n\nAn \\(SMA\\) forecasting model with a rolling window of a length of 1 is equivalent to the Naïve forecasting model.\n\nNote that While the \\(SMA\\) function is fairly simple to use and cheap on compute power, it has some limitations:\n\nThe forecasting power of the \\(SMA\\) function is limited to a short horizon and may have poor performance in the long run.\nThis method is limited for time series data, with no trend or seasonal patterns. This mainly effects the arithmetic average that smooths the seasonal pattern and becomes flat in the long run."
  },
  {
    "objectID": "time_forecast_ma.html#weighted-moving-average",
    "href": "time_forecast_ma.html#weighted-moving-average",
    "title": "38  Forecasting with Smoothing Methods (MA Models)",
    "section": "38.2 Weighted Moving Average",
    "text": "38.2 Weighted Moving Average\nThe weighted moving average (WMA) is an extended version of the \\(SMA\\) function, and it is based on the use of the weighted average (as opposed to arithmetic average).\nThe main advantage of the \\(WMA\\) function, with respect to the \\(SMA\\) function, is that it allows you to distribute the weight of the lags on the rolling window. This can be useful when the series has a high correlation with some of its lags. The use of the \\(WMA\\) provides more flexibility as it can handle series with a seasonal pattern.\nExample:\nIn the following example, we will use the sma_forecast function we created previously to forecast the last 24 months of the USgas dataset. In this case, we will utilize the w argument to set the average weight and therefore transform the function from \\(SMA\\) to \\(WMA\\). Like we did previously, we will first transform the series into data.frame format with the ts_to_prophet function:\n\ndata(USgas)\nUSgas_df &lt;- ts_to_prophet(USgas)\n\nFor this example, we will use the following two strategies:\n\nThe \\(WMA\\) model for applying all the weight on the seasonal lag (lag 12):\n\nUSgas_fc_m12a &lt;- sma_forecast(USgas_df,\n                              h = 24,\n                              m = 12,\n                              w = c(1, rep(0,11)))\n\nThe \\(WMA\\) model for weighting the first lag with \\(0.2\\) and the seasonal lag (lag 12) with \\(0.8\\):\n\nUSgas_fc_m12b &lt;- sma_forecast(USgas_df,\n                              h = 24,\n                              m = 12,\n                              w = c(0.8, rep(0,10), 0.2))\n\n\nLet’s utilize the plotly package to plot the output of both \\(WMA\\) models:\n\nplot_ly(data = USgas_df[190:nrow(USgas_df),],\n        x = ~ ds,\n        y = ~ y,\n        type = \"scatter\",\n        mode = \"lines\",\n        name = \"Actual\") %&gt;%\n  add_lines(x = USgas_fc_m12a$date,\n            y = USgas_fc_m12a$yhat,\n            name = \"WMA - Seasonal Lag\",\n            line = list(dash = \"dash\")) %&gt;%\n  add_lines(x = USgas_fc_m12b$date,\n            y = USgas_fc_m12b$yhat,\n            name = \"WMA - 12 (0.2/0.8)\",\n            line = list(dash = \"dash\")) %&gt;%\n  layout(title = \"Forecasting the Monthly Consumption of Natural \n         Gas in the US\",\n         xaxis = list(title = \"\"),\n         yaxis = list(title = \"Billion Cubic Feet\"))\n\n\n\n\n\nAs you can see in the preceding plot, both models captured the seasonal oscillation of the series to some extent. Setting the full weight on the seasonal lag is equivalent to the seasonal Naïve model. This strategy could be useful for a series with a dominant seasonal pattern, such as \\(USgas\\).\nIn the second example, we weighted the average between the most recent lag and the seasonal lag. It would make sense to distribute the weights between the different lags when the series has a high correlation with those lags.\nWhile the \\(WMA\\) can capture the seasonal component of a series, it cannot capture the series trend (due to the average effect). Therefore, this method will start to lose its effectiveness once the forecast horizon crosses the length of the series frequency (for example, more than a year for monthly series)."
  },
  {
    "objectID": "time_forecast_ma.html#removing-trend-and-season-differencing",
    "href": "time_forecast_ma.html#removing-trend-and-season-differencing",
    "title": "38  Forecasting with Smoothing Methods (MA Models)",
    "section": "38.3 Removing Trend and Season (Differencing)",
    "text": "38.3 Removing Trend and Season (Differencing)\nA simple and popular method for removing a trend and/or a seasonal pattern from a series is by the operation of differencing.\nDifferencing means taking the difference between two values:\n\nA lag-1 difference (also called first difference) means taking the difference between every two consecutive values in the series (\\(y_t − y_{t−1}\\)). Lag-1 differencing is useful for removing a trend.\nDifferencing at lag-k means subtracting the value from \\(k\\) periods back (\\(y_t − y_{t−k}\\)). For example, for a daily series, lag-7 differencing means subtracting from each value (\\(y_t\\)) the value on the same day in the previous week (\\(y_{t−7}\\)).\n\n\n38.3.1 Removing trend (de-trending)\nLag-1 differencing is useful for removing a trend. For quadratic and exponential trends, often another round of lag-1 differencing must be applied in order to remove the trend. This means taking lag-1 differences of the differenced series.\nOne advantage of differencing over other methods (e.g., a regression with a trend) is that differencing does not assume that the trend is global.\n\n\n38.3.2 Removing seasonality (de-seasonalizing)\nFor removing a seasonal pattern with \\(M\\) seasons, we difference at \\(lag M\\). For example, to remove a day-of-week pattern in daily data, we can take lag- 7 differences. With a lag-12 differenced series the monthly pattern will no longer appears in a differenced series.\n\n\n38.3.3 Removing trend and seasonality\nWhen both trend and seasonality exist, we can apply differencing twice to the series in order to de-trend and deseasonalize it. For example, we can first apply differencing at lag-12, and then applying a lag-1 difference to the differenced series. The result is a series with no trend and no monthly seasonality in a monthly data series."
  },
  {
    "objectID": "time_forecast_exponential.html#simple-exponential-smoothing-model",
    "href": "time_forecast_exponential.html#simple-exponential-smoothing-model",
    "title": "39  Forecasting with Exponential Smoothing",
    "section": "39.1 Simple Exponential Smoothing Model",
    "text": "39.1 Simple Exponential Smoothing Model\nThe simple exponential smoothing (SES) is the simplest forecasting model among the exponential smoothing family. The main assumption of this model is that the series stays at the same level (that is, the local mean of the series is constant) over time, and therefore, this model is suitable for series with neither trend nor seasonal components.\nThe main attribute of the \\(SES\\) model is the weighted average technique, which is based on the exponential decay of the observation weights according to their chronological distance (i.e., series index or timestamp) from the first forecasted values. The decay rate of the observation weights is set by \\(\\alpha\\), a constant between 0 and 1 called the smoothing constant.\nWe can write the exponential smoother equation as a weighted average of all past observations, with exponentially decaying weights:\n\\[F_{t+1} = F_t + \\alpha \\epsilon_t\\] where \\(\\epsilon_t\\) is the forecast error (distance from the actual value) at time \\(t\\) and \\(F_t\\) is the previous forecast (Ft). If the forecast was too high in the last period, the next period is adjusted down. The amount of correction depends on the value of the smoothing constant \\(\\alpha\\).\nThe smoothing constant \\(\\alpha\\), which is set by the user, determines the rate of learning. A value close to \\(1\\) indicates fast learning (that is, only the most recent values influence the forecasts), whereas a value close to \\(0\\) indicates slow learning (past observations have a large influence on forecasts). Hence, the choice of \\(\\alpha\\) depends on the required amount of smoothing, and on how relevant the history is for generating forecasts. Default values that have been shown to work well are in the range \\(0.1-0.2\\). Trial and error can also help in the choice of \\(\\alpha\\).\nExample:\nThe following example demonstrates the decay of the weights of the observations on the most recent 15 observations, for values between 0.01 to 1:\n\nalpha_df &lt;- data.frame(index = seq(from = 1, to = 15, by = 1),\n                       power = seq(from = 14, to = 0, by = -1))\nalpha_df$alpha_0.01 &lt;- 0.01 * (1 - 0.01) ^ alpha_df$power\nalpha_df$alpha_0.2 &lt;- 0.2 * (1 - 0.2) ^ alpha_df$power\nalpha_df$alpha_0.4 &lt;- 0.4 * (1 - 0.4) ^ alpha_df$power\nalpha_df$alpha_0.6 &lt;- 0.6 * (1 - 0.6) ^ alpha_df$power\nalpha_df$alpha_0.8 &lt;- 0.8 * (1 - 0.8) ^ alpha_df$power\nalpha_df$alpha_1 &lt;- 1 * (1 - 1) ^ alpha_df$power\n\nLet’s plot the results:\n\nlibrary(plotly)\nplot_ly(data = alpha_df) %&gt;%\n  add_lines(x = ~ index, y = ~ alpha_0.01, name = \"alpha = 0.01\") %&gt;%\n  add_lines(x = ~ index, y = ~ alpha_0.2, name = \"alpha = 0.2\") %&gt;%\n  add_lines(x = ~ index, y = ~ alpha_0.4, name = \"alpha = 0.4\") %&gt;%\n  add_lines(x = ~ index, y = ~ alpha_0.6, name = \"alpha = 0.6\") %&gt;%\n  add_lines(x = ~ index, y = ~ alpha_0.8, name = \"alpha = 0.8\") %&gt;%\n  add_lines(x = ~ index, y = ~ alpha_1, name = \"alpha = 1\") %&gt;%\n  layout(title = \"Decay Rate of the SES Weights\",\n         xaxis = list(title = \"Index\"),\n         yaxis = list(title = \"Weight\"))\n\n\n\n\n\n\n39.1.1 Forecasting with the ses function\nThe forecast package provides a customized \\(SES\\) model with the ses function. The main arguments of this function are as follows:\n\ninitial: Defines the method for initializing the value of \\(\\hat Y_1\\) , which can be calculated by using the first few observations of the series by setting the argument to simple, or by estimating it with ets model (an advanced version of the Holt-Winters model from the forecast package) when setting it to optimal.\nalpha: Defines the value of the smoothing parameter of the model. If set to NULL, the function will estimate it.\nh: Sets the forecast horizon.\n\nLet’s use the ses function to forecast the monthly prices of the Robusta coffee data again. We will leave the last 12 months of the series as a testing set for benchmarking the model’s performance. We will do this using the ts_split function from the TSstudio package:\nExample:\n\n\n The Coffee_Prices series is a mts object with 2 variables and 701 observations\n Frequency: 12 \n Start time: 1960 1 \n End time: 2018 5 \n\n\n\nlibrary (TSstudio)\nrobusta_par &lt;- ts_split(robusta, sample.out = 12)\ntrain &lt;- robusta_par$train\ntest &lt;- robusta_par$test\n\nAfter we set the training and testing partition, we will use the training partition to train a \\(SES\\) model with the ses function:\n\nlibrary(forecast)\nfc_ses &lt;- ses(train, h = 12, initial = \"optimal\")\n# Review the model details\nfc_ses$model\n\nSimple exponential smoothing \n\nCall:\nses(y = train, h = 12, initial = \"optimal\")\n\n  Smoothing parameters:\n    alpha = 0.9999 \n\n  Initial states:\n    l = 0.6957 \n\n  sigma:  0.161\n\n     AIC     AICc      BIC \n1989.646 1989.681 2003.252 \n\n\nWe can review the model’s performance by using the test_forecast function:\nExample:\n\ntest_forecast(actual = robusta,\n              forecast.obj = fc_ses,\n              test = test) %&gt;%\n  layout(title = \"Robusta Coffee Prices Forecast vs. Actual\",\n         xaxis = list(range = c(2010, max(time(robusta)))),\n         yaxis = list(range = c(1, 3)))\n\n\n\n\n\nAs we can see from the preceding forecast plot, the ses function is utilizing the training set to identify the series level by estimating the alpha parameter and the initial level of the model (or \\(\\hat Y_i\\)). The forecast value level is fairly close to the value of the last observation of the series since the value, in this case, is close to \\(1\\). Since the goal of the \\(SE\\) model is to forecast the level of the series, the model won’t capture any short-term oscillation.\nIn the case of a flat forecast, the confidence intervals of the model play a critical role, since the level of uncertainty is higher. Therefore, it will be useful to evaluate whether the forecast values are within the model confidence interval bounds. We will use the plot_forecast function from the TSstudio package to create an interactive plot for the fs_ses model we created and plot the testing set:\n\nplot_forecast(fc_ses) %&gt;%\n  add_lines(x = time(test) + deltat(test),\n            y = as.numeric(test),\n            name = \"Testing Partition\") %&gt;%\n  layout(title = \"Robusta Coffee Prices Forecast vs. Actual\",\n         xaxis = list(range = c(2010, max(time(robusta)) +\n                                  deltat(robusta))),\n         yaxis = list(range = c(0, 4)))\n\n\n\n\n\nAs you can see from the preceding forecast plot, the testing set is within the range of the 80% confidence interval.\n\n\n39.1.2 Model optimization with grid search\nThe ses function optimizes the values of the model parameters (\\(\\alpha\\) and \\(l_0\\)) that minimize the sum of squared errors (SSE) of the model on the training set. An alternative optimization approach is to use a grid search.\nA grid search is a powerful approach that’s used to identify the values of the model’s parameters that minimize the model error. In the case of the \\(SES\\) model, we will apply a grid search to identify the optimal value of that minimizes some error metric of the model (for example, MAPE, RMSE, and so on).\nIn the following example, we will use a grid search to tune the model parameters \\(\\alpha\\) and \\(l_0\\), which minimize the model’s MAPE for the Robusta prices series. As we saw in the preceding performance plot of the Robusta forecast, there is a gap between the structure of the fitted values (marked in red) and the forecasted value (marked in green) since the \\(SES\\) model has a flat forecast. Therefore, we will split the model into training, testing, and validation partitions, and for each value of \\(\\alpha\\), we will apply the following steps:\n\nTrain the \\(SES\\) model using the training partition and forecast the observations of the testing partition.\nEvaluate the performance of the model on both the training and testing partition.\nAppend the training and testing partitions (in chronological order) and retrain the model on the new partition (training and testing) before forecasting the values of the validation partition.\nEvaluate the performance of the second model on the validation partition.\n\nExample:\n\n# Set the training, testing, and validation partitions\nrobusta_par1 &lt;- ts_split(robusta, sample.out = 24)\ntrain1 &lt;- robusta_par1$train\ntest1 &lt;- ts_split(robusta_par1$test, sample.out = 12)$train\nrobusta_par2 &lt;- ts_split(robusta, sample.out = 12)\ntrain2 &lt;- robusta_par2$train\nvalid &lt;- robusta_par2$test\n\nWe will now use the train1 and test1 variables for training and testing partition, and train2 for retraining the model and validating the results on the valid partition. The following \\(\\alpha\\) variable defines the search range. We will assign a sequence of values between \\(0\\) and \\(1\\) with an increment of \\(0.01\\) using the seq function:\nExample:\n\nalpha &lt;- seq(from = 0, to = 1, by = 0.01)\n\nSince the value of alpha must be greater than zero, we will replace \\(0\\) with a small number that’s fairly close to zero:\n\nalpha[1] &lt;- 0.001\n\nWe will use the lapply function to iterate on the model using the different values of \\(\\alpha\\):\n\nlibrary(dplyr)\nses_grid &lt;- lapply(alpha, function(i){\n  md1 &lt;- md_accuracy1 &lt;- md2 &lt;- md_accuracy2 &lt;- results &lt;- NULL\n  md1 &lt;- ses(train1, h = 12, alpha = i, initial = \"simple\")\n  md_accuracy1 &lt;- accuracy(md1, test1)\n  md2 &lt;- ses(train2, h = 12, alpha = i, initial = \"simple\")\n  md_accuracy2 &lt;- accuracy(md2, valid)\n  resutls &lt;- data.frame(alpha = i,\n                        train = md_accuracy1[9],\n                        test = md_accuracy1[10],\n                        valid = md_accuracy2[10])\n  }) %&gt;% \n  bind_rows()\n\nAs you can see in the following testing results, while \\(\\alpha = 1\\) minimizes the MAPE on the training partition, \\(\\alpha=0.03\\) minimizes the error rate on the testing partition. The results on the validation partition are following the same pattern as the testing partition, with an MAPE score of 9.98% on the testing partition and 6.60% on the validation partition:\n\nplot_ly(data = ses_grid, x = ~ alpha, y = ~ train,\n        line = list(color = 'rgb(205, 12, 24)'),\n        type = \"scatter\",\n        mode = \"lines\",\n        name = \"Training\") %&gt;%\n  add_lines(x = ~ alpha, y = ~ test, line = list(color = \"rgb(22, 96,\n                                                   167)\", dash = \"dash\"),\n              name= \"Testing\") %&gt;%\n  add_lines(x = ~ alpha, y = ~ valid, line = list(color = \"green\", dash = \"dot\"), name = \"Validation\") %&gt;%\n  layout(title = \"SES Model Grid Search Results\",\n         yaxis = list(title = \"MAPE (%)\"))"
  },
  {
    "objectID": "time_forecast_exponential.html#holt-method",
    "href": "time_forecast_exponential.html#holt-method",
    "title": "39  Forecasting with Exponential Smoothing",
    "section": "39.2 Holt method",
    "text": "39.2 Holt method\nThe Holt method, also known as the double exponential smoothing model, is an expanded version of the \\(SES\\) model. It’s based on estimating the most recent level and trend with the use of two smoothing parameters, \\(\\alpha\\) and \\(\\beta\\). Once the model estimates the most recent level and trend (\\(L_t\\), and \\(T_t\\), respectively), it utilizes them to construct the series forecast using the following equation:\n\nFor a series with additive trend, \\(\\hat{Y}_{T+h} = L_T + hT_T\\).\nFor a series with multiplicative trend, \\(\\hat{Y}_{T+1} = L_T \\times hT_T\\).\n\nThe forecast package provides an implementation of the Holt model with the holt function. This function automatically initializes the values of \\(\\hat L_1\\) and \\(\\hat T_1\\), and identifies the values of \\(\\alpha\\) and \\(\\beta\\) that minimize the \\(SSE\\) of the model on the training set.\nIn the following example, we will retrieve the US Gross Domestic Product (GDP) quarterly data from the Federal Reserve Economic Data (FRED) API using the Quandl package:\nExample:\n\ngdp &lt;- fredr(\n  series_id = \"GDP\",\n  observation_start = as.Date(\"2010-01-01\"),\n  observation_end = as.Date(\"2019-01-01\")\n  )\nhead(gdp)\n\n# A tibble: 6 × 5\n  date       series_id  value realtime_start realtime_end\n  &lt;date&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;date&gt;         &lt;date&gt;      \n1 2010-01-01 GDP       14765. 2024-10-07     2024-10-07  \n2 2010-04-01 GDP       14980. 2024-10-07     2024-10-07  \n3 2010-07-01 GDP       15142. 2024-10-07     2024-10-07  \n4 2010-10-01 GDP       15309. 2024-10-07     2024-10-07  \n5 2011-01-01 GDP       15351. 2024-10-07     2024-10-07  \n6 2011-04-01 GDP       15558. 2024-10-07     2024-10-07  \n\n# Convert df to ts\ngdp_ts &lt;- ts(data = gdp$value,\n             start = c(2010, 1),\n             frequency = 4\n             )\nstr(gdp_ts)\n\n Time-Series [1:37] from 2010 to 2019: 14765 14980 15142 15309 15351 ...\n\nts_info(gdp_ts)\n\n The gdp_ts series is a ts object with 1 variable and 37 observations\n Frequency: 4 \n Start time: 2010 1 \n End time: 2019 1 \n\n\nYou will notice in the following plot that the GDP series has a strong linear trend and no seasonal component (since the series is seasonally adjusted):\n\nts_plot(gdp_ts,\n        title = \"US Gross Domestic Product\",\n        Ytitle = \"Billions of Dollars\",\n        Xtitle = \"Source: U.S. Bureau of Economic Analysis /fred.stlouisfed.org\")\n\n\n\n\n\nLike we did previously, we will leave the last eight quarters for testing and train the model with the rest of the observations of the series with the holt function:\n\ngdp_par &lt;- ts_split(gdp_ts, sample.out = 8)\ntrain &lt;- gdp_par$train\ntest &lt;- gdp_par$test\nfc_holt &lt;- holt(train, h = 8, initial = \"optimal\")\n# Review the model's parameters\nfc_holt$model\n\nHolt's method \n\nCall:\nholt(y = train, h = 8, initial = \"optimal\")\n\n  Smoothing parameters:\n    alpha = 0.6836 \n    beta  = 1e-04 \n\n  Initial states:\n    l = 14642.6449 \n    b = 159.7415 \n\n  sigma:  76.3941\n\n     AIC     AICc      BIC \n354.8299 357.4386 361.6664 \n\n\nThe initialized values of \\(\\hat L_1\\) and \\(\\hat T_1\\) of the function are relatively close to the values of the first observation of the series (\\(Y_1 = 14642.64\\)) and the average difference between each quarter. In addition, the selected \\(\\alpha\\) of \\(0.68\\) indicated that the model heavily weighed the last observation of the series, \\(Y_t\\). On the other hand, the value of \\(\\beta\\) is fairly close to zero, which indicates that updating the trend value from period to period doesn’t take into account the change in the level and carries the initial value of the trend, \\(\\hat{T_1}\\), forward.\nLet’s compare the model’s performance in the training and testing partitions with the accuracy function:\n\naccuracy(fc_holt, test)\n\n                      ME      RMSE       MAE          MPE      MAPE       MASE\nTraining set  -0.4552329  70.93013  55.42122 -0.006041138 0.3248532 0.08781452\nTest set     378.4198543 430.06268 378.41985  1.832552431 1.8325524 0.59960352\n                   ACF1 Theil's U\nTraining set 0.04955818        NA\nTest set     0.63394683  1.810023\n\n\nAs you can see from the output of the accuracy function, the ratio between the error rate on the testing and training set is more than 6 times for the RMSE and 5.5 for the MAPE. This large ratio in the error metrics is mainly derived from the following two reasons:\n\nThe fitted values of the model on the training set are not bound by a linear line (as opposed to the forecast output).\nThe growth of the trend in the last few quarters shift from a linear rate of growth to an exponential rate.\n\nThe changes in the trend growth and the forecast can be observed with the test_forecast function:\n\ntest_forecast(gdp_ts, forecast.obj = fc_holt, test = test)\n\n\n\n\n\nWhile the Holt model was designed to handle time series with the linear trend, the exponential argument of the holt function provides the option to handle series with exponential or decaying trends when set to TRUE.\n\n# Use the exponential argument to modify the growth pattern of the trend\n# We seek a higher weight for the trend and we will set $\\beta$ to 0.75\nfc_holt_exp &lt;- holt(train,\n                    h = 8,\n                    beta = 0.75,\n                    initial = \"optimal\",\n                    exponential = TRUE)\n\nThe output of this model is as follows:\n\nfc_holt_exp$model\n\nHolt's method with exponential trend \n\nCall:\nholt(y = train, h = 8, initial = \"optimal\", exponential = TRUE, \n    beta = 0.75)\n\n  Smoothing parameters:\n    alpha = 0.75 \n    beta  = 0.75 \n\n  Initial states:\n    l = 14644.6809 \n    b = 1.0113 \n\n  sigma:  0.006\n\n     AIC     AICc      BIC \n369.7359 371.4026 375.2051 \n\n\nWe can review the model’s accuracy on the training and testing set with the accuracy function:\n\naccuracy(fc_holt_exp, test)\n\n                      ME      RMSE       MAE         MPE      MAPE      MASE\nTraining set  -0.6785595  95.90897  79.45979 -0.00608536 0.4676049 0.1259035\nTest set     158.0928706 194.12178 169.47521  0.76460898 0.8231642 0.2685322\n                   ACF1 Theil's U\nTraining set -0.2405994        NA\nTest set      0.5932521 0.8186658\n\n\nSimilarly, we can plot the fitted and forecasted values against the actual data with the test_forecast functions:\n\ntest_forecast(gdp_ts, forecast.obj = fc_holt_exp, test = test)\n\n\n\n\n\nAs you can see, the error rate of the second Holt model is more balanced, where the ration between the error on the testing and training set is \\(2\\) and \\(2.1\\) for the RMSE and MAPE metrics, respectively."
  },
  {
    "objectID": "time_forecast_exponential.html#holt-winters-model",
    "href": "time_forecast_exponential.html#holt-winters-model",
    "title": "39  Forecasting with Exponential Smoothing",
    "section": "39.3 Holt-Winters model",
    "text": "39.3 Holt-Winters model\nThe Holt-Winters model is the most advanced model among the exponential smoothing family of forecasting models. The Holt-Winters (HW) model is an extended version of the Holt model and can handle time series data with both trend and seasonal components. Forecasting the seasonal component requires a third smoother parameter and equation, in addition to the ones of the level and trend.\nBoth of the trend and seasonal components could have either an additive or multiplicative structure, which adds some complexity to the model as there are multiple possible combinations:\n\nAdditive trend and seasonal components.\nAdditive trend and multiplicative seasonal components.\nMultiplicative trend and additive seasonal components.\nMultiplicative trend and seasonal component.\n\nThe most common implementation of the HW model in R is the HoltWinters and hw functions from the stats and forecast packages. The main difference between the two functions is that the hw function can handle time series with an exponential or damped trend (similar to the Holt model).\nIn the following example, we will use the HoltWinters function to forecast the last 12 months of the USgas series.\nExample:\n\n# Diagnose the structure of the trend and seasonal components\ndata(USgas)\ndecompose(USgas) %&gt;% plot()\n\n\n\n\nWe can observe from the preceding plot that both the trend and seasonal components of the series have an additive structure. Like we did previously, we will create training and testing partitions using the last 12 months of the series to evaluate the performance of the model:\n\nUSgas_par &lt;- ts_split(USgas, 12)\ntrain &lt;- USgas_par$train\ntest &lt;- USgas_par$test\n\nNext, we will use the HoltWinters model to forecast the last 12 months of the series (or the testing set):\n\nmd_hw &lt;- HoltWinters(train)\n# Review the parameters of the trained model\nmd_hw\n\nHolt-Winters exponential smoothing with trend and additive seasonal component.\n\nCall:\nHoltWinters(x = train)\n\nSmoothing parameters:\n alpha: 0.371213\n beta : 0\n gamma: 0.4422456\n\nCoefficients:\n            [,1]\na   2491.9930104\nb     -0.1287005\ns1    32.0972651\ns2   597.1088003\ns3   834.9628439\ns4   353.8593860\ns5   318.1927058\ns6  -173.0721496\ns7  -346.6229990\ns8  -329.7169608\ns9  -112.1664217\ns10 -140.3186476\ns11 -324.5343787\ns12 -243.9334551\n\n\nYou will notice from the preceding model output that the model is mainly learning from the level and seasonal update (with \\(\\alpha = 0.37\\) and \\(\\gamma = 0.44\\) ). On the other hand, there is no learning from the trend initialized value \\(\\beta = 0\\).\nThe next step is to forecast the next 12 months (or the values of the testing set) and evaluate the model’s performance with the accuracy and test_forecast functions:\n\nfc_hw &lt;- forecast(md_hw, h = 12)\naccuracy(fc_hw, test)\n\n                    ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set  7.828361 115.2062 87.67420 0.2991952 4.248131 0.7614222\nTest set     51.013877 115.1555 98.06531 1.7994297 3.766099 0.8516656\n                    ACF1 Theil's U\nTraining set  0.21911103        NA\nTest set     -0.01991923 0.3652142\n\n\nThe accuracy metrics of the model are fairly balanced, with an \\(MAPE\\) of 4.2% in the training set and 3.8% in the testing set.\nIn the plot of the following model performance, you will notice that most of the forecast errors are related to the seasonal peak and the last observations of the series, which the model was underestimating:\n\ntest_forecast(actual = USgas,\n              forecast.obj = fc_hw,\n              test = test)\n\n\n\n\n\nPlotting the fitted and forecasted values provides us with a better understanding of the model’s performance. As we can see in the preceding plot, the HW model is doing a good job of capturing both the series seasonal patterns. On the other hand, the model is missing the peak of the year during the month of January in most cases.\nNote that alternatively, the model can be trained with a grid search in a similar manner to what we did with the \\(SES\\) model. In this case, there are three parameters to optimize: \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\). The TSstudio package provides a customized grid search function based on the backtesting approach for training a HoltWinters function."
  },
  {
    "objectID": "time_forecast_regression.html#the-linear-regression",
    "href": "time_forecast_regression.html#the-linear-regression",
    "title": "40  Forecasting with Linear Regression",
    "section": "40.1 The Linear Regression",
    "text": "40.1 The Linear Regression\nThe primary usage of the linear regression model is to quantify the relationship between the dependent variable \\(Y\\) (also known as the response variable) and the independent variable/s \\(X\\) (also known as the predictor, driver, or regressor variables) in a linear manner. In other words, the model expresses the dependent variable as a linear combination of the independent variables.\nA linear relationship between the dependent and independent variables can be generalized by the following equations:\n\nIn the case of a single independent variable, the equation is as follows:\n\n\n\\[Y_i = \\beta_0 + \\beta_1 * X_{1,i} + \\epsilon_i\\]\n\nHere we are modeling three of the four time series components: level (\\(\\beta_0\\)), trend (\\(\\beta_1\\)) and noise (\\(\\epsilon\\)). Seasonality is not modeled.\n\nFor n independent variables, the equation looks as follows:\n\n\n\\[Y_i = \\beta_0 + \\beta_1 * X_{1,i} + \\beta_2 * X_{2,i} + ... + \\beta_n * X_{n,i} + \\epsilon_i\\]\n\nThe term linear, in the context of regression, referred to the model coefficients, which must follow a linear structure (as this allows us to construct a linear combination from the independent variables). On the other hand, the independent variables can follow both a linear and nonlinear formation.\nA linear regression model provides an estimation for those coefficients (that is, \\(\\beta_0\\), \\(\\beta_1\\),…, \\(\\beta_n\\)), which can be formalized by the following equations:\n\nFor the univariate linear regression model, the equation is as follows:\n\n\n\\[\\hat{Y_i} = \\hat{\\beta_0} + \\hat{\\beta_1} X_{1,i}\\]\n\n\nFor the multivariate linear regression model, the equation is as follows:\n\n\n\\[\\hat{Y_i} = \\hat{\\beta_0} + \\hat{\\beta_1} X_{1,i} + \\hat{\\beta_2} X_{2,i} + ... + \\hat{\\beta_n} X_{n,i}\\]\n\n\n40.1.1 Estimation of coefficients\nThe estimation of the model’s coefficients is based on the following two steps:\n\nDefine a cost function (also known as loss function) —setting some error metric to minimize\nApply mathematical optimization for minimizing the cost function.\n\nThe most common estimation approach is applying the Ordinary Least Squares (OLS) method as an optimization technique for identifying the coefficients that minimize the residuals sum of squares:\n\\[\\sum_{i=1}^{N} \\hat{\\epsilon_i^2}\\]\nBesides the \\(OLS\\), there are other multiple estimation methods, such as the maximum likelihood, method of moments, and Bayesian."
  },
  {
    "objectID": "time_forecast_regression.html#forecasting-with-linear-regression",
    "href": "time_forecast_regression.html#forecasting-with-linear-regression",
    "title": "40  Forecasting with Linear Regression",
    "section": "40.2 Forecasting with Linear Regression",
    "text": "40.2 Forecasting with Linear Regression\nThe linear regression model, unlike the traditional time series models such as the \\(ARIMA\\) or \\(Holt-Winters\\), was not designed explicitly to handle and forecast time series data. Instead, it is a generic model with a wide range of applications from causal inference to predictive analysis. Therefore, forecasting with a linear regression model is mainly based on the following two steps:\n\nIdentifying the series structure, key characteristics, patterns, outliers, and other features.\nTransforming those features into input variables and regressing them with the series to create a forecasting model.\n\n\n40.2.1 Forecasting the trend and seasonal components\nFor the sake of simplicity, we can drop the cycle component as it typically merged into the trend component (or ignored the cycle component). We can now transform the (additive and multiplicative) time series decomposition equations for a linear regression model:\n\n\\[Y_t = \\beta_0 + \\beta_1 T_t + \\beta_2 S_t + \\epsilon_t\\]\n\nWhere:\n\n\\(Y_t\\) represents a time series with \\(n\\) observations.\n\\(T\\), an independent variable with \\(n\\) observations, represents the series trend component.\n\\(S_t\\), an independent variable with \\(n\\) observations, represents the series seasonal component.\n\\(\\epsilon_t\\), the regression error term, represents the irregular component or any pattern that is not captured by the series trend and seasonal component.\n\\(\\beta_0, \\beta_1, \\beta_2\\) represent the level (model intercept), and coefficients of the trend and seasonal components, respectively.\n\nFor the sake of convenience and in the context of working with time series, we will change the observations notation from \\(i\\) to \\(t\\), as it represents the time dimension of the series.\nThe transformation of a series with a multiplicative structure into a linear regression is done in two steps:\n\nThe transformation of the series by applying a log transformation for both sides of the equations of the series:\n\n\n\\[log(Y_t) = log(T_t) + log(S_t) + log(I_t)\\]\n\n\nOnce the series is transformed into an additive structure, the transformation to a linear regression formation:\n\n\n\\[log(Y_t) = \\beta_0 + \\beta_1 log(T_t) + \\beta_2 log(S_t) + \\epsilon_t\\]\n\n\n\n40.2.2 Features engineering of the series components\nBefore creating the regression inputs that represent the series trend and seasonal components, we first need to understand their structure.\nExample:\n\nlibrary(TSstudio)\ndata(USgas)\nts_plot(USgas,\n        title = \"US Monthly Natural Gas consumption\",\n        Ytitle = \"Billion Cubic Feet\",\n        Xtitle = \"Year\")\n\n\n\n\n\nAs you can see in the series plot, and as we saw on the previous chapters, USgas is a monthly series with a strong monthly seasonal component and fairly stable trend line.\n\nts_info(USgas)\n\n The USgas series is a ts object with 1 variable and 238 observations\n Frequency: 12 \n Start time: 2000 1 \n End time: 2019 10 \n\n\nWe can explore the series components structure with the ts_decompose function further:\n\nts_decompose(USgas)\n\n\n\n\n\nYou can see in the preceding plot that the trend of the series is fairly flat between 2000 and 2010, and has a fairly linear growth moving forward. Therefore, the overall trend between 2000 and 2018 is not strictly linear. This is an important insight that will help us to define the trend input for the regression model.\nNext we have to transform the series from a ts object to a data.frame object. Therefore, we will utilize the ts_to_prophet function from the TSstudio package:\n\nUSgas_df &lt;- ts_to_prophet(USgas)\nhead(USgas_df)\n\n          ds      y\n1 2000-01-01 2510.5\n2 2000-02-01 2330.7\n3 2000-03-01 2050.6\n4 2000-04-01 1783.3\n5 2000-05-01 1632.9\n6 2000-06-01 1513.1\n\n\nNow we can start to create the regression input features. The first feature we will create is the series trend. A basic approach for constructing the trend variable is by indexing the series observations in chronological order:\n\nUSgas_df$trend &lt;- 1:nrow(USgas_df)\n\nThe second feature we want to create is the seasonal component. Since we want to measure the contribution of each frequency unit to the oscillation of the series, we will use a categorical variable for each frequency unit. Therefore, we will create a categorical variable with 12 categories, each category corresponding to a specific month of the year in the USgas series.\n\nlibrary(lubridate)\nUSgas_df$seasonal &lt;- factor(month(USgas_df$ds, label = T), ordered = FALSE)\nhead(USgas_df)\n\n          ds      y trend seasonal\n1 2000-01-01 2510.5     1      ene\n2 2000-02-01 2330.7     2      feb\n3 2000-03-01 2050.6     3      mar\n4 2000-04-01 1783.3     4      abr\n5 2000-05-01 1632.9     5      may\n6 2000-06-01 1513.1     6      jun\n\n\nLast but not least, before we start to regress the series with those features, we will split the series into a training and testing partition. We will set the last 12 months of the series as a testing partition:\n\nh &lt;- 12 # setting a testing partition length\ntrain &lt;- USgas_df[1:(nrow(USgas_df) - h), ]\ntest &lt;- USgas_df[(nrow(USgas_df) - h + 1):nrow(USgas_df), ]\n\nNow we can proceed and review how the regression model captures each one of the components separately and all together.\n\n\n40.2.3 Modeling the series trend and seasonal components\nTREND COMPONENT\nWe will first model the series trend by regressing the series with the trend variable, on the training partition:\nExample:\n\nmd_trend &lt;- lm(y ~ trend, data = train)\nsummary(md_trend)\n\n\nCall:\nlm(formula = y ~ trend, data = train)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-547.2 -307.4 -153.2  333.1 1052.6 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1751.0074    52.6435   33.26  &lt; 2e-16 ***\ntrend          2.4489     0.4021    6.09 4.86e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 394.4 on 224 degrees of freedom\nMultiple R-squared:  0.1421,    Adjusted R-squared:  0.1382 \nF-statistic: 37.09 on 1 and 224 DF,  p-value: 4.861e-09\n\n\nAs you can see from the preceding regression output, the coefficient of the trend variable is statistically significant to a level of 0.001. However, the adjusted R-squared of the regression is fairly low, which generally makes sense, as most of the series variation of the series is related to the seasonal pattern as we saw in the plots previously.\nExplanation: As you can note from the preceding regression output, the fourth column represents the p-value of each one of the model coefficients. The p-value provides the probability that we will reject the null hypothesis given it is actually true, or the type I error. Therefore, for the p-value smaller than \\(\\alpha\\), the threshold value, we will reject the null hypothesis with a level of significance of \\(\\alpha\\), where typical values of \\(\\alpha\\) are 0.1, 0.05, 0.01, and so on.\nOur next step is to use the model we created to predict the fitted values on the training partition and the forecasted values on the testing partition.\n\ntrain$yhat &lt;- predict(md_trend, newdata = train)\ntest$yhat &lt;- predict(md_trend, newdata = test)\n\nWe will create a utility function that plots the series and the model output, utilizing the plotly package:\n\nlibrary(plotly)\nplot_lm &lt;- function(data, train, test, title = NULL){\n  p &lt;- plot_ly(data = data,\n               x = ~ ds,\n               y = ~ y,\n               type = \"scatter\",\n               mode = \"line\",\n               name = \"Actual\") %&gt;%\n    add_lines(x = ~ train$ds, \n              y = ~ train$yhat,\n              line = list(color = \"red\"),\n              name = \"Fitted\") %&gt;%\n    add_lines(x = ~ test$ds,\n              y = ~ test$yhat,\n              line = list(color = \"green\", dash = \"dot\", width = 3),\n              name = \"Forecasted\") %&gt;%\n    layout(title = title,\n           xaxis = list(title = \"Year\"),\n           yaxis = list(title = \"Billion Cubic Feet\"),\n           legend = list(x = 0.05, y = 0.95))\n  return(p)\n}\n\nLet’s set the inputs of the plot_lm function with the model output:\n\nplot_lm(data = USgas_df,\n        train = train,\n        test = test,\n        title = \"Predicting the Trend Component of the Series\")\n\n\n\n\n\nOverall, the model was able to capture the general movement of the trend, yet a linear trend may fail to capture the structural break of the trend that occurred around 2010.\nLast but not least, for comparison analysis, we want to measure the model error rate both in the training and the testing sets:\n\nmape_trend &lt;- c(mean(abs(train$y - train$yhat) / train$y),\nmean(abs(test$y - test$yhat) / test$y))\nmape_trend\n\n[1] 0.1644088 0.1299951\n\n\nSEASONAL COMPONENT\nThe process of modeling and forecasting the seasonal component follows the same process as we applied with the trend, by regressing the series with the seasonal variable we created before:\nExample:\n\nmd_seasonal &lt;- lm(y ~ seasonal, data = train)\nsummary(md_seasonal)\n\n\nCall:\nlm(formula = y ~ seasonal, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-608.98 -162.34  -50.77  148.40  566.89 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2774.28      49.75  55.759  &lt; 2e-16 ***\nseasonalfeb  -297.92      70.36  -4.234 3.41e-05 ***\nseasonalmar  -479.10      70.36  -6.809 9.77e-11 ***\nseasonalabr  -905.28      70.36 -12.866  &lt; 2e-16 ***\nseasonalmay -1088.42      70.36 -15.468  &lt; 2e-16 ***\nseasonaljun -1105.49      70.36 -15.711  &lt; 2e-16 ***\nseasonaljul  -939.35      70.36 -13.350  &lt; 2e-16 ***\nseasonalago  -914.12      70.36 -12.991  &lt; 2e-16 ***\nseasonalsep -1114.74      70.36 -15.843  &lt; 2e-16 ***\nseasonaloct -1022.21      70.36 -14.527  &lt; 2e-16 ***\nseasonalnov  -797.53      71.33 -11.180  &lt; 2e-16 ***\nseasonaldic  -256.67      71.33  -3.598 0.000398 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 216.9 on 214 degrees of freedom\nMultiple R-squared:  0.7521,    Adjusted R-squared:  0.7394 \nF-statistic: 59.04 on 11 and 214 DF,  p-value: &lt; 2.2e-16\n\n\nSince we regress the dependent variable with a categorical variable, the regression model creates coefficients for 11 out of the 12 categories, which are those embedded with the slope values. As you can see in the regression summary of the seasonal model, all the model’s coefficients are statistically significant. Also, you can notice that the adjusted R-squared of the seasonal model is somewhat higher with respect to the trend model (0.78 as opposed to 0.1).\nBefore we plot the fitted model and forecast values with the plot_lm function, we will update the values of yhat with the predict function:\n\ntrain$yhat &lt;- predict(md_seasonal, newdata = train)\ntest$yhat &lt;- predict(md_seasonal, newdata = test)\n\nNow we can use the plot_lm function to visualize the fitted model and forecast values:\n\nplot_lm(data = USgas_df,\n        train = train,\n        test = test,\n        title = \"Predicting the Seasonal Component of the Series\")\n\n\n\n\n\nAs you can see in the preceding plot, the model is doing a fairly good job of capturing the structure of the series seasonal pattern. However, you can observe that the series trend is missing. Before we add both the trend and the seasonal components, we will score the model performance:\n\nmape_seasonal &lt;- c(mean(abs(train$y - train$yhat) / train$y),\nmean(abs(test$y - test$yhat) / test$y))\nmape_seasonal\n\n[1] 0.08628973 0.21502100\n\n\nThe high error rate on the testing set is related to the trend component that was not included in the model.\nThe next step is to join the two components into one model and to forecast the feature values of the series:\n\nmd1 &lt;- lm(y ~ seasonal + trend, data = train)\nsummary(md1)\n\n\nCall:\nlm(formula = y ~ seasonal + trend, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-514.73  -77.17  -17.70   85.80  336.95 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2488.8994    32.6011  76.344  &lt; 2e-16 ***\nseasonalfeb  -300.5392    41.4864  -7.244 7.84e-12 ***\nseasonalmar  -484.3363    41.4870 -11.674  &lt; 2e-16 ***\nseasonalabr  -913.1334    41.4880 -22.010  &lt; 2e-16 ***\nseasonalmay -1098.8884    41.4895 -26.486  &lt; 2e-16 ***\nseasonaljun -1118.5855    41.4913 -26.960  &lt; 2e-16 ***\nseasonaljul  -955.0563    41.4936 -23.017  &lt; 2e-16 ***\nseasonalago  -932.4482    41.4962 -22.471  &lt; 2e-16 ***\nseasonalsep -1135.6874    41.4993 -27.366  &lt; 2e-16 ***\nseasonaloct -1045.7687    41.5028 -25.198  &lt; 2e-16 ***\nseasonalnov  -808.0016    42.0617 -19.210  &lt; 2e-16 ***\nseasonaldic  -269.7642    42.0635  -6.413 9.05e-10 ***\ntrend           2.6182     0.1305  20.065  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 127.9 on 213 degrees of freedom\nMultiple R-squared:  0.9142,    Adjusted R-squared:  0.9094 \nF-statistic: 189.2 on 12 and 213 DF,  p-value: &lt; 2.2e-16\n\n\nRegressing the series with both the trend and the seasonal components together provides additional lift to the adjusted R-squared of the model from 0.78 to 0.91. This can be seen in the plot of the model output:\n\ntrain$yhat &lt;- predict(md1, newdata = train)\ntest$yhat &lt;- predict(md1, newdata = test)\nplot_lm(data = USgas_df,\n        train = train,\n        test = test,\n        title = \"Predicting the Trend and Seasonal Components of the Series\")\n\n\n\n\n\nLet’s measure the model’s \\(MAPE\\) score on both the training and testing pPartitions:\n\nmape_md1 &lt;- c(mean(abs(train$y - train$yhat) / train$y),\nmean(abs(test$y - test$yhat) / test$y))\nmape_md1\n\n[1] 0.04774945 0.09143290\n\n\nRegressing the series with both the trend and the seasonal components provides a significant lift in both the quality of fit of the model and with the accuracy of the model.\nCAPTURING A NON-LINEAR TREND\nWhen looking at the plot of the model fit and forecast, you can notice that the model trend is too linear and missing the structural break of the series trend. This is the point where adding a polynomial component for the model could potentially provide additional improvement for the model accuracy.\nA simple technique to capture a non-linear trend is to add a polynomial component to the series trend in order to capture the trend curvature over time. We will use the I argument, which allows us to apply mathematical operations on any of the input objects. Therefore, we will use this argument to add a second degree of the polynomial for the trend input:\nExample:\n\nmd2 &lt;- lm(y ~ seasonal + trend + I(trend^2), data = train)\nsummary(md2)\n\n\nCall:\nlm(formula = y ~ seasonal + trend + I(trend^2), data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-468.47  -54.66   -2.21   63.11  294.32 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.635e+03  3.224e+01  81.738  &lt; 2e-16 ***\nseasonalfeb -3.004e+02  3.540e+01  -8.487 3.69e-15 ***\nseasonalmar -4.841e+02  3.540e+01 -13.676  &lt; 2e-16 ***\nseasonalabr -9.128e+02  3.540e+01 -25.787  &lt; 2e-16 ***\nseasonalmay -1.099e+03  3.540e+01 -31.033  &lt; 2e-16 ***\nseasonaljun -1.118e+03  3.540e+01 -31.588  &lt; 2e-16 ***\nseasonaljul -9.547e+02  3.540e+01 -26.968  &lt; 2e-16 ***\nseasonalago -9.322e+02  3.541e+01 -26.329  &lt; 2e-16 ***\nseasonalsep -1.136e+03  3.541e+01 -32.070  &lt; 2e-16 ***\nseasonaloct -1.046e+03  3.541e+01 -29.532  &lt; 2e-16 ***\nseasonalnov -8.001e+02  3.590e+01 -22.286  &lt; 2e-16 ***\nseasonaldic -2.618e+02  3.590e+01  -7.293 5.95e-12 ***\ntrend       -1.270e+00  4.472e-01  -2.840  0.00494 ** \nI(trend^2)   1.713e-02  1.908e-03   8.977  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 109.1 on 212 degrees of freedom\nMultiple R-squared:  0.9379,    Adjusted R-squared:  0.9341 \nF-statistic: 246.1 on 13 and 212 DF,  p-value: &lt; 2.2e-16\n\n\nAdding the second-degree polynomial to the regression model did not lead to a significant improvement of the goodness of fit of the model.\nOn the other model, as you can see in the following model output plot, this simple change in the model structure allows us to capture the structural break of the trend over time:\n\ntrain$yhat &lt;- predict(md2, newdata = train)\ntest$yhat &lt;- predict(md2, newdata = test)\nplot_lm(data = USgas_df,\n        train = train,\n        test = test,\n        title = \"Predicting the Trend (Polynomial) and Seasonal Components of the Series\")\n\n\n\n\n\nAs we can see from the model following the MAPE score, the model accuracy significantly improved from adding the polynomial trend to the regression model, as the error on the testing set dropped from 9.2% to 4.5%:\n\nmape_md2 &lt;- c(mean(abs(train$y - train$yhat) / train$y),\nmean(abs(test$y - test$yhat) / test$y))\nmape_md2\n\n[1] 0.03688770 0.04212618\n\n\n\n\n40.2.4 The tslm Function\nSo far, we have seen the manual process of transforming a ts object to a linear regression forecasting model format. The tslmfunction from the forecast package provides a built-in function for transforming a ts object into a linear regression forecasting model. Using the tslm function, you can set the regression component along with other features.\nWe will now repeat the previous example and forecast the last 12 observations of the \\(USgas\\) series with the tslm function using the trend, square of the trend, and the seasonal components. First, let’s split the series to training and testing partitions using the ts_split function:\nExample:\n\nUSgas_split &lt;- ts_split(USgas, sample.out = h)\ntrain.ts &lt;- USgas_split$train\ntest.ts &lt;- USgas_split$test\n\nNext, we will apply the same formula we used to create the preceding \\(md2\\) forecasting model using the tslm function:\n\nlibrary(forecast)\nmd3 &lt;- tslm(train.ts ~ season + trend + I(trend^2))\n\nLet’s now review \\(md3\\), the output of the tslm function, and compare it with the output of \\(md2\\):\n\nsummary(md3)\n\n\nCall:\ntslm(formula = train.ts ~ season + trend + I(trend^2))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-468.47  -54.66   -2.21   63.11  294.32 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.635e+03  3.224e+01  81.738  &lt; 2e-16 ***\nseason2     -3.004e+02  3.540e+01  -8.487 3.69e-15 ***\nseason3     -4.841e+02  3.540e+01 -13.676  &lt; 2e-16 ***\nseason4     -9.128e+02  3.540e+01 -25.787  &lt; 2e-16 ***\nseason5     -1.099e+03  3.540e+01 -31.033  &lt; 2e-16 ***\nseason6     -1.118e+03  3.540e+01 -31.588  &lt; 2e-16 ***\nseason7     -9.547e+02  3.540e+01 -26.968  &lt; 2e-16 ***\nseason8     -9.322e+02  3.541e+01 -26.329  &lt; 2e-16 ***\nseason9     -1.136e+03  3.541e+01 -32.070  &lt; 2e-16 ***\nseason10    -1.046e+03  3.541e+01 -29.532  &lt; 2e-16 ***\nseason11    -8.001e+02  3.590e+01 -22.286  &lt; 2e-16 ***\nseason12    -2.618e+02  3.590e+01  -7.293 5.95e-12 ***\ntrend       -1.270e+00  4.472e-01  -2.840  0.00494 ** \nI(trend^2)   1.713e-02  1.908e-03   8.977  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 109.1 on 212 degrees of freedom\nMultiple R-squared:  0.9379,    Adjusted R-squared:  0.9341 \nF-statistic: 246.1 on 13 and 212 DF,  p-value: &lt; 2.2e-16\n\n\nAs you can observe from the preceding output, both models (\\(md2\\) and \\(md3\\)) are identical.\nThere are several advantages to using the tslm function, as opposed to manually setting a regression model for the series with the lm function:\n\nEfficiency —does not require transforming the series to a data.frame object and feature engineering.\nThe output object supports all the functionality of the forecast (such as the accuracy and checkresiduals functions) and TSstudio packages (such as the test_forecast and plot_forecast functions).\n\n\n\n40.2.5 Forecasting a series with multiseasonality components\nWe will use the UKgrid series, which represents the national grid demand for electricity in the UK and it is available in the UKgrid package, to demonstrate the forecasting approach of a multiseasonality series with a linear regression model.\nExample:\n\nlibrary(UKgrid)\n# Set the series to daily frequency using the data.frame structure\nUKdaily &lt;- extract_grid(type = \"data.frame\",\n                        columns = \"ND\",\n                        aggregate = \"daily\")\nclass(UKdaily)\n\n[1] \"data.frame\"\n\nhead(UKdaily)\n\n   TIMESTAMP      ND\n1 2005-04-01 1920069\n2 2005-04-02 1674699\n3 2005-04-03 1631352\n4 2005-04-04 1916693\n5 2005-04-05 1952082\n6 2005-04-06 1964584\n\n\nAs you can see, this series has two variables:\n\n\\(TIMESTAMP\\): A date object used as the series timestamp or index.\n\\(ND\\): The net demand of electricity.\n\nWe will use the ts_plot function to plot and review the series structure:\n\nts_plot(UKdaily,\n        title = \"The UK National Demand for Electricity\",\n        Ytitle = \"MW\",\n        Xtitle = \"Year\")\n\n\n\n\n\nAs you can see in the preceding plot, the series has a clear downtrend and has multiple seasonality patterns:\n\nDaily: A cycle of 365 days a year.\nDay of the week: A 7-day cycle.\nMonthly: Effected from the weather.\n\nEvidence for those patterns can be seen in the following heatmap of the series since 2016 using the ts_heatmap function from the TSstudio package:\n\nts_heatmap(UKdaily[which(year(UKdaily$TIMESTAMP) &gt;= 2016),],\n           title = \"UK the Daily National Grid Demand Heatmap\")\n\n\n\n\n\nAs you can see in the series heatmap, the overall demand increases throughout the winter weeks (for example, calendar weeks 1 to 12 and weeks 44 to 52). In addition, you can observe the change of the series during the days of the weeks, as the demand increases during the working days of the week, and decreases over the weekend."
  },
  {
    "objectID": "time_forecast_regression.html#alternative-trend-shapes",
    "href": "time_forecast_regression.html#alternative-trend-shapes",
    "title": "40  Forecasting with Linear Regression",
    "section": "40.3 Alternative Trend Shapes",
    "text": "40.3 Alternative Trend Shapes\n\n40.3.1 Exponential Trend\nBesides linear trend, several alternative trend shapes can be fitted via a linear regression model. An exponential trend implies a multiplicative increase/decrease of the series over time.\nTo fit an exponential trend, simply replace the output variable y with \\(log(y)\\) and fit a linear regression:\n\\[log(y_t) = \\beta_0 + \\beta_1t + \\epsilon\\] Exponential trends are popular in sales data, where they reflect percentage growth.\n\n\n40.3.2 Polynomial Trend\nAnother nonlinear trend shape that is easy to fit via linear regression is a polynomial trend, and in particular, a **quadratic relationship of the form:\n\\[y_t = \\beta_0 + \\beta_1t + \\beta_2t^2 + \\epsilon\\]\nThis is done by creating an additional predictor \\(t^2\\) (the square of \\(t\\)) and fitting a multiple linear regression with the two predictors \\(t\\) and \\(t^2\\)."
  },
  {
    "objectID": "time_forecast_arima.html#the-stationary-process",
    "href": "time_forecast_arima.html#the-stationary-process",
    "title": "41  Forecasting with ARIMA Models",
    "section": "41.1 The Stationary Process",
    "text": "41.1 The Stationary Process\nOne of the main assumptions of the ARIMA family of models is that the input series follows the stationary process structure. Much of the probability theory of time series is concerned with stationary time series, and for this reason time series analysis often requires one to transform a non-stationary series into a stationary one so as to use this theory. This assumption is based on the Wold representation theorem, which states that any stationary process can be represented as a linear combination of white noise.\nTime series data is stationary if the following conditions take place:\n\nThe mean and variance of the series do not change over time.\nThe correlation structure of the series, along with its lags, remains the same over time.\n\nIn other words, the properties of one section of the data are much like those of any other section.\nThe arima.sim function from the stats package enables us to simulate a stationary and non-stationary time series data and plot it with the ts_plot function from the TSstudio package.\nThe arima.sim function allows us to simulate time series data based on the ARIMA model’s components and main characteristics:\n\nAn Autoregressive (AR) process: Establish a relationship between the series and its past \\(p\\) lags with the use of a regression model (between the series and its \\(p\\) lags).\nA Moving Average (MA) process: Similar to the AR process, the MA process establishes the relationship with the error term at time \\(t\\) and the past error terms, with the use of regression between the two components (error at time \\(t\\) and the past error terms).\nIntegrated (I) process: The process of differencing the series with its \\(d\\) lags to transform the series into a stationary state.\n\nHere, the model argument of the function defines \\(p\\), \\(q\\), and \\(d\\), as well as the order of the AR, MA, and I processes of the model.\n\n41.1.1 Stationary series\nIn the following example, we will simulate an AR process with one lag (that is, \\(p = 1\\)) and 500 observations with the arima.sim function. Before running the simulation, we will set the seed value to \\(12345\\):\nExample:\n\nset.seed(12345) \nstationary_ts &lt;- arima.sim(model = list(order = c(1,0,0), \n                                        ar = 0.5), \n                           n = 500)\n\nNow let’s plot the simulate time series with the ts_plot function:\nExample:\n\nlibrary(TSstudio)\nts_plot(stationary_ts,\n        title = \"Stationary Time Series\",\n        Ytitle = \"Value\",\n        Xtitle = \"Index\")\n\n\n\n\n\nIn this case, overall, the mean of the series, over time, remains around the zero line. In addition, the series’ variance does not change over time.\n\n\n41.1.2 Non-stationary series\nLet’s utilize the arima.sim function to create an example for non-stationary series:\nExample:\n\nset.seed(12345)\nnon_stationary_ts &lt;- arima.sim(model = list(order = c(1,1,0),\n                                            ar = 0.3),\n                               n = 500)\nts_plot(non_stationary_ts,\n        title = \"Non-Stationary Time Series\",\n        Ytitle = \"Value\",\n        Xtitle = \"Index\")\n\n\n\n\n\nAs you can see this example violates the stationary condition as it is trending over time, which means it is changing over time. Common examples of a series with a non-stationary structure are as follows:\n\nA series with a dominant trend: The series’ mean changes over time as a function of the change in the series trend, and therefore the series is non-stationary.\nA series with a multiplicative seasonal component: In this case, the variance of the series is a function of the seasonal oscillation over time, which either increases or decreases over time.\n\nThe AirPassenger series (the monthly airline passenger numbers between 1949 and 1960) from the datasets package is a good example of a series that violates the two conditions of the stationary process. Since the series has both a strong linear trend and a multiplicative seasonal component, the mean and variance are both changing over time:\nExample:\n\ndata(AirPassengers)\nts_plot(AirPassengers,\n        title = \"Monthly Airline Passenger Numbers 1949-1960\",\n        Ytitle = \"Thousands of Passengers\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n41.1.3 Transforming a non-stationary series into a stationary series\nIn most cases, unless you are very lucky, your raw data would probably come with a trend or other form of oscillation that violates the assumptions of the stationary process. To handle this, you will have to apply some transformation steps in order to bring the series into a stationary state.\nCommon transformations methods are:\n\nDifferencing the series (or de-trending).\nLog transformation\nor both.\n\nDIFFERENCING TIME SERIES\nThe most common approach to transforming a non-stationary time series data into a stationary state is by differencing the series with its lags. The main effect of differencing a series is the removal of the series trend (or detrending the series), which help to stabilize the mean of the series.\nIt is common to use seasonal differencing when a series has a seasonal component. The diff function from the base package differences the input series with a specific lag by setting the lag argument of the function to the relevant lag.\nExample:\n\n# First order difference\nts_plot(diff(AirPassengers, lag = 1),\n        title = \"AirPassengers Series - First Differencing\",\n        Xtitle = \"Year\",\n        Ytitle = \"Differencing of Thousands of Passengers\")\n\n\n\n\n\nYou can see that the first difference of the AirPassenger series removed the series trend and that the mean of the series is, overall, constant over time. On the other hand, there is clear evidence that the variation of the series is increasing over time, and therefore the series is not stationary yet.\nIn addition to the first order difference, taking the seasonal difference of the series could solve this issue. Let’s add the seasonal difference (double differencing) to the first order difference and plot it again:\nExample:\n\nts_plot(diff(diff(AirPassengers, lag = 1), 12),\n        title = \"AirPassengers Series - First and Seasonal Differencing\",\n        Xtitle = \"Year\",\n        Ytitle = \"Differencing of Thousands of Passengers\")\n\n\n\n\n\nThe seasonal difference did a good job of stabilizing the series variation, as the series now seems to be stationary.\nLOG TRANSFORMATION\nWe can utilize the log transformation approach to stabilize a multiplicative seasonal oscillation, if it exists. This approach is not a replacement for differencing, but an addition.\nFor instance, in the example of the AirPassenger in the preceding section, we saw that the first differencing is doing a great job in stabilizing the mean of the series, but is not sufficient enough to stabilize the variance of the series. Therefore, we can apply a log transformation to transform the seasonal structure from multiplicative to additive and then apply the first-order difference to stationarize the series:\nExample:\n\nts_plot(diff(log(AirPassengers), lag = 1),\n    title = \"AirPassengers Series - First Differencing with Log\n    Transformation\",\n    Xtitle = \"Year\",\n    Ytitle = \"Differencing/Log of Thousands of Passengers\")\n\n\n\n\n\nThe log transformation with the first-order differencing is doing a better job of transforming the series into a stationary state with respect to the double differencing (first-order with seasonal differencing) approach we used prior.\n\n\n41.1.4 The random walk process\nThe random walk, in the context of time series, describes a stochastic process of an object over time, where the main characteristics of this process are as follows:\n\nThe starting point of this process at time \\(0 - Y_0\\) is known\nThe movement (or the walk) of the series with random walk process from time \\(t-1\\) to time \\(t\\) are defined with the following equation:\n\n\n\\(Y_t = Y_t-1 + \\epsilon_t\\)\n\nThe following example demonstrates the simulation of 20 different random walk paths of 500 steps, all starting at point 0 at time 0. We will create two plots: one for the random walk paths and another for their first-order difference.\nExample:\n\nlibrary(plotly)\nset.seed(12345)\np1 &lt;- plot_ly()\np2 &lt;- plot_ly()\nfor(i in 1:20){\n  rm &lt;- NULL\n  rw &lt;- arima.sim(model = list(order = c(0, 1, 0)), n = 500)\n  p1 &lt;- p1 %&gt;% add_lines(x = time(rw), y = as.numeric(rw))\n  p2 &lt;- p2 %&gt;% add_lines(x = time(diff(rw)), y = as.numeric(diff(rw)))\n  }\n\nHere, \\(p1\\) represents the plot of the random walk simulation:\n\np1 %&gt;% layout(title = \"Simulate Random Walk\",\n              yaxis = list(title = \"Value\"),\n              xaxis = list(title = \"Index\")) %&gt;%\n  hide_legend()\n\n\n\n\n\nHere, \\(p2\\) represents the corresponding plot of the first-order differencing of the random walk simulation:\n\np2 %&gt;% layout(title = \"Simulate Random Walk with First-Order Differencing\",\n              yaxis = list(title = \"Value\"),\n              xaxis = list(title = \"Index\")) %&gt;%\n  hide_legend()"
  },
  {
    "objectID": "time_forecast_arima.html#the-ar-process",
    "href": "time_forecast_arima.html#the-ar-process",
    "title": "41  Forecasting with ARIMA Models",
    "section": "41.2 The AR Process",
    "text": "41.2 The AR Process\nThe \\(AR\\) process defines the current value of the series, \\(Y_t\\), as a linear combination of the previous \\(p\\) lags of the series, and can be formalized with the following equation:\n\\[AR(p): Y_t = c + \\sum^{p}_{i=1} \\phi_i Y_{t-i} + \\epsilon_t\\]\nWhere:\n\n\\(AR(p)\\) is the notation for an \\(AR\\) process with p-order.\n\\(c\\) represents a constant (or drift).\n\\(p\\) defines the number of lags to regress against \\(Y_t\\).\n\\(\\phi_i\\) is the coefficient of the \\(i\\) lag of the series (here, must be between -1 and 1, otherwise, the series would be trending up or down and therefore cannot be stationary over time).\n\\(Y_{t-i}\\) is the \\(i\\) lag of the series.\n\\(\\epsilon_t\\) represents the error term, which is white noise.\n\nAn AR process can be used on time series data if, and only if, the series is stationary. Therefore, before applying an \\(AR\\) process on a series, you will have to verify that the series is stationary. Otherwise, you will have to apply some transformation method (such as differencing, log transformation, and so on) to transform the series into a stationary state.\n\n41.2.1 Example of AR process\n\n\n\n\n\n\n\n\nTask\nFunction\nVariable\n\n\n\n\nSimulate and plot a AR(2) series\narima.sim\nts_plot\nar2\n\n\nModel (fit) a AR(2)\nar(ar2)\nmd_ar\n\n\nIdentify the AR(2) process - acf output tails off and pacfcuts off at lag \\(p\\)\nacf(ar2)\npacf(ar2)\nN/A\n\n\nForecast\nforecast(md_ar)\nar_fc\n\n\n\n\nIn the following example, we will utilize the arima.sim function again to simulate an \\(AR(2)\\) process structure time series data with 500 observations, and then use it to fit an \\(AR\\) model. We will use the model argument to set the \\(AR\\) order to \\(2\\) and set the lags coefficients \\(\\phi_1 = 0.9\\) and \\(\\phi_2 = -0.3\\):\n\nExample:\n\nset.seed(12345)\nar2 &lt;- arima.sim(model = list(order = c(2,0,0),\n                              ar = c(0.9, -0.3)),\n                 n = 500)\n\nLet’s review the simulate time series:\n\nts_plot(ar2,\n        title = \"Simulate AR(2) Series\",\n        Ytitle = \"Value\",\n        Xtitle = \"Index\")\n\n\n\n\n\n\nThe ar function from the stats package allows us to fit an \\(AR\\) model on time series data and then forecast its future values. This function identifies the \\(AR\\) order automatically based on the Akaike Information Criterion (AIC). The method argument allows you to define the coefficients estimation method, such as the ordinary least squares (OLS), maximum likelihood estimation (MLE), and Yule-Walker (default).\n\nLet’s apply the ar function to identify the \\(AR\\) order and estimate its coefficients accordingly:\nExample:\n\nmd_ar &lt;- ar(ar2)\nmd_ar\n\n\nCall:\nar(x = ar2)\n\nCoefficients:\n      1        2  \n 0.8851  -0.2900  \n\nOrder selected 2  sigma^2 estimated as  1.049\n\n\nAs you can see the ar function was able to identify that the input series is a second order AR process, and provided a fairly close estimate for the value of the actual coefficients, \\(\\hat\\phi_1 = 0.88\\), \\(\\hat\\phi_2 = -0.29\\) (as opposed to the actual coefficients’ values, \\(\\phi_1 = 0.9\\), \\(\\phi_2 = -0.3\\)).\n\nForecast the AR model\n\nThe following code demonstrates the forecast of the next 100 observations of the \\(AR\\) model we trained previously with the ar function:\nExample:\n\nlibrary(forecast)\nar_fc &lt;- forecast(md_ar, h = 100)\n# Use `plot_forecast to plot the forecast output:\nplot_forecast(ar_fc,\n              title = \"Forecast AR(2) Model\",\n              Ytitle = \"Value\",\n              Xtitle = \"Year\")\n\n\n\n\n\n\n\n41.2.2 Identifying the AR process and its characteristics\nIn the preceding example, we simulated an \\(AR(2)\\) series, and it was clear that we need to apply an \\(AR\\) model on the data. However, when working with real-time series data, you will have to identify the structure of the series before modeling it.\nIn the world of the non-seasonal ARIMA family of models, a series could have one of the following structures:\n\n\\(AR\\).\n\\(MA\\).\nRandom walk.\nA combination of the preceding three (for example, \\(AR\\) and \\(MA\\) processes).\n\nIdentifying the series structure includes the following two steps:\n\nCategorizing the type of process (for example, \\(AR\\), \\(MA\\), and so on).\nOnce we have classified the process type, we need to identify the order of the process (for example, \\(AR(1)\\), \\(AR(2)\\), and so on).\n\nUtilizing the autocorrelation function (ACF) and partial autocorrelation function (PACF), allows us to classify the process type and identify its order.\nIf the ACF output tails off and the PACF output cuts off at lag \\(p\\), this indicates that the series is an \\(AR(p)\\) process.\nExample:\n\n# Use the `par` function to plot the two plots side by side\npar(mfrow=c(1,2))\n# Generate the plots with the acf and pacf functions\nacf(ar2)\npacf(ar2)\n\n\n\n\nIn the case of the ar2 series, you can see that the ACF plot is tailing off and that the PACF plot is cut off at the second lag. Therefore, we can conclude that the series has a second order AR process."
  },
  {
    "objectID": "time_forecast_arima.html#the-ma-process",
    "href": "time_forecast_arima.html#the-ma-process",
    "title": "41  Forecasting with ARIMA Models",
    "section": "41.3 The MA Process",
    "text": "41.3 The MA Process\nIn some cases, the forecasting model is unable to capture all the series patterns, and therefore some information is left over in model residuals (or forecasting error). The goal of the moving average process is to capture patterns in the residuals, if they exist, by modeling the relationship between \\(Y_t\\), the error term, \\(\\epsilon_t\\), and the past \\(q\\) error terms of the models (for example, \\(\\epsilon_{t-1}\\), \\(\\epsilon_{t-2}\\), \\(\\epsilon_{t-q}\\)).\nThe structure of the MA process is fairly similar to the ones of the \\(AR\\). The following equation defines an \\(MA\\) process with a \\(q\\) order:\n\\[MA(q): Y_t = \\mu + \\epsilon_t +  \\sum^{q}_{i=1} \\theta_i \\epsilon_{t-i}\\] Where:\n\n\\(MA(q)\\) is the notation for an \\(MA\\) process with q-order.\n\\(\\mu\\) represents the mean of the series.\n\\(\\epsilon_{t-q}\\),…, \\(\\epsilon_t\\) are white noise error terms.\n\\(\\theta_i\\) is the corresponding coefficient of \\(\\epsilon_{t-i}\\)\n\\(q\\) defines the number of past error terms to be used in the equation.\n\nLike the \\(AR\\) process, the MA equation holds only if the series is a stationary process; otherwise, a transformation must be used on the series before applying the \\(MA\\) process.\n\n41.3.1 Example of MA process\n\n\n\n\n\n\n\n\nTask\nFunction\nVariable\n\n\n\n\nSimulate and plot a MA(2) series\narima.sim\nts_plot\nma2\n\n\nModel (fit) a MA(2)\narima\nmd_ma\n\n\nIdentify the MA process - acfis cut off at lag \\(q\\) and pacf tails off\nacf(ma2)\nccf(ma2)\nN/A\n\n\nForecast the MA model\nforecast(md_ma)\nma_fc\n\n\n\n\nWe will utilize the arima.sim function to simulate a series with an \\(MA(2)\\) structure. In this case, we will set the \\(q\\) parameter in the order argument to \\(2\\) and set the \\(MA\\) coefficients to \\(0.5\\) and \\(-0.3\\):\n\nExample:\n\nset.seed(12345)\nma2 &lt;- arima.sim(model = list(order = c(0, 0, 2),\n                              ma = c(0.5, -0.3)),\n                 n = 500)\n# Use the ts_plot function to plot the simulated series\nts_plot(ma2,\n        title = \"Simulate MA(2) Series\",\n        Ytitle = \"Value\",\n        Xtitle = \"Index\")\n\n\n\n\n\n\nModeling the MA process can be done with the arima function from the stats package. This function, when setting the order of the \\(AR\\) and the differencing components of the model to \\(0\\) with the order argument (that is, \\(p = 0\\) and \\(d = 0\\)), is modeling only on the MA component.\n\nExample:\n\n# Apply a second-order MA model with the arima function\n# on the simulated MA(2) series\nmd_ma &lt;- arima(ma2, order = c(0,0,2), method = \"ML\")\n\nSimilar to the ar function, you can select the coefficients estimation approach. In this case, there are three methods: maximum likelihood (ML), minimize conditional sum-of-squares (CSS), and the combination of the two, which is known as CSS-ML.\nThe output of the arima function is more detailed than the ones of the ar function, as it also provides the level of significance of each coefficient (the s.e.):\n\nmd_ma\n\n\nCall:\narima(x = ma2, order = c(0, 0, 2), method = \"ML\")\n\nCoefficients:\n        ma1      ma2  intercept\n      0.530  -0.3454     0.0875\ns.e.  0.041   0.0406     0.0525\n\nsigma^2 estimated as 0.9829:  log likelihood = -705.81,  aic = 1419.62\n\n\n\n\n41.3.2 Identifying the MA process and its characteristics\nSimilar to the $AR4 process, we can identify an MA process and its order with the \\(ACF\\) and \\(PACF\\) functions. If the ACF is cut off at lag \\(q\\) and the PACF function tails off, we can conclude that the process is an \\(MA(q)\\). Let’s repeat the process we applied on the ar2 series with the ma2 series:\nExample:\n\npar(mfrow=c(1,2))\nacf(ma2)\npacf(ma2)\n\n\n\n\nIn the case of the ma2 series, the \\(ACF\\) plot is cut off on the second lag (note that lag \\(0\\) is the correlation of the series with itself, and therefore it is equal to \\(1\\) and we can ignore it), and so the \\(PACF\\) tails off. Therefore, we can conclude that the ma2 series is an \\(MA(2)\\) process."
  },
  {
    "objectID": "time_forecast_arima.html#the-arma-model",
    "href": "time_forecast_arima.html#the-arma-model",
    "title": "41  Forecasting with ARIMA Models",
    "section": "41.4 The ARMA model",
    "text": "41.4 The ARMA model\nUp until now, we have seen how the applications of \\(AR\\) and \\(MA\\) are processed separately. However, in some cases, combining the two allows us to handle more complex time series data. The ARMA model is a combination of the \\(AR(p)\\) and \\(MA(q)\\) processes and can be written as follows:\n\\[ARMA(p,q): Y_t = c + \\sum^{p}_{i=1} \\phi_i Y_{t-i} + \\sum^{q}_{i=1} \\theta_i \\epsilon_{t-i} + \\epsilon_t\\] For instance, an ARMA(3,2) model is defined by the following equation:\n\\[Y_t = c + \\phi_1 Y_{t-i} + \\phi_2 Y_{t-2} + \\phi_3 Y_{t-3} + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\epsilon_t\\]\n\n41.4.1 Example of ARMA process\n\n\n\n\n\n\n\n\nTask\nFunction\nVariable\n\n\n\n\nSimulate and plot a ARMA(1,2) series\narima.sim\narma\n\n\nModel (fit) a ARMA(1,2)\narima\narma.md\n\n\nIdentify the ARMA process - acfis cut off at lag \\(q\\) and pacf tails offIdentify the MA process - both acf and pacf tails off\nacf\nccf\nN/A\n\n\nForecast the ARMA model\nforecast(arma.md)\narma_fc\n\n\n\n\nLet’s simulate a time series data with an ARMA(1,2) structure with the arima.sim function and review the characteristics of the \\(ARMA\\) model. We will set the \\(p\\) and \\(q\\) parameters of the order argument to \\(1\\) and \\(2\\), respectively, and set the \\(AR\\) coefficient as \\(0.7\\), and the \\(MA\\) coefficients as \\(0.5\\) and \\(-0.3\\):\n\nExample:\n\nset.seed(12345)\narma &lt;- arima.sim(model = list(order(1,0,2),\n                               ar = c(0.7),\n                               ma = c(0.5,-0.3)),\n                  n = 500)\n# Plot and review the series structure\nts_plot(arma,\n        title = \"Simulate ARMA(1,2) Series\",\n        Ytitle = \"Value\",\n        Xtitle = \"Index\")\n\n\n\n\n\n\nFitting an ARMA model is straightforward with the arima function. In this case, we have to set the \\(p\\) and \\(q\\) parameters on the order argument:\n\n\narma_md &lt;- arima(arma, order = c(1,0,2))\narma_md\n\n\nCall:\narima(x = arma, order = c(1, 0, 2))\n\nCoefficients:\n         ar1     ma1      ma2  intercept\n      0.7439  0.4785  -0.3954     0.2853\ns.e.  0.0657  0.0878   0.0849     0.1891\n\nsigma^2 estimated as 1.01:  log likelihood = -713.18,  aic = 1436.36\n\n\nYou can observe from the following output of the fitted model that the values of the model coefficients are fairly close to the one we simulated.\nHere, as the coefficient’s name implies, ar1, ma1, and ma2 represent the estimation of the \\(\\phi_1\\), \\(\\theta_1\\), and \\(\\theta_2\\) coefficients, respectively. You can note that the intercept parameter is not statistically significant, which should make sense as we didn’t add an intercept to the simulated data.\n\n\n41.4.2 Identifying an ARMA process\nIdentifying the \\(ARMA\\) process follows the same approach that we used previously with the \\(AR\\) and \\(MA\\) processes. An ARMA process exists in time series data if both the ACF and PACF plots tail off, as we can see in the following example:\nExample:\n\npar(mfrow=c(1,2))\nacf(arma)\npacf(arma)\n\n\n\n\nOn the other hand, unlike the \\(AR\\) and \\(MA\\) processes, we cannot conclude the order of the ARMA process. There are several approaches for tuning the ARMA \\(p\\) and \\(q\\) parameters:\n\nManual tuning: By starting with a combination of \\(p\\) and \\(q\\) and using some error criteria for identifying the model parameters.\nGrid search: By trying different combinations of the \\(p\\) and \\(q\\) parameters based on the grid matrix. Likewise, manual tuning and the selection of a specific combination of \\(p\\) and \\(q\\) parameters should be based on error criterion.\nAlgorithm-based search: By using a function or algorithm for tuning the model parameters."
  },
  {
    "objectID": "time_forecast_arima.html#the-arima-model",
    "href": "time_forecast_arima.html#the-arima-model",
    "title": "41  Forecasting with ARIMA Models",
    "section": "41.5 The ARIMA model",
    "text": "41.5 The ARIMA model\nOne of the limitations of the \\(AR\\), \\(MA\\), and \\(ARMA\\) models is that they cannot handle non-stationary time series data. Therefore, if the input series is non-stationary, a pre-processing step is required to transform the series from a non-stationary state into a stationary state.\nThe ARIMA model provides a solution for this issue by adding the integrated process for the \\(ARMA\\) model. The Integrated (I) process is simply differencing the series with its lags, where the degree of the differencing is represented by the \\(d\\) parameter.\nThe differencing process is one of the ways you can transform the methods of a series from non-stationary to stationary. For instance, \\(Y_t - Y_{t-1}\\) represents the first differencing of the series, while \\((Y_t - Y_{t-1}) - (Y_{t-1} - Y_{t-2})\\) represents the second differencing.\nWe can generalize the differencing process with the following equation:\n\\[Y_d = (Y_t - Y_{t-1}) - ... - (Y_{t-d+1} - Y_{t-d})\\]\nWhere, \\(Y_d\\) is the \\(d\\) differencing of the series.\nLet’s add the differencing component to the \\(ARMA\\) model and formalize the \\(ARIMA(p,d,q)\\) process with a p-order \\(AR\\) process, d-degree of differencing, and q-order \\(MA\\) process:\n\\[ARIMA(p,q,d): Y_d = c + \\sum^{p}_{i=1} \\phi_i Y_{d-i} + \\sum^{q}_{i=1} \\theta_i \\epsilon_{t-i} + \\epsilon_t\\]\nNote that both the \\(AR\\) and \\(MA\\) models can be represented with the \\(ARMA\\) model, and that you can also represent the \\(AR\\), \\(MA\\), or \\(ARMA\\) models with the \\(ARIMA\\) model, for example:\n\nThe ARIMA(0, 0, 0) model is equivalent to white noise.\nThe ARIMA(0, 1, 0) model is equivalent to a random walk.\nThe ARIMA(1, 0, 0) model is equivalent to an AR(1) process.\nThe ARIMA(0, 0, 1) model is equivalent to an MA(1) process.\nThe ARIMA(1, 0, 1) model is equivalent to an ARMA(1,1) process.\n\n\n41.5.1 Identifying an ARIMA process\nIdentifying and setting the \\(ARIMA model\\) is a two-step process based on the following steps:\n\nIdentify the degree of differencing that is required to transfer the series into a stationary state.\nIdentify the ARMA process (or AR and MA processes), as introduced in the previous section.\n\nBased on the findings of these steps, we will set the model parameters \\(p\\), \\(d\\), and \\(q\\).\nSimilar to the setting the \\(d\\) parameter (the degree of differencing of the series) can be done with the ACF and PACF plots. In the following example, we will use the monthly prices of Robusta coffee since 2000.\n\n\n41.5.2 Example of ARIMA process\n\n\n\n\n\n\n\n\nTask\nFunction\nVariable\n\n\n\n\nPlot the series\nts_plot (robusta_price)\nrobusta_price\n\n\nIdentify the relationship between the series and its lags\nacf(robusta_price)\nN/A\n\n\nFirst differencing the series\ndiff(robusta_price)\nrobusta_price_d1\n\n\nDetermine the order of the model\nacf(robusta_price_d1) pacf(robusta_price_d1)\nN/A\n\n\nFit the model ARIMA\narima(robusta_price)\nrobusta_md\n\n\nCheck the residuals\ncheckresiduals(robusta_md)\n\n\n\n\n\nWe first start the \\(ARIMA\\) process by extracting the data set and visually analyzing its structure with a plot of the series.\n\nExample:\n\ndata(\"Coffee_Prices\")\nrobusta_price &lt;- window(Coffee_Prices[,1], start = c(2000, 1))\nts_plot(robusta_price,\n        title = \"The Robusta Coffee Monthly Prices\",\n        Ytitle = \"Price in USD\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\nAs you can see, the \\(Robusta\\) coffee prices over time are trending up, and therefore it is not in a stationary state. In addition, since this series represents continues prices, it is likely that the series has a strong correlation relationship with its past lags (as changes in price are typically close to the previous price).\n\n\n# Identify the type of relationship between the series and it lags\nacf(robusta_price)\n\n\n\n\n\nAs you can see in the preceding output of the ACF plot, the correlation of the series with its lags is slowly decaying over time in a linear manner. Removing both the series trend and correlation between the series and its lags can be done by differencing the series.\n\n\n# We will start with the first differencing using the diff function\nrobusta_price_d1 &lt;- diff(robusta_price)\n\n\nLet’s review the first difference of the series with the acf and pacf functions:\n\n\npar(mfrow=c(1,2))\nacf(robusta_price_d1)\npacf(robusta_price_d1)\n\n\n\n\n\nThe \\(ACF\\) and \\(PACF\\) plots of the first difference of the series indicate that an \\(AR(1)\\) process is appropriate to use on the differenced series since the \\(ACF\\) is tailing off and the PACF cuts on the first lag. Therefore, we will apply an \\(ARIMA(1,1,0)\\) model on the robusta_price series to include the first difference:\n\n\nrobusta_md &lt;- arima(robusta_price, order = c(1, 1, 0))\n# Use the summary function to review the model details\nsummary(robusta_md)\n\n\nCall:\narima(x = robusta_price, order = c(1, 1, 0))\n\nCoefficients:\n         ar1\n      0.2780\ns.e.  0.0647\n\nsigma^2 estimated as 0.007142:  log likelihood = 231.38,  aic = -458.76\n\nTraining set error measures:\n                      ME       RMSE        MAE        MPE     MAPE     MASE\nTraining set 0.002595604 0.08432096 0.06494772 0.08104715 4.254984 1.001542\n                    ACF1\nTraining set 0.001526295\n\n\n\nYou can see from the model summary output that the ar1 coefficient is statistically significant. Last but not least, we will check the model residuals:\n\n\ncheckresiduals(robusta_md)\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,1,0)\nQ* = 26.896, df = 23, p-value = 0.2604\n\nModel df: 1.   Total lags used: 24\n\n\nOverall, the plot of the model’s residuals and the Ljung-Box test indicate that the residuals are white noise. The \\(ACF\\) plot indicates that there are some correlated lags, but they are only on the border of being significant and so we can ignore them."
  },
  {
    "objectID": "time_forecast_arima.html#the-seasonal-arima-model-sarima",
    "href": "time_forecast_arima.html#the-seasonal-arima-model-sarima",
    "title": "41  Forecasting with ARIMA Models",
    "section": "41.6 The Seasonal ARIMA Model (SARIMA)",
    "text": "41.6 The Seasonal ARIMA Model (SARIMA)\nThe Seasonal ARIMA (SARIMA) model is a version of the \\(ARIMA\\) model for time series with a seasonal component. Recall that a time series with a seasonal component has a strong relationship with its seasonal lags. The \\(SARIMA\\) model is utilizing the seasonal lags in a similar manner to how the \\(ARIMA\\) model is utilizing the non-seasonal lags with the \\(AR\\) and \\(MA\\) processes and differencing. It does this by adding the following three components to the \\(ARIMA\\) model:\n\nSAR(P) process: A seasonal \\(AR\\) process of the series with its past \\(P\\) seasonal lags. For example, a \\(SAR(2)\\) is an \\(AR\\) process of the series with its past two seasonal lags, that is, \\(Y_t = c + \\Phi_1 Y_{t-f} + \\Phi_2 Y_{t-2f} + \\epsilon_t\\) , where \\(\\Phi\\) represents the seasonal coefficient of the SAR process, and \\(f\\) represents the series frequency.\nSMA(Q) process: A seasonal \\(MA\\) process of the series with its past \\(Q\\) seasonal error terms. For instance, a \\(SMA(1)\\) is a moving average process of the series with its past seasonal error term, that is, \\(Y_t = \\mu + \\epsilon_t + \\Theta_1 \\epsilon_{t-f}\\), where \\(\\Theta\\) represents the seasonal coefficient of the SMA process, and \\(f\\) represents the series frequency.\nSI(D) process: A seasonal differencing of the series with its past \\(D\\) seasonal lags. In a similar manner, we can difference the series with its seasonal lag, that is, \\(Y_{D=1} = Y_t - Y_{t-f}\\).\n\nWe use the following notation to denote the SARIMA parameters:\n\\[SARIMA(p,d,q) \\times (P,D,Q)_s\\]\nThe \\(p\\) and \\(q\\) parameters define the order of the \\(AR\\) and \\(MA\\) processes with its non-seasonal lags, respectively, and \\(d\\) defines the degree of differencing of the series with its non-seasonal lags. Likewise, the \\(P\\) and \\(Q\\) parameters represent the corresponding order of the seasonal \\(AR\\) and \\(MA\\) processes of the series with its seasonal lags, and \\(D\\) defines the degree of differencing of the series with its non-seasonal lags. For example, a \\(SARIMA(1,0,0) \\times (1,1,0)_s\\) model is a combination of an \\(AR\\) process of one non-seasonal and one seasonal lag, along with seasonal differencing.\n\n41.6.1 Forecasting US monthly natural gas consumption with the SARIMA model\nWe will forecast the monthly consumption of natural gas in the US using the \\(SARIMA\\) model. Let’s load the \\(USgas\\) series from the TSstudio package:\n\n\n\n\n\n\n\n\nTask\nFunction\nVariable\n\n\n\n\nPlot the series\nts_plot(USgas)\n\n\n\nSplit the series into training and testing partitions\nts_split(USgas, sample.out = 12)\ntrain\ntest\n\n\nDiagnose the series correlation with its seasonal lags\nacf(train, lag.max = 60)\npacf(train, lag.max = 60)\nN/A\n\n\nConduct a seasonal differencing of the series and plot the output and check if in stationary state\ndiff(train, 12) ts_plot(USgas)\nUSgas_d12\n\n\nAdd the first differencing of the series\ndiff(diff(USgas_d12, 1)) ts_plot(USgas_d12_1)\nUSgas_d12_1\n\n\nReview the ACF and PACF functions again to identify the required process\nacf(USgas_d12_1)\nccf(USgas_d12_1)\nN/A\n\n\n\n\nWe will first start plotting the \\(USgas\\) series and reviewing its main characteristics.\n\nExample:\n\ndata(USgas)\n# Plot the series and review its main characteristics\nts_plot(USgas,\n        title = \"US Monthly Natural Gas consumption\",\n        Ytitle = \"Billion Cubic Feet\",\n        Xtitle = \"Year\")\n\n\n\n\n\nThe USgas series has a strong seasonal pattern, and therefore among the \\(ARIMA\\) family of models, the \\(SARIMA\\) model is the most appropriate model to use. In addition, since the series is trending up, we can already conclude that the series is not stationary and some differencing of the series is required.\n\nWe will start by setting the training and testing partitions with the ts_split functions, leaving the last 12 months of the series as the testing partition:\n\n\nUSgas_split &lt;- ts_split(USgas, sample.out = 12)\ntrain &lt;- USgas_split$train\ntest &lt;- USgas_split$test\n\n\nWe will conduct diagnostics in regards to the series correlation with the ACF and PACF functions. Since we are interested in viewing the relationship of the series with its seasonal lags, we will increase the number of lags to calculate and display by setting the lag.max argument to \\(60\\) lags:\n\n\npar(mfrow=c(1,2))\nacf(train, lag.max = 60)\npacf(train, lag.max = 60)\n\n\n\n\nThe preceding ACF plot indicates that the series has a strong correlation with both the seasonal and non-seasonal lags. Furthermore, the linear decay of the seasonal lags indicates that the series is not stationary and that seasonal differencing is required.\n\nWe will start with a seasonal differencing of the series and plot the output to identify whether the series is in a stationary state:\n\n\nUSgas_d12 &lt;- diff(train, 12)\nts_plot(USgas_d12,\n      title = \"US Monthly Natural Gas consumption - First Seasonal Difference\",\n      Ytitle = \"Billion Cubic Feet (First Difference)\",\n      Xtitle = \"Year\")\n\n\n\n\n\n\nWhile we removed the series trend, the variation of the series is not stable yet. Therefore, we will also try to take the first difference of the series:\n\n\nUSgas_d12_1 &lt;- diff(diff(USgas_d12, 1))\nts_plot(USgas_d12_1,\n      title = \"US Monthly Natural Gas consumption - First Seasonal and Non-Seasonal Differencing\",\n      Ytitle = \"Billion Cubic Feet (Difference)\",\n      Xtitle = \"Year\")\n\n\n\n\n\n\nAfter taking the first order differencing, along with the first order seasonal differencing, the series seems to stabilize around the zero \\(x\\) axis line (or fairly close to being stable). After transforming the series into a stationary state, we can review the ACF and PACF functions again to identify the required process:\n\n\npar(mfrow=c(1,2))\nacf(USgas_d12_1, lag.max = 60)\npacf(USgas_d12_1, lag.max = 60)\n\n\n\n\nThe main observation from the preceding \\(ACF\\) and \\(PACF\\) plots is that both the non-seasonal and seasonal lags (in both plots) are tailing off. Hence, we can conclude that after we difference the series and transform them into a stationary state, we should apply an ARMA process for both the seasonal and non-seasonal components of the SARIMA model.\nTUNING PROCESS\nThe tuning process of the \\(SARIMA\\) model parameters follow the same steps that we applied previously with the \\(ARMA\\) model:\n\nWe set the model maximum order (that is, the sum of the six parameters of the model).\nWe set a range of a possible combination of the parameters’ values under the model’s maximum order constraint.\nWe test and score each model, that is, a typical score methodology with the AIC (which we used previously) or BIC.\nWe select a set of parameter combinations that give the best results.\n\nNow, we will start the tuning process for the USgas series by setting the model order to seven and setting the values of the model parameters to be in the range of \\(0\\) and \\(2\\). Given that we already identified the values of \\(d\\) and \\(D\\) (for example, \\(d = 1\\) and \\(D = 1\\)), which are the differencing parameters of the SARIMA model, we now can focus on tuning the remaining four parameters of the model, that is, \\(p\\), \\(q\\), \\(P\\), and \\(Q\\). Let’s define those parameters and assign the search values:\n\np &lt;- q &lt;- P &lt;- Q &lt;- 0:2\n\nUnder the model’s order constraint and the possible range of values of the model parameters, there are 66 possible combinations. Therefore, it will make sense, in this case, to automate the search process and build a grid search function to identify the values of the parameters that minimize the AIC score.\nWe will utilize the expand.grid function in order to create a data.frame with all the possible search combinations:\n\narima_grid &lt;- expand.grid(p,q,P,Q)\nnames(arima_grid) &lt;- c(\"p\", \"q\", \"P\", \"Q\")\narima_grid$d &lt;- 1\narima_grid$D &lt;- 1\n\nNext, we will trim the grid search table by using combinations that exceed the order constraint of the model (for example, \\(k \\le 7\\)). We will calculate and assign this to the \\(k\\) variable with the rowSums function:\n\narima_grid$k &lt;- rowSums(arima_grid)\n\nWe will utilize the filter function from the dplyr package to remove combinations where the k value is greater than \\(7\\):\n\nlibrary(dplyr)\narima_grid &lt;- arima_grid %&gt;% filter(k &lt;= 7)\n\nNow that the grid search table is ready, we can start the search process. We will use the lapply function to iterate over the grid search table. This function will train the SARIMA model and score its AIC for each set of parameters in the grid search table. The arima function can train the SARIMA model by setting the seasonal argument of the model with the values of \\(P\\), \\(D\\), and \\(Q\\):\narima_search &lt;- lapply(1:nrow(arima_grid), function(i){\n  md &lt;- NULL\n  md &lt;- arima(train, order = c(arima_grid$p[i], 1, arima_grid$q[i]), seasonal = list(order = c(arima_grid$P[i], 1, arima_grid$Q[i])))\n  results &lt;- data.frame(p = arima_grid$p[i], d = 1, q = arima_grid$q[i],\n                        P = arima_grid$P[i], D = 1, Q = arima_grid$Q[i],\n                        AIC = md$aic)\n}) %&gt;% bind_rows() %&gt;% arrange(AIC)\nWe used the bind_rows and arrange functions to append the search results and arranged the table for the dplyr functions. Let’s review the top results of the search table:\nhead(arima_search)\nThe leading model based on the preceding search table is the model \\(SARIMA(1,1,1)(2,1,1)_{12}\\). Before we finalize the forecast, let’s evaluate the selected model’s performance on the testing set.\nWe will retrain the model using the settings of the selected model:\n\nUSgas_best_md &lt;- arima(train, \n                       order = c(1,1,1), \n                       seasonal = list(order = c(2,1,1)))\n\nThe model coefficients, as we can see in the following model summary, are all statistically significant at a level of \\(0.1\\):\n\nUSgas_best_md\n\n\nCall:\narima(x = train, order = c(1, 1, 1), seasonal = list(order = c(2, 1, 1)))\n\nCoefficients:\n         ar1      ma1    sar1     sar2     sma1\n      0.4247  -0.9180  0.0132  -0.2639  -0.7449\ns.e.  0.0770   0.0376  0.0894   0.0834   0.0753\n\nsigma^2 estimated as 10160:  log likelihood = -1292.96,  aic = 2597.91\n\n\nLet’s use the USgas_best_md trained model to forecast the corresponding observations of the testing set:\n\nUSgas_test_fc &lt;- forecast(USgas_best_md, h = 12)\n\nLike we did previously, we will assess the model’s performance with the accuracy function:\n\naccuracy(USgas_test_fc, test)\n\n                    ME      RMSE      MAE       MPE     MAPE      MASE\nTraining set  6.081099  97.85701 73.36854 0.1298714 3.517097 0.6371821\nTest set     42.211253 104.79281 83.09943 1.4913412 3.314280 0.7216918\n                     ACF1 Theil's U\nTraining set  0.004565602        NA\nTest set     -0.049999868 0.3469228\n\n\nWe can use the performance of the seasonal naive model as a benchmark for the SARIMA model’s performance. Recall that the seasonal naive model’s MAPE score was 5.2% on the training set and 9.7% on the testing set. Therefore, the SARIMA provides us with a lift in accuracy with a MAPE score of 3.52% and 3.31% on the training and testing partitions, respectively.\nNow, we will use the test_forecast function to get a more intuitive view of the model’s performance on the training and testing partitions:\n\ntest_forecast(USgas,\nforecast.obj = USgas_test_fc,\ntest = test)\n\n\n\n\n\nAs you can see, the SARIMA model successfully captures the seasonal and trend pattern of the series. On the other hand, the model finds it challenging to capture the seasonal peaks (month of January) on the training partition and has 5.49% absolute error for the month of January (yearly peak) in the testing partition. This is the result of high fluctuation during the winter time. We can handle this uncertainty of the model during peak times with the model confidence intervals or path simulations.\nAt this point, you should pause and evaluate the general performance of the model thus far on the major metrics:\n\nAIC score on the training set.\nThe model coefficients significant.\nMAPE score on both the training and testing set.\nBenchmark the performance against other models (for example, a naive model).\nFitted and forecasted plot.\n\nIf you are satisfied with the results of the model on the different performance metrics, you should move forward and retrain the model with all the series and generate the final forecast. Otherwise, you should return the model parameters and repeat the evaluation process.\nNow that we’ve satisfied the preceding conditions, we can move on to the last step of the forecasting process and generate the final forecast with the selected model. We will start by retraining the selected model on all the series:\n\nfinal_md &lt;- arima(USgas, order = c(1,1,1), \n                  seasonal = list(order = c(2,1,1)))\n\nBefore we forecast the next 12 months, let’s verify that the residuals of the model satisfy the model condition:\n\ncheckresiduals(final_md)\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,1,1)(2,1,1)[12]\nQ* = 30.173, df = 19, p-value = 0.04964\n\nModel df: 5.   Total lags used: 24\n\n\nThe output of the Ljung-Box test suggested that the residuals of the model are white noise.\nBy looking at the preceding residuals plot, you can see that the residuals are white noise and normally distributed. Furthermore, the Ljung-Box test confirms that there is no autocorrelation left on the residuals —with a p-value of 0.04, we cannot reject the null hypothesis that the residuals are white noise. We are good to go! Let’s use the forecast function to forecast the next 12 months of the USgas series:\n\nUSgas_fc &lt;- forecast(final_md, h = 12)\n# Plot the forecast\nplot_forecast(USgas_fc,\n              title = \"US Natural Gas Consumption - Forecast\",\n              Ytitle = \"Billion Cubic Feet\",\n              Xtitle = \"Year\")"
  },
  {
    "objectID": "time_forecast_arima.html#the-auto.arima-function",
    "href": "time_forecast_arima.html#the-auto.arima-function",
    "title": "41  Forecasting with ARIMA Models",
    "section": "41.7 The auto.arima function",
    "text": "41.7 The auto.arima function\nOne of the main challenges of forecasting with the ARIMA family of models is the cumbersome tuning process of the models. As we saw in this chapter, this process includes many manual steps that are required for verifying the structure of the series (stationary or non-stationary), data transformations, descriptive analysis with the \\(ACF\\) and \\(PACF\\) plots to identify the type of process, and eventually tune the model parameters. While it might take a few minutes to train an ARIMA model for a single series, it may not scale up if you have dozens of series to forecast.\nThe auto.arima function from the forecast package provides a solution to this issue. This algorithm automates the tuning process of the ARIMA model with the use of statistical methods to identify both the structure of the series (stationary or not) and type (seasonal or not), and sets the model’s parameters accordingly.\nExample:\n\nUSgas_auto_md1 &lt;- auto.arima(train)\n\nUsing the default arguments of the auto.arima function returns the \\(ARIMA\\) model that minimizes the AIC score. In this case, a model was selected with an AIC score of \\(2599,67\\):\n\nUSgas_auto_md1\n\nSeries: train \nARIMA(2,1,1)(2,1,1)[12] \n\nCoefficients:\n         ar1      ar2      ma1    sar1     sar2     sma1\n      0.4301  -0.0372  -0.9098  0.0117  -0.2673  -0.7431\ns.e.  0.0794   0.0741   0.0452  0.0887   0.0830   0.0751\n\nsigma^2 = 10446:  log likelihood = -1292.83\nAIC=2599.67   AICc=2600.22   BIC=2623.2\n\n\nBy default, the auto.arima function applies a shorter model search by using a step-wise approach for reducing the search time. The trade-off of this approach is that the model may miss some models that may achieve better results.\nWe can improvise with the auto.arima results by setting the search argument of the model. The stepwise argument, when set to FALSE, allows you to set a more robust and thorough search, with the cost of higher search time. This trade-off between performance and compute time can be balanced whenever you have prior knowledge about the series’ structure and characteristics.\nExample:\n\n# Retrain the training set of the USgas series again\nUSgas_auto_md2 &lt;- auto.arima(train,\n                             max.order = 5,\n                             D = 1,\n                             d = 1,\n                             stepwise = FALSE,\n                             approximation = FALSE)\n\nYou can see from the following results that with the robust search, the auto.arima algorithm returns the same model that was identified with the grid search we used in the Seasonal ARIMA model section:\n\nUSgas_auto_md2\n\nSeries: train \nARIMA(1,1,1)(2,1,1)[12] \n\nCoefficients:\n         ar1      ma1    sar1     sar2     sma1\n      0.4247  -0.9180  0.0132  -0.2639  -0.7449\ns.e.  0.0770   0.0376  0.0894   0.0834   0.0753\n\nsigma^2 = 10405:  log likelihood = -1292.96\nAIC=2597.91   AICc=2598.32   BIC=2618.08"
  },
  {
    "objectID": "time_forecast_arima.html#linear-regression-with-arima-errors",
    "href": "time_forecast_arima.html#linear-regression-with-arima-errors",
    "title": "41  Forecasting with ARIMA Models",
    "section": "41.8 Linear Regression with ARIMA Errors",
    "text": "41.8 Linear Regression with ARIMA Errors\nOne of the main assumptions of the linear regression model is that the error term of the series,\\(\\epsilon\\), is the white noise series (for example, there is no correlation between the residuals and their lags). However, when working with time series data, this assumption is violated as, typically, the model predictors do not explain all the variations of the series, and some patterns are left on the model residuals.\nAn example of the failure of this assumption can be seen while fitting a linear regression model to forecast the AirPassenger series. To illustrate this point, we will utilize the tslm function to regress the AirPassenger series with its trend, seasonal component, and seasonal lag (lag 12), and then evaluate the model residuals with the checkresiduals function.\nExample:\n\ndf &lt;- ts_to_prophet(AirPassengers)\nnames(df) &lt;- c(\"date\", \"y\")\ndf$lag12 &lt;- dplyr::lag(df$y, n = 12)\nlibrary(lubridate)\ndf$month &lt;- factor(month(df$date, label = TRUE), ordered = FALSE)\ndf$trend &lt;- 1:nrow(df)\n\nNow, we will split the series into training and testing partitions, leaving the last 12 months for testing using the ts_split function:\n\npar &lt;- ts_split(ts.obj = AirPassengers, sample.out = 12)\ntrain &lt;- par$train\ntest &lt;- par$test\n\nFor the regression of the time series object with an external data.frame object (df), we will apply the same split for training and testing partitions on the predictor’s data.frame object:\n\ntrain_df &lt;- df[1:(nrow(df) - 12), ]\ntest_df &lt;- df[(nrow(df) - 12 + 1):nrow(df), ]\n# Train the model with the tslm function\nmd1 &lt;- tslm(train ~ season + trend + lag12, data = train_df)\n\nLet’s use the checkresiduals function to review the model’s residuals:\n\ncheckresiduals(md1)\n\n\n\n\n\n    Breusch-Godfrey test for serial correlation of order up to 24\n\ndata:  Residuals from Linear regression model\nLM test = 83.147, df = 24, p-value = 1.903e-08\n\n\nIn the preceding ACF plot, the residuals series has a strong correlation with its past lags, and therefore the series is not white noise. We can conclude from the residuals plot that the regression model could not capture all the series patterns. Generally, this should not come as a surprise since the variation of the series could be affected by other variables, such as the ticket and oil prices, the unemployment rate, and other external factors.\nA simple solution to this problem is to model the error terms with the \\(ARIMA\\) model and add it to the regression.\n\n41.8.1 Modeling the residuals with the ARIMA model\nAs we saw in the preceding ACF plot, the AirPassengers residuals indicate that the model was unable to capture all the patterns of the series. Another way to think about this is that the modeling of the series is not complete yet, and additional modeling can be applied on the model residuals to reveal the true error term of the model.\nLet’s remodel the AirPassengers series, but this time with a linear regression model with ARIMA errors. While modeling the model’s error term with an ARIMA model, we should treat the residuals as a series by itself and set the model components \\(p\\), \\(d\\), and \\(q\\) (and \\(P\\), \\(D\\), and \\(Q\\) if the residuals series has seasonal component) using any of the approaches we introduced in this chapter (for example, manual tuning, grid search, or automating the search process with theauto.arima process). For simplicity, we will utilize the auto.arima function. The auto.arima function can be used to model a linear regression model with ARIMA errors via the use of the xreg argument:\nExample:\n\nmd2 &lt;- auto.arima(train,\n                  xreg = cbind(model.matrix(~ month,train_df)[,-1],\n                               train_df$trend,\n                               train_df$lag12),\n                  seasonal = TRUE,\n                  stepwise = FALSE,\n                  approximation = FALSE)\n\nNote that the auto.arima and arima functions do not support categorical variables (that is, the factor class) with the xreg argument. Capturing the seasonal effect will require one-hot encoding (transforming each category into multiple binary variables) of the categorical variable.\nTherefore, we used the model.matrix function from the stats package on the xreg argument of the auto.arima function to transfer the month variable from a categorical variable into a binary variable. In addition, we dropped the first column, which represents the first category (in this case, the month of January), to avoid the dummy variable trap.\nWe set the seasonal argument to TRUE to include a search of both non-seasonal and seasonal models. The auto.arima function will conduct a full search when the step-wise and approximation arguments are set to FALSE. Let’s use the summary function to review the model summary:\n\nsummary(md2)\n\nSeries: train \nRegression with ARIMA(2,0,0)(2,0,0)[12] errors \n\nCoefficients:\n         ar1     ar2     sar1     sar2  monthfeb  monthmar  monthabr  monthmay\n      0.5849  0.3056  -0.4421  -0.2063   -2.7523    0.8231    0.2066    2.8279\ns.e.  0.0897  0.0903   0.1050   0.1060    1.8417    2.3248    2.4162    2.5560\n      monthjun  monthjul  monthago  monthsep  monthoct  monthnov  monthdic\n        6.6057   11.2337   12.1909    3.8269    0.6350   -2.2723   -0.9918\ns.e.    3.2916    4.2324    4.1198    2.9243    2.3405    2.4211    1.9172\n                    \n      0.2726  1.0244\ns.e.  0.1367  0.0426\n\nsigma^2 = 72.82:  log likelihood = -426.93\nAIC=889.86   AICc=896.63   BIC=940.04\n\nTraining set error measures:\n                    ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.4117148 8.353629 6.397538 0.2571543 2.549682 0.2100998\n                    ACF1\nTraining set 0.005966713\n\n\nThe auto.arima function regresses the series with the trend, month, and lag12 variables. In addition, the model used the AR(2) and SAR(2) processes for modeling the error term. We can now review the residuals of the modified md2 model with the checkresiduals functions:\n\ncheckresiduals(md2)\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(2,0,0)(2,0,0)[12] errors\nQ* = 25.816, df = 20, p-value = 0.172\n\nModel df: 4.   Total lags used: 24\n\n\nYou can see the change in the ACF plot after applying the linear regression model with ARIMA errors as the majority of the lags are not statistically significant, as opposed to the preceding linear regression model (md1).\nLet’s evaluate the performance of both models (md1 and md2) on the testing set:\n\nfc1 &lt;- forecast(md1, newdata = test_df)\nfc2 &lt;- forecast(md2, xreg = cbind(model.matrix(~ month,test_df)[,-1],\n                                  test_df$trend,\n                                  test_df$lag12))\n\nNow, let’s use the accuracy function to review the two model’s performance on both the testing and training partitions:\n\naccuracy(fc1, test)\n\n                        ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -1.894492e-15 13.99206 11.47136 -0.1200023 4.472154 0.3541758\nTest set      1.079611e+01 20.82782 18.57391  1.9530865 3.828115 0.5734654\n                  ACF1 Theil's U\nTraining set 0.7502578        NA\nTest set     0.1653119 0.4052175\n\naccuracy(fc2, test)\n\n                      ME      RMSE       MAE        MPE     MAPE      MASE\nTraining set   0.4117148  8.353629  6.397538  0.2571543 2.549682 0.2100998\nTest set     -11.2123706 17.928195 13.353910 -2.4898843 2.924174 0.4385521\n                     ACF1 Theil's U\nTraining set  0.005966713        NA\nTest set     -0.325044929 0.3923795\n\n\nYou can see from the output of the accuracy function that the linear regression with the \\(ARIMA\\) model provides a significant improvement in the model’s accuracy with a lift of 43% on the \\(MAP\\)E score on the training set (from 4.5% to 2.5%) and 31% on the testing set (from 3.8% to 2.9%)."
  },
  {
    "objectID": "time_forecast_ml.html#exploratory-analysis",
    "href": "time_forecast_ml.html#exploratory-analysis",
    "title": "42  Forecasting with Machine Learning Models",
    "section": "42.1 Exploratory Analysis",
    "text": "42.1 Exploratory Analysis\nIn this section, we will focus on exploring and learning about the main characteristics of the USVSales series. These insights will be used to build new features as inputs for the ML model. The exploratory analysis of the USVSales series will focus on the following topics:\n\nView the time series structure (frequency, start and end of the series, and so on).\nExplore the series components (seasonal, cycle, trend, and random components).\nSeasonality analysis.\nCorrelation analysis.\n\n\n42.1.1 Series structure\nLet’s start with ts_info and review the structure of the USVSales series:\nExample\n\nlibrary(TSstudio)\nts_info(USVSales)\n\n The USVSales series is a ts object with 1 variable and 528 observations\n Frequency: 12 \n Start time: 1976 1 \n End time: 2019 12 \n\n\nThe USVSales series is a monthly ts object which represents the total vehicle sales in the US between 1976 and 2018 in thousands of units. Let’s plot the series and review its structure with the ts_plot function:\n\nts_plot(USVSales,\n        title = \"US Total Monthly Vehicle Sales\",\n        Ytitle = \"Thousands of Units\",\n        Xtitle = \"Year\")\n\n\n\n\n\nAs you can see in the preceding plot, the series has cycle patterns, which is common for a macro economy indicator. In this case, it is a macro indicator of the US economy.\n\n\n42.1.2 Series components\nWe can get a deeper view of the series components by decomposing the series into its components and plotting them with the ts_decompose function:\n\nts_decompose(USVSales)\n\n\n\n\n\nBeside the cycle-trend component, we can observe that the plot has a strong seasonal pattern, which we will explore next.\n\n\n42.1.3 Seasonal analysis\nTo get a closer look at the seasonal component of the series, we will subtract from the series, decompose the trend we discussed previously, and use the ts_seasonal function to plot the box plot of the seasonal component of the detrend series:\n\nUSVSales_detrend &lt;- USVSales - decompose(USVSales)$trend\nts_seasonal(USVSales_detrend, type = \"box\")\n\n\n\n\n\nWe can see from the preceding seasonal plot that, typically, the peak of the year occurred during the months of March, May, and June. In addition, you can see that the sales decay from the summer months and peak again in December during the holiday seasons. On the other hand, the month of January is typically the lowest month of the year in terms of sales.\n\n\n42.1.4 Correlation analysis\nThe USVSales series has a high correlation with its first seasonal lag. We can review this assessment with the use of the ts_acf function from the TSstudio package for reviewing the autocorrelation of the series:\n\n# `ts_cor`substitutes ts_acf\nts_cor(USVSales)\n\n\n\n\n\nWe can zoom in on the relationship of the series with the last three seasonal lags using the ts_lags function:\n\nts_lags(USVSales, lags = c(12, 24, 36))\n\n\n\n\n\nThe relationship of the series with the first and also second seasonal lags has a strong linear relationship, as shown in the preceding lags plot.\n\n\n42.1.5 Key findings\nWe can conclude our exploratory analysis of the USVSales series with the following observations:\n\nThe USVSales series is a monthly series with a clear monthly seasonality.\nThe series trend has a cyclic shape, and so the series has a cycle component embedded in the trend.\nThe series’ most recent cycle starts right after the end of the 2008 economic crisis, between 2009 and 2010.\nIt seems that the current cycle reached its peak as the trend starts to flatten out.\nThe series has a strong correlation with its first seasonal lag.\n\nMoreover, as we intend to have a short-term forecast (of 12 months), there is no point in using the full series, as it may enter some noise into the model due to the change of the trend direction every couple of years. (If you were trying to create a long-term forecast, then it may be a good idea to use all or most of the series.)\nTherefore, we will use the model training observations from 2010 and onward. We will use the ts_to_prophet function from the TSstudio package to transform the series from a ts object into a data.frame, and the window function to subset the series observations since January 2010:\n\ndf &lt;- ts_to_prophet(window(USVSales, start = c(2010,1)))\nnames(df) &lt;- c(\"date\", \"y\")\nhead(df)\n\n        date        y\n1 2010-01-01  712.469\n2 2010-02-01  793.362\n3 2010-03-01 1083.953\n4 2010-04-01  997.334\n5 2010-05-01 1117.570\n6 2010-06-01 1000.455\n\n\nBefore we move forward and start with the feature engineering stage, let’s plot and review the subset series of USVSales with the ts_plot function:\n\nts_plot(df, title = \"US Total Monthly Vehicle Sales (Subset)\",\n        Ytitle = \"Thousands of Units\",\n        Xtitle = \"Year\")"
  },
  {
    "objectID": "time_forecast_ml.html#feature-engineering",
    "href": "time_forecast_ml.html#feature-engineering",
    "title": "42  Forecasting with Machine Learning Models",
    "section": "42.2 Feature Engineering",
    "text": "42.2 Feature Engineering\nFeature engineering plays a pivotal role when modeling with ML algorithms. Our next step, based on the preceding observations, is to create new features that can be used as informative input for the model. In the context of time series forecasting, these are some examples of possible new features that can be created from the series itself:\n\nThe series trend: This uses a numeric index. In addition, as the series trend isn’t linear, we will use a second polynomial of the index to capture the overall curvature of the series trend.\nSeasonal component: This creates a categorical variable for the month of the year to capture the series’ seasonality.\nSeries correlation: This utilizes the strong correlation of the series with its seasonal lag and uses the seasonal lag (lag12) as an input to the model.\n\nExample:\n\nlibrary(dplyr)\nlibrary(lubridate)\ndf &lt;- df %&gt;% mutate(month = factor(month(date, label = TRUE), \n                                   ordered = FALSE),\n                    lag12 = lag(y, n = 12)) %&gt;%\n  filter(!is.na(lag12))\n# Add the trend component and its second polynomial (trend squared)\ndf$trend &lt;- 1:nrow(df)\ndf$trend_sqr &lt;- df$trend ^ 2\n# View the structure of the df object after adding the new features\nstr(df)\n\n'data.frame':   108 obs. of  6 variables:\n $ date     : Date, format: \"2011-01-01\" \"2011-02-01\" ...\n $ y        : num  836 1007 1277 1174 1081 ...\n $ month    : Factor w/ 12 levels \"ene\",\"feb\",\"mar\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ lag12    : num  712 793 1084 997 1118 ...\n $ trend    : int  1 2 3 4 5 6 7 8 9 10 ...\n $ trend_sqr: num  1 4 9 16 25 36 49 64 81 100 ...\n\n\nThere are additional feature engineering steps, which in the case of the USVSales series are not required, but may be required in other cases, i.e. scaling, hot encoding."
  },
  {
    "objectID": "time_forecast_ml.html#training-testing-and-model-evaluation",
    "href": "time_forecast_ml.html#training-testing-and-model-evaluation",
    "title": "42  Forecasting with Machine Learning Models",
    "section": "42.3 Training, Testing, and Model Evaluation",
    "text": "42.3 Training, Testing, and Model Evaluation\nIn order to compare the different models that we will be testing in this chapter, we will use the same inputs that we used previously. This includes executing same training and testing partitions throughout this chapter. Since our forecast horizon is for 12 months, we will leave the last 12 months of the series as testing partitions and use the rest of the series as a training partition:\n\nh &lt;- 12\ntrain_df &lt;- df[1:(nrow(df) - h), ]\ntest_df &lt;- df[(nrow(df) - h + 1):nrow(df), ]\n\nPreviously, the \\(h\\) variable represented the forecast horizon, which, in this case, is also equal to the length of the testing partition. We will evaluate the model’s performance based on the MAPE score on the testing partition.\nBe aware that One of the main characteristics of ML models is the tendency to overfit on the training set. Therefore, you should expect that the ratio between the error score on the testing and training partition will be relatively larger than the corresponding results of traditional time series models, such as ARIMA, Holt-Winters, and time series linear regression. In addition to the training and testing partitions, we need to create the inputs for the forecast itself. We will create a data.frame with the dates of the following 12 months and build the rest of the features:\n\nforecast_df &lt;- data.frame(date = seq.Date(from = max(df$date) + month(1),\n                                          length.out = h, by = \"month\"),\n                          trend = seq(from = max(df$trend) + 1, \n                                      length.out = h, by = 1))\nforecast_df$trend_sqr &lt;- forecast_df$trend ^ 2\nforecast_df$month &lt;- factor(month(forecast_df$date, label = TRUE), \n                            ordered = FALSE)\n\nLast but not least, we will extract the last 12 observations of the series from the df object and assign them as the future lags of the series:\n\nforecast_df$lag12 &lt;- tail(df$y, 12)"
  },
  {
    "objectID": "time_forecast_ml.html#model-benchmark",
    "href": "time_forecast_ml.html#model-benchmark",
    "title": "42  Forecasting with Machine Learning Models",
    "section": "42.4 Model Benchmark",
    "text": "42.4 Model Benchmark\nThe performance of a forecasting model should be measured by the error rate, mainly on the testing partition, but also on the training partition. You should evaluate the performance of the model with respect to some baseline model. Since we are using a family of ML regression models, it makes more sense to use a regression model as a benchmark for the ML models.\nWe will train a time series linear regression model using the training and testing partitions we created previously, and evaluate its performance with the testing partitions:\n\nlr &lt;- lm(y ~ month + lag12 + trend + trend_sqr, data = train_df)\nsummary(lr)\n\n\nCall:\nlm(formula = y ~ month + lag12 + trend + trend_sqr, data = train_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-146.625  -38.997    0.111   39.196  112.577 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 542.93505   72.59490   7.479 7.91e-11 ***\nmonthfeb    112.73160   34.16141   3.300 0.001439 ** \nmonthmar    299.20932   54.24042   5.516 4.03e-07 ***\nmonthabr    182.52406   42.53129   4.292 4.88e-05 ***\nmonthmay    268.75603   51.28464   5.240 1.24e-06 ***\nmonthjun    224.66897   44.26374   5.076 2.41e-06 ***\nmonthjul    177.88564   42.21898   4.213 6.49e-05 ***\nmonthago    241.63260   47.00693   5.140 1.86e-06 ***\nmonthsep    152.99058   37.04199   4.130 8.76e-05 ***\nmonthoct    125.16484   35.04896   3.571 0.000601 ***\nmonthnov    127.97288   34.18772   3.743 0.000338 ***\nmonthdic    278.67994   51.09552   5.454 5.21e-07 ***\nlag12         0.33906    0.10738   3.158 0.002236 ** \ntrend         7.73667    1.72415   4.487 2.36e-05 ***\ntrend_sqr    -0.05587    0.01221  -4.576 1.69e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 59.6 on 81 degrees of freedom\nMultiple R-squared:  0.9198,    Adjusted R-squared:  0.9059 \nF-statistic: 66.36 on 14 and 81 DF,  p-value: &lt; 2.2e-16\n\n\nNext, we will predict the corresponding values of the series on the testing partition with the predict function by using test_df as input:\n\ntest_df$yhat &lt;- predict(lr, newdata = test_df)\n\nNow, we can evaluate the model’s performance on the testing partition:\n\nmape_lr &lt;- mean(abs(test_df$y - test_df$yhat) / test_df$y)\nmape_lr\n\n[1] 0.03594578\n\n\nHence, the MAPE score of the linear regression forecasting model is 3.5%. We will use this to benchmark the performance of the ML models."
  },
  {
    "objectID": "time_forecast_ml.html#building-a-ml-model",
    "href": "time_forecast_ml.html#building-a-ml-model",
    "title": "42  Forecasting with Machine Learning Models",
    "section": "42.5 Building a ML Model",
    "text": "42.5 Building a ML Model\n\n42.5.1 Starting a h2o Cluster\nWe will start loading the package and then set the in-memory cluster with the h2o.init function:\n\nlibrary(h2o)\nh2o.init(max_mem_size = \"16G\")\n\n\nH2O is not running yet, starting it now...\n\nNote:  In case of errors look at the following log files:\n    C:\\Users\\javie\\AppData\\Local\\Temp\\Rtmp4WiDYv\\file30d0715134f0/h2o_javie_started_from_r.out\n    C:\\Users\\javie\\AppData\\Local\\Temp\\Rtmp4WiDYv\\file30d04b5a7735/h2o_javie_started_from_r.err\n\n\nStarting H2O JVM and connecting:  Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         5 seconds 971 milliseconds \n    H2O cluster timezone:       Europe/Paris \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.44.0.3 \n    H2O cluster version age:    9 months and 25 days \n    H2O cluster name:           H2O_started_from_R_javie_kii293 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   16.00 GB \n    H2O cluster total cores:    8 \n    H2O cluster allowed cores:  8 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.4.1 (2024-06-14 ucrt) \n\n\nh2o.init allows you to set the memory size of the cluster with the max_mem_size argument. The output of the function, as shown in the preceding code, provides information about the cluster’s setup.\nAny data that is used throughout the training and testing process of the models by the h2o package must load to the cluster itself. The as.h2o function allows us to transform any data.frame object into a h2o cluster:\nExample:\n\ntrain_h &lt;- as.h2o(train_df)\ntest_h &lt;- as.h2o(test_df)\n\nIn addition, we will transform the forecast_df object (the future values of the series inputs) into an h2o object, which will be used to generate the final forecast:\n\nforecast_h &lt;- as.h2o(forecast_df)\n\nFor our convenience, we will label the names of the dependent and independent variables:\n\nx &lt;- c(\"month\", \"lag12\", \"trend\", \"trend_sqr\")\ny &lt;- \"y\"\n\n\n\n42.5.2 Training an ML model\nThe h2o package provides a set of tools for training and testing ML models. The most common model training approaches are as follows:\n\nTraining/testing: This is based on splitting the input data into training and testing partitions by allocating most of the input data to the training partition and leaving the rest to the testing partition. As the names of the partitions imply, the training set is used to train the model, while the testing partition is used to test its performance on new data. Typical allocations are 70/30 (that is, 70% of the data to the training partition and 30% to the testing partition), or roughly close to that ratio, where the data allocation between the two partitions must be random.\nTraining/testing/validation: This is relatively similar to the previous approach, except with added validation partitions. The validation partition is used during the training process to evaluate the tuning of the model parameters. The tuned models are then tested on the testing partition.\nCross-validation: This is one of the most popular training methods for ML models as it reduces the chance of overfitting of the model. This method is based on the following steps:\n\nSplitting the training set, randomly, into \\(K\\) folders.\nTraining the model \\(K\\) times, each time leaving a different folder out as a testing partition, and training the model with the remaining \\(K-1\\) folders.\nThroughout the training process, the model tunes the model’s parameters.\nThe final model is tested on the testing partition.\n\n\nwe will use the cross-validation (CV) approach to train these models.\n\n\n42.5.3 Forecasting with the Random Forest model\nNow that we have prepared the data, created new features, and launched a h2o cluster, we are ready to build our first forecasting model with the Random Forest (RF) algorithm. The RF algorithm is one of the most popular ML models, and it can be used for both classification and regression problems. In a nutshell, the RF algorithm is based on an ensemble of multiple tree models.\nAs its name implies, it has two main components:\n\nRandom: The input for each tree model is based on a random sample, along with the replacement of both the columns and rows of the input data. This method is also known as bagging.\nForest: The collection of tree-based models, which eventually creates the forest.\n\nWe will start with a simplistic RF model by using 500 trees and 5 folder CV training. In addition, we will add a stop criteria to prevent the model from fitting the model while there is no significant change in the model’s performance. In this case, we will set the stopping metric as RMSE, the stopping tolerance as 0.0001, and the stopping rounds to 10:\nExample:\n\nrf_md &lt;- h2o.randomForest(training_frame = train_h, \n                          nfolds = 5,\n                          x = x,\n                          y = y,\n                          ntrees = 500,\n                          stopping_rounds = 10,\n                          stopping_metric = \"RMSE\",\n                          score_each_iteration = TRUE,\n                          stopping_tolerance = 0.0001,\n                          seed = 1234)\n\nThe h2o.randomForest function returns an object with information about the parameter settings of the model and its performance on the training set (and validation, if used). We can view the contribution of the model inputs with the h2o.varimp_plot function. This function returns a plot with the ranking of the input variables’ contribution to the model performance using a scale between 0 and 1, as shown in the following code:\n\nh2o.varimp_plot(rf_md)\n\n\n\n\nAs we can see from the preceding variable importance plot, the lag variable, lag12, is the most important to the model. This shouldn’t be a surprise as we saw the strong relationship between the series and its seasonal lag in the correlation analysis. Right after this, the most important variables are trend_sqr, month, and trend. The output of the model contains (besides the model itself) information about the model’s performance and parameters.\nLet’s review the model summary:\n\nrf_md@model$model_summary\n\nModel Summary: \n  number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n1              41                       41               31591         8\n  max_depth mean_depth min_leaves max_leaves mean_leaves\n1        12   10.04878         45         66    56.70732\n\n\nWe can see that we utilized only 41 trees out of the 500 that were set by the ntrees argument. This is as a result of the stopping parameters that were used on the model. The following plot demonstrates the learning process of the model as a function of the number of trees:\n\nlibrary(plotly)\ntree_score &lt;- rf_md@model$scoring_history$training_rmse\nplot_ly(x = seq_along(tree_score), y = tree_score,\n        type = \"scatter\", mode = \"line\") %&gt;%\n  layout(title = \"The Trained Model Score History\",\n         yaxis = list(title = \"RMSE\"),\n         xaxis = list(title = \"Num. of Trees\"))\n\n\n\n\n\nLast but not least, let’s measure the model’s performance on the testing partition. We will use the h2o.predict function to predict the corresponding values of the series on the testing partition:\n\n# test_h &lt;- as.h2o(test_df)\ntest_h$pred_rf &lt;- h2o.predict(rf_md, test_h)\nh2o.shutdown()\n\nNext, we will transfer the h2o data frame to a data.frame object with the as.data.frame function:\n\ntest_1 &lt;- as.data.frame(test_h)\n\nNow, we can calculate the MAPE score of the RF model on the test partition:\n\nmape_rf &lt;- mean(abs(test_1$y - test_1$pred_rf) / test_1$y)\nmape_rf\n\n[1] 0.04647164\n\n\nAs you can see from the model error score, the RF model with its default settings was able to achieve a lower error rate than our benchmark model, that is, the linear regression model, with a MAPE score of 4.6% as opposed to 3.6%.\n\n\n42.5.4 Model optimization\nGenerally, when using the default option for the model’s parameters, the model may perform well, but there might be some room left for additional optimization and improvement in regards to the performance of the model. There are a variety of techniques for model optimization and tuning parameters, such as manual tuning, grid search, and algorithm-based tuning.\nGRID SEARCH APPROACH\nThe h2o.grid function allows you to set a set of values for some selected parameters and test their performance on the model in order to identify the tuning parameters that optimize the model’s performance.\nExample:\n\n# Start by setting the search parameters\nhyper_params_rf &lt;- list(mtries = c(2, 3, 4), \n                        sample_rate = c(0.632, 0.8, 0.95),\n                        col_sample_rate_per_tree = c(0.5, 0.9, 1.0),\n                        max_depth = c(seq(1, 30, 3)),\n                        min_rows = c(1, 2, 5, 10))\n\nThe more parameters you add or define for a wide range of values, the larger the possible search combination. For efficiency reasons, we will set a random search and restrict the search time to 20 minutes with max_runtime_sec. We will use the same stopping metric that we used previously:\n#{r Five}\nsearch_criteria_rf &lt;- list(strategy = \"RandomDiscrete\",\n                           stopping_metric = \"rmse\",\n                           stopping_tolerance = 0.0001,\n                           stopping_rounds = 10,\n                           max_runtime_secs = 60 * 5)\nAfter we set the search arguments for the h2o.grid function, we can start the search:\n#{r Six}\nrf2 &lt;- h2o.grid(algorithm = \"randomForest\", \n                search_criteria = search_criteria_rf,\n                hyper_params = hyper_params_rf,\n                x = x,\n                y = y,\n                training_frame = train_h,\n                ntrees = 5000,\n                nfolds = 5,\n                grid_id = \"rf_grid\",\n                parallelism = 1,\n                seed = 1234)\nNote that setting a large number of trees with a tree-based model such as RF or GBM, along with a stopping metric, will ensure that the model will keep building additional trees until it meets the stopping criteria. Therefore, setting the stopping criteria plays a critical roll in both the efficiency of the model and its results.\nWe will now extract the grid results, sort the models by their RMSE score, and pull the lead model:\nrf2_grid_search &lt;- h2o.getGrid(grid_id = \"rf_grid\",\n                               sort_by = \"rmse\",\n                               decreasing = FALSE)\nrf_grid_model &lt;- h2o.getModel(rf2_grid_search@model_ids[[1]])\nLet’s test the model on the testing partition and evaluate its performance:\ntest_h$rf_grid &lt;- h2o.predict(rf_grid_model, test_h)\nmape_rf2 &lt;- mean(abs(test_1$y - test_1$rf_grid) / test_1$y)\nmape_rf2\nThe additional optimization step contributed to the lift in the model’s accuracy, with a MAPE score of 3.33% compared to 3.7% and 4% for the first RF model we trained, and the linear regression model, respectively.\nThe following plot provides an additional view of the model’s performance:\nplot_ly(data = test_1) %&gt;%\n  add_lines(x = ~ date, y = ~y, name = \"Actual\") %&gt;%\n  add_lines(x = ~ date, y = ~ yhat, name = \"Linear Regression\", \n            line = list(dash = \"dot\")) %&gt;% \n  add_lines(x = ~ date, y = ~ pred_rf, name = \"Random Forest\", \n            line = list(dash = \"dash\")) %&gt;%\n  add_lines(x = ~ date, y = ~ rf_grid, name = \"Random Forest (grid)\", \n            line = list(dash = \"dash\")) %&gt;%\n  layout(title = \"Total Vehicle Sales - Actual vs. Prediction (Random Forest)\",\n         yaxis = list(title = \"Thousands of Units\"),\n         xaxis = list(title = \"Month\"))\n\n\n42.5.5 Forecasting with the GBM model\nThe GBM algorithm is another ensemble and tree-based model. It uses the boosting approach in order to train different subsets of the data, and repeats the training of subsets that the model had with a high error rate. This allows the model to learn from past mistakes and improve the predictive power of the model.\nThe following example demonstrates the use of the h2o.gbm function for training the GBM model with the same input data we used previously:\nExample:\n\ngbm_md &lt;- h2o.gbm(training_frame = train_h,\n                  nfolds = 5,\n                  x = x, \n                  y = y, \n                  max_depth = 20,\n                  distribution = \"gaussian\",\n                  ntrees = 500,\n                  learn_rate = 0.1,\n                  score_each_iteration = TRUE\n)\n\nWe can review the rank of the importance of the model’s variables with the h2o.varimp_plot function:\n\nh2o.varimp_plot(gbm_md)\n\n\n\n\nFor RF, the GBM model ranks the lag12 variable as the most important to the model. Let’s test the model’s performance on the testing set:\n\ntest_h$pred_gbm &lt;- h2o.predict(gbm_md, test_h)\ntest_1 &lt;- as.data.frame(test_h)\nmape_gbm &lt;- mean(abs(test_1$y - test_1$pred_gbm) / test_1$y)\nmape_gbm\n\nThe GBM model scored the lowest MAPE (is not!) among the models we’ve tested so far with a 3.8% error rate, compared to the RF model (with grid search) and linear regression model. It would be a great exercise to apply a grid search on the GBM model and test whether any additional improvements can be achieved.\nLet’s visualize the results and compare the prediction with the actual and baseline prediction:\n\nplot_ly(data = test_1) %&gt;%\n  add_lines(x = ~ date, y = ~y, name = \"Actual\") %&gt;%\n  add_lines(x = ~ date, y = ~ yhat, \n            name = \"Linear Regression\", \n            line = list(dash = \"dot\")) %&gt;%\n  add_lines(x = ~ date, y = ~ pred_gbm, \n            name = \"Gradient Boosting Machine\", line = list(dash = \"dash\"))%&gt;%\n  layout(title = \"Total Vehicle Sales - Actual vs. Prediction (Gradient Boosting Machine)\",\n         yaxis = list(title = \"Thousands of Units\"),\n         xaxis = list(title = \"Month\"))\n\n\n\n\n\n\n\n42.5.6 Forecasting with the AutoML model\nLet’s take a look at a third approach to tuning ML models. Here, we will use the h2o.automl function that provides an automated approach to training, tuning, and testing multiple ML algorithms before selecting the model that performed best based on the model’s evaluation. It utilizes algorithms such as RF, GBM, DL, and others using different tuning approaches.\nSimilarly, the h2o.grid function can apply any of the training approaches (CV, training with validation, and so on) during the training process of the models. Let’s use the same input as before, and train the forecasting model:\nExample:\nautoML1 &lt;- h2o.automl(training_frame = train_h,\n                      x = x,\n                      y = y,\n                      nfolds = 5,\n                      max_runtime_secs = 60*20,\n                      seed = 1234)\nNote that we can set the runtime of the function. A longer running time could potentially yield better results.\nIn the preceding example, the function’s running time was set to 20 minutes. The function returns a list with the leaderboard of all the tested models:\nautoML1@leaderboard"
  },
  {
    "objectID": "time_forecast_ml.html#selecting-the-final-model",
    "href": "time_forecast_ml.html#selecting-the-final-model",
    "title": "42  Forecasting with Machine Learning Models",
    "section": "42.6 Selecting the Final Model",
    "text": "42.6 Selecting the Final Model\nNow that we’ve finished the training and testing process of the models, it’s time to finalize the process and choose the model to forecast with the series. We trained the following models:\n\nBaseline model: Linear regression model with 4% MAPE score on the testing partition.\nRF: Using default tuning parameters with 3.74% MAPE score on the testing partition.\nRF: Using grid search for tuning the model parameters with 3.33% MAPE score on the testing partition.\nGBM: Using the default tuning parameters with 2.75% MAPE score on the testing partition.\nAutoML: Selecting a deep learning model with 3.48% MAPE score on the testing partition.\n\nSince all of these models achieved better results than the baseline, we can drop the baseline model. Also, since the second RF model (with grid search) achieved better results than the first, there is no point in keeping the first model. This leaves us with three forecasting models, that is, RF (with grid search), GBM, and AutoML. Generally, since the GBM model achieved the best MAPE results, we will select it. However, it is always nice to plot the actual forecast and check what the actual forecast looks like.\nBefore we plot the results, let’s use these three models to forecast the next 12 months using the data.frame forecast we created in the Forecasting with the Random Forest model and Forecasting with the GBM model sections:\nforecast_h$pred_gbm &lt;- h2o.predict(gbm_md, forecast_h)\nforecast_h$pred_rf &lt;- h2o.predict(rf_grid_model, forecast_h)\nforecast_h$pred_automl &lt;- h2o.predict(autoML1@leader, forecast_h)\nWe will transform the object back into a data.frame object with the as.data.frame function:\nfinal_forecast &lt;- as.data.frame(forecast_h)\nNow, we can plot the final forecast with the plotly package:\nplot_ly(x = df$date, \n        y = df$y,\n        type = \"scatter\",\n        mode = \"line\",\n        name = \"Actual\") %&gt;%\n    add_lines(x = final_forecast$date, \n              y = final_forecast$pred_rf, \n              name = \"Random Forest\") %&gt;%\n    add_lines(x = final_forecast$date, \n              y = final_forecast$pred_gbm, \n              name = \"GBM\") %&gt;%\n    add_lines(x = final_forecast$date, \n              y = final_forecast$pred_automl, \n              name = \"Auto ML\") %&gt;%\n    layout(title = \"Total Vehicle Sales - Final Forecast\",\n          yaxis = list(title = \"Thousands of Units\", \n                      range = c(1100, 1750)),\n          xaxis = list(title = \"Month\", \n                      range = c(as.Date(\"2016-01-01\"),\n                      as.Date(\"2020-01-01\")))\n)\nIt seems that all three models capture the seasonality component of the vehicle sales series. However, it seems that the oscillation of AutoML is higher with respect to one of the RF and GBM models. Therefore, it would make sense to select either the GBM or RF models as the final forecast.\nA more conservative approach would be to create and ensemble of the three forecasts by either weighted on regular average. For instance, you can use a simple function for testing the different average of different models and select the combination that minimizes the forecast error rate on the testing set."
  },
  {
    "objectID": "products.html#manipulate",
    "href": "products.html#manipulate",
    "title": "43  Building Data Products",
    "section": "43.1 Manipulate",
    "text": "43.1 Manipulate\nThe manipulate package creates a quick interactive graphic that offers simple controls, including sliders, pickers and checkboxes.\nlibrary(manipulate) \nmanipulate(plot(1:x), x = slider(1, 100))\nWith manipulate you can have more than one set of controls by simply adding more arguments to the manipulate function.\nNote that it’s difficult to share a manipulate interactive graphic.\nThis is a link on how manipulate actually works: link"
  },
  {
    "objectID": "products.html#shiny",
    "href": "products.html#shiny",
    "title": "43  Building Data Products",
    "section": "43.2 Shiny",
    "text": "43.2 Shiny\nIt is described by RStudio as “A web application framework for R”. “Turn your analyses into interactive web applications No HTML, CSS, or JavaScript knowledge required”.\nOnly those who have shiny installed and have access to your code could run your web page. However, RStudio offers a service for hosting shiny apps (their servers) on a platform called shinyapps.io.\nIf on Windows, make sure that you have Rtools installed. Then, you can install shiny with library(shiny):\nExample:\n\nlibrary(shiny)\ntextInput(\"name\", \"What is your name?\")\n\n\nWhat is your name?\n\n\n\nnumericInput(\"age\", \"How old are you?\", NA, min = 0, max = 150)\n\n\nHow old are you?\n\n\n\n\nA shiny app consists of two files. First, a file called ui.R that controls the User Interface (hence the ui in the filename) and secondly, a file server.R that controls the shiny server (hence the server in the filename).\n\n43.2.1 Create ui.R file:\nlibrary(shiny)\nshinyUI(\n  pageWithSidebar( \n    headerPanel(\"Hello Shiny!\"), \n    sidebarPanel( \n      h3('Sidebar text')\n    ),\n    mainPanel( \n      h3('Main Panel text')\n    )\n  )\n)\n\n\n43.2.2 Create server.R file:\nlibrary(shiny) \nshinyServer( \n  function(input, output) {\n  }\n)\nThe current version of Rstudio has a “Run app” button in the upper right hand corner of the editor if a ui.R or server.R file is open.\nTo get used to programming shiny apps, you need to throw away a little of your thinking about R; it’s a different style of programming.\n\n\n43.2.3 Types of inputs\n\nNumeric input:\nnumericInput('id1', 'Numeric input, labeled id1', 0, min = 0, max = 10, step= 1)\nCheckbox input:\ncheckboxGroupInput(\"id2\", \"Checkbox\", c(\"Value 1\" = \"1\", \"Value 2\" = \"2\", \"Value 3\" = \"3\"))\nDate input:\ndateInput(\"date\", \"Date:\")\n\n\n\n43.2.4 Sharing your app\nNow that we have a working app we’d like to share it with the world. It’s much nicer to have it display as a standalone web application.\nThis requires running a shiny server to host the app. Instead of creating and deploying our own shiny server, we’ll rely on RStudio’s service: link\n\nAfter login in shinyapps:\ninstall.packages(\"devtools\")\ninstall.packages(\"shinyapps\")\nRun code:\nshinyapps::setAccountInfo(name='&lt;ACCOUNT NAME&gt;', \n      token='&lt;TOKEN&gt;', \n      secret='&lt;SECRET&gt;')\nSubmit code:\ndeployApp(appName = \"myFirstApp\")\n\n\n\n43.2.5 Build a GUI with html\nCheck out this link"
  },
  {
    "objectID": "products.html#reproducible-presentations",
    "href": "products.html#reproducible-presentations",
    "title": "43  Building Data Products",
    "section": "43.3 Reproducible presentations",
    "text": "43.3 Reproducible presentations\n\n43.3.1 Slidify\nSlidify is for building reproducible presentations.\ninstall.packages(\"devtools\") \nlibrary(devtools)/ require(devtools)\ninstall_github('ramnathv/slidify')\ninstall_github('ramnathv'/'slidifyLibraries') o devtools::install_github(pkgs, force = TRUE)\nlibrary(slidify)\n\n\n43.3.2 R studio presenter\nFor more information about R studio presenter, chek out this link\nTip: if you’re sort of a hacker type and you like to tinker with things, use slidify. If you just want to get it done and not worry about it, use RPres. Either way, you really can’t go wrong."
  },
  {
    "objectID": "products.html#interactive-graphs",
    "href": "products.html#interactive-graphs",
    "title": "43  Building Data Products",
    "section": "43.4 Interactive Graphs",
    "text": "43.4 Interactive Graphs\n\n43.4.1 htmlwidgets\nHTML is an interactive format, and you can take advantage of that interactivity with htmlwidgets, R functions that produce interactive HTML visualizations. For example, take the leaflet map below, where you can drag the map around, zoom in and out, etc. You obviously can’t do that in a book, so Quarto automatically inserts a static screenshot for you.\nExample:\n\nlibrary(leaflet)\nleaflet() |&gt;\n  setView(-3.7077, 40.4156, , zoom = 16) |&gt; \n  addTiles() |&gt;\n  addMarkers(-3.7077, 40.4156, popup = \"Plaza Mayor de Madrid\") \n\n\n\n\n\nThere are many packages that provide htmlwidgets, including:\n\ndygraphs for interactive time series visualizations.\nDT for interactive tables.\nthreejs for interactive 3d plots.\nDiagrammeR for diagrams (like flow charts and simple node-link diagrams).\n\nTo see a complete list of the packages that provide htmlwidgets visit https://www.htmlwidgets.org.\n\n\n43.4.2 leaflet\nlink to leaflet\nleaflet seems to be emerging as the most popular R package for creating interactive maps.\nThe map widget (the leaflet() command) starts out a map and then you add elements or modify the map by passing it as arguments to mapping functions.\n\n\n43.4.3 rCharts\nlink to rCharts\nrCharts is a way to create interactive javascript visualizations using R.\nrequire(devtools) \ninstall_github('rCharts', 'ramnathv')\n\n\n43.4.4 googleVis\nlink to googlevis\nGoogle has some nice visualization tools built into their products (e.g. Google Maps). These include maps and interactive graphs.\ninstall.packages(\"googleVis\")\nlibrary(googlevis)\n\n\n43.4.5 plot.ly\nlink to plot.ly\nplotly relies on the platform/website plot.ly for creating interactive graphics.\nrequire(devtools) \ninstall_github(\"ropensci/plotly\")\nPlotly will give you the json data and gives a tremendous number of options for publishing the graph.\nNotably, plotly allows for integration with ggplot2.\nFor interactive graphics, learning some javascript and following that up with D3 would be the logical next step"
  },
  {
    "objectID": "product_repro.html#goal",
    "href": "product_repro.html#goal",
    "title": "44  Reproducible Reporting",
    "section": "44.1 Goal",
    "text": "44.1 Goal\nThe goal is to have independent people to do independent things with different data, different methods, and different laboratories and see if you get the same result. But the problem is that it’s becoming more and more challenging to do replication or to replicate other studies. Part of the reason is because studies are getting bigger and bigger.\nThe idea behind a reproducible reporting is to create a kind of minimum standard or a middle ground where we won’t be replicating a study, but maybe we can do something in between.\nYou need to make the data available for the original study and the computational methods available so that other people can look at your data and run the kind of analysis that you’ve run, and come to the same findings that you found. If you can take someone’s data and reproduce their findings, then you can, in some sense, validate the data analysis.\nUnderstanding what someone did in a data analysis now requires looking at code and scrutinizing the computer programs that people used."
  },
  {
    "objectID": "product_repro.html#the-data-science-pipeline",
    "href": "product_repro.html#the-data-science-pipeline",
    "title": "44  Reproducible Reporting",
    "section": "44.2 The Data Science Pipeline",
    "text": "44.2 The Data Science Pipeline\nThe basic idea behind reproducibility is to focus on the elements in the blue blox: the analytic data and the computational results.\n\n\n\nFigure 8: The Data Science Pipeline\n\n\nWith reproducibility the goal is to allow the author of a report and the reader of that report to “meet in the middle”."
  },
  {
    "objectID": "product_repro.html#organizing-a-data-analysis",
    "href": "product_repro.html#organizing-a-data-analysis",
    "title": "44  Reproducible Reporting",
    "section": "44.3 Organizing a Data Analysis",
    "text": "44.3 Organizing a Data Analysis\n\nRaw Data:\n\nYou want to store this raw data in your analysis folder.\nIf the data were accessed from the web you want to include things like the URL, where you got the data, what the data set is, a brief description of what it’s for, the date that you accessed the URL on, the website, etc.\nYou may want this in a README file.\nIf you’re using git to track things that are going on in your project, add your raw data, if possible. In the log message, when you add it you can talk about what the website was where you got it, what the URL was, etc.\n\nProcessed Data:\n\nYour processed data should be named so that you can easily see what script generated what data.\nIn any README file or any sort of documentation, it’s important to document what code files were used to transform the raw data into the processed data.\n\nFigures:\n\nExploratory figures.\nFinal figures. The final figures usually make a very small subset of the set of exploratory figures that you might generate. You typically don’t want to inundate people with a lot of figures because then the ultimate message of what you’re trying to communicate tends to get lost in a pile of figures.\n\nScripts:\n\nFinal scripts will be much more clearly commented. You’ll likely have bigger comment blocks for whole sections of code.\n\nR Markdown files:\n\nThey may not be exactly required, but they can be very useful to summarize parts of an analysis or an entire analysis.\nYou can embed code and text into the same document and then you process the document into something readable like a webpage or a PDF file.\n\nFinal Report:\n\nThe point of this is to tell the final story of what you generated here.\nYou’ll have a title, an introduction that motivates your problem, the methods that you used to refine, the results and any measures of uncertainty, and then any conclusions that you might draw from the data analysis that you did, including any pitfalls or potential problems."
  },
  {
    "objectID": "product_repro.html#structure-of-a-data-analysis",
    "href": "product_repro.html#structure-of-a-data-analysis",
    "title": "44  Reproducible Reporting",
    "section": "44.4 Structure of a Data Analysis",
    "text": "44.4 Structure of a Data Analysis\n1. Defining the question:\n\nA proper data analysis has a scientific context, and at least some general question that we’re trying to investigate which will narrow down the kind of dimensionality of the problem. Then we’ll apply the appropriate statistical methods to the appropriate data.\nDefining a question is the most powerful dimension reduction tool you can ever employ.\nThe idea is, if you can narrow down your question as specifically as possible, you’ll reduce the kind of noise that you’ll have to deal with when you’re going through a potentially very large data set.\nThink about what type of question you’re interested in answering before you go delving into all the details of your data set. That will lead you to the data. Which may lead you to applied statistics, which you use to analyze the data.\n\n2. Defining the ideal dataset/Determining what data you can access (the real data set):\n\nsometimes you have to go for something that is not quite the ideal data set.\nYou might be able to find free data on the web. You might need to buy some data from a provider.\nIf the data simply does not exist out there, you may need to generate the data yourself in some way.\n\n3. Obtaining the data:\n\nYou have to be careful to reference the source, so wherever you get the data from, you should always reference and keep track of where it came from.\nIf you get data from an Internet source, you should always make sure at the very minimum to record the URL, which is the web site indicator of where you got the data, and the time and date that you accessed it.\n\n4. Cleaning the data:\n\nRaw data typically needs to be processed in some way to get it into a form where you can model it or feed it into a modeling program.\nIf the data is already pre-processed, it’s important that you understand how it was done. Try to get some documentation about what the pre-processing was and how the sampling was done.\nIt is very important that anything you do to clean the data is recorded.\nOnce you have cleaned the data and you have gotten a basic look at it, it is important to determine if the data are good enough to solve your problems.\nIf you determine the data are not good enough for your question, then you’ve got to quit, try again, change the data, or try a different question. It is important to not simply push on with the data you have, just because that’s all that you’ve got, because that can lead to inappropriate inferences or conclusions.\n\n5. Exploratory data analysis:\n\nIt would be useful to look at what are the data, what did the data look like, what’s the distribution of the data, what are the relationships between the variables.\nYou want to look at basic summaries, one dimensional, two dimensional summaries of the data and we want to check for is there any missing data, why is there missing data, if there is, create some exploratory plots and do a little exploratory analyses. o Split the data set into Train and Test data sets:\nlibrary(kernlab) \ndata(spam)\nset.seed(3435) \ntrainIndicator = rbinom(4601, size = 1, prob = 0.5) table(trainIndicator)\ntrainSpam = spam[trainIndicator == 1, ] \ntestSpam = spam[trainIndicator == 0, ]\nWe can make some plots and we can compare, what are the frequencies of certain characteristics between the spam and the non spam emails:\nboxplot(capitalAve ~ type, data = trainSpam)\npairs(log10(trainSpam[, 1:4] + 1))   ## pairs plot of the first four variables\nYou can see that some of them are correlated, some of them are not particularly correlated, and that’s useful to know.\nExplore the predictors space a little bit more by doing a hierarchical cluster analysis, e.g. the Dendrogram just to see how what predictors or what words or characteristics tend to cluster together\nhCluster = hclust(dist(t(trainSpam[, 1:57]))) \nplot(hCluster)\n\n6. Statistical prediction/modeling:\n\nAny statistical modeling that you engage in should be informed by questions that you’re interested in, of course, and the results of any exploratory analysis. The exact methods that you employ will depend on the question of interest.\nwe’re just going to cycle through all the variables in this data set using this for-loop to build a logistic regression model, and then subsequently calculate the cross validated error rate of predicting spam emails from a single variable.\nOnce we’ve done this, we’re going to try to figure out which of the individual variables has the minimum cross validated error rate. It turns out that the predictor that has the minimum cross validated error rate is this variable called charDollar. This is an indicator of the number of dollar signs in the email.\nWe can actually make predictions now from the model on the test data (now we’re going to predict the outcome on the test data set to see how well we do).\nwe can take a look at the predicted values from our model, and then compare them with the actual values from the test data set, because we know which was spam, and which was not. Now we can just calculate the error rate.\n\n7. Interpretation of results:\n\nThink carefully about what kind of language you use to interpret your results. It’s also good to give an explanation for why certain models predict better than others, if possible.\nIf there are coefficients in the model that you need to interpret, you can do that here.\nAnd in particular it’s useful to bring in measures of uncertainty, to calibrate your interpretation of the final results.\n\n8. Challenging of results:\n\nIt’s good to challenge everything, the whole process by which you’ve gone through this problem. Is the question even a valid question to ask? Where did the data come from? How did you get the data? How did you process the data? How did you do the analysis and draw any conclusions?\nAnd if you built models, why is your model the best model? Why is it an appropriate model for this problem? How do you choose the things to include in your model?\nAll these things are questions that you should ask yourself and should have a reasonable answer to, so that when someone else asks you, you can respond in kind.\n\n9. Synthesis and write up:\n\nTypically in any data analysis, there are going to be many, many, many things that you did. And when you present them to another person or to a group you’re going to want to have winnowed it down to the most important aspects to tell a coherent story.\nTypically you want to lead with the question that you were trying to address.\nIt’s important that you don’t include every analysis that you ever did, but only if its needed for telling a coherent story. Talk about the analyses of your data set in the order that’s appropriate for the story you’re trying to tell.\nInclude very well done figures so that people can understand what you’re trying to say in one picture or two.\n\n10. Creating reproducible code:\n\nYou can use tools like RMarkdown and knitr and RStudio to document your analyses as you do them.\nYou can preserve the R code as well as any kind of a written summary of your analysis in a single document using knitr.\nIf someone cannot reproduce your data analysis then the conclusions that you draw will be not as worthy as an analysis where the results are reproducible."
  },
  {
    "objectID": "product_repro.html#r-markdown",
    "href": "product_repro.html#r-markdown",
    "title": "44  Reproducible Reporting",
    "section": "44.5 R Markdown",
    "text": "44.5 R Markdown\nlink to R Markdown guide\nThe benefit of Markdown for writers is that it allows one to focus on writing as opposed to formatting. It has simple and minimal yet intuitive formatting elements and can be easily converted to valid HTML (and other formats) using existing tools.\nR markdown is the integration of R code with Markdown. Documents written in R Markdown have R coded nested inside of them, which allows one to create documents containing “live” R code.\nR markdown can be converted to standard markdown using the knitr package in R. Markdown can subsequently be converted to HTML using the markdown package in R."
  },
  {
    "objectID": "product_repro.html#knitr",
    "href": "product_repro.html#knitr",
    "title": "44  Reproducible Reporting",
    "section": "44.6 Knitr",
    "text": "44.6 Knitr\nFor literate statistical programming, the idea is that a report is viewed as a stream of text and code.\nAnalysis code is divided into code chunks with text surrounding the code chunks explaining what is going on: - In general, literate programs are weaved to produce human-readable documents - and tangled to produce machine- readable documents\nThe requirements for writing literate programs are a documentation language (Markdown) and a programming language (R).\nMy First knitr Document:\n\nOpen an R Markdown document.\nRStudio will prompt you with a dialog box to set some of the metadata for the document.\nWhen you are ready to process and view your R Markdown document the easiest thing to do is click on the Knit HTML button that appears at the top of the editor window.\nNote here that the the code is echoed in the document in a grey background box and the output is shown just below it in a white background box. Notice also that the output is prepended with two pound symbols.\nCode chunks begin with {r} and end with just. Any R code that you include in a document must be contained within these delimiters, unless you have inline code.\nHiding code:\n{r pressure, echo=FALSE}\nHiding results:\n{r pressure, echo=FALSE, results = “hide”}\nRather than try to copy and paste the result into the paragraph, it’s better to just do the computation right there in the text:\nMy favourite random number is r rnorm(1)\nTables can be made in R Markdown documents with the help of the xtable package.\nThe opts_chunk variable sets an option that applies to all chunks in your document. For example, if we wanted the default to be that all chunks do NOT echo their code and always hide their results, we could set:\nknitr::opts_chunk$set(echo = FALSE, results = “hide”)\nGlobal options can always be overridden by any specific options that are set in at the chunk level:\n{r pressure, echo=FALSE, results = “asis”}\nChunk caching. If you have a long document or one involving lengthy computations, then every time you want to view your document in the pretty formatted version, you need to re-compile the document, meaning you need to re- run all the computations. Chunk caching is one way to avoid these lengthy computations.\ncache = TRUE\nIncluding a call to sessionInfo() at the end of each report written in R (perhaps with markdown or knitr) can be useful for communicating to the reader what type of environment is needed to reproduce the contents of the report."
  }
]